{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NW_JBhf02YC8","outputId":"aea76776-ec5a-4312-869a-2001010d0cff","executionInfo":{"status":"ok","timestamp":1749052876148,"user_tz":-420,"elapsed":815048,"user":{"displayName":"Resconnect","userId":"00882996269091038835"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","‚úÖ ƒê√£ gi·∫£i n√©n: /content/drive/MyDrive/kaggle_data/aptos2019/train_images.zip ‚Üí /content/extracted_zip_files/train_images\n","‚úÖ ƒê√£ gi·∫£i n√©n: /content/drive/MyDrive/kaggle_data/aptos2019/test_images.zip ‚Üí /content/extracted_zip_files/test_images\n","ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng 3662 ·∫£nh.\n","Train X size: 2379\n","Train y size: 2379\n","Valid X size: 550\n","Valid y size: 550\n","Test X size: 733\n","Test y size: 733\n"]}],"source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import glob\n","import zipfile\n","import matplotlib.pyplot as plt\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","\n","# 1. Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# 2. ƒê∆∞·ªùng d·∫´n ƒë·∫øn d·ªØ li·ªáu\n","drive_folder = \"/content/drive/MyDrive/kaggle_data/aptos2019\"\n","extract_root = \"/content/extracted_zip_files\"\n","os.makedirs(extract_root, exist_ok=True)\n","\n","# Gi·∫£i n√©n c√°c file ZIP n·∫øu ch∆∞a gi·∫£i (n·∫øu ƒë√£ gi·∫£i th√¨ b·ªè qua)\n","zip_files = glob.glob(os.path.join(drive_folder, \"*.zip\"))\n","for zip_path in zip_files:\n","    zip_name = os.path.basename(zip_path).replace(\".zip\", \"\")\n","    extract_path = os.path.join(extract_root, zip_name)\n","    os.makedirs(extract_path, exist_ok=True)\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        zip_ref.extractall(extract_path)\n","    print(f\"‚úÖ ƒê√£ gi·∫£i n√©n: {zip_path} ‚Üí {extract_path}\")\n","\n","# 3. ƒê·ªçc file CSV\n","df_train = pd.read_csv(os.path.join(drive_folder, \"train.csv\"))\n","df_test = pd.read_csv(os.path.join(drive_folder, \"test.csv\"))\n","\n","# 4. ƒê·ªãnh nghƒ©a h√†m x·ª≠ l√Ω ·∫£nh: c·∫Øt, resize v√† tƒÉng c∆∞·ªùng ·∫£nh\n","def crop_image_from_gray_to_color(img, tol=7):\n","    \"\"\"\n","    C·∫Øt b·ªè c√°c v√πng kh√¥ng c·∫ßn thi·∫øt (ƒë·∫∑c bi·ªát l√† c√°c c·∫°nh t·ªëi) c·ªßa ·∫£nh d·ª±a tr√™n th√¥ng tin t·ª´ ·∫£nh x√°m,\n","    sau ƒë√≥ √°p d·ª•ng v√πng c·∫Øt n√†y l√™n ·∫£nh m√†u g·ªëc.\n","    \"\"\"\n","    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","    mask = gray > tol\n","    if mask.sum() == 0:\n","        return img\n","    rows = mask.any(axis=1)\n","    cols = mask.any(axis=0)\n","    cropped_img = img[np.ix_(rows, cols)]\n","    return cropped_img\n","\n","def load_ben_color(path, sigmaX=10, IMG_SIZE=244):\n","    \"\"\"\n","    Load ·∫£nh t·ª´ ƒë∆∞·ªùng d·∫´n, c·∫Øt b·ªè bi√™n t·ªëi d·ª±a tr√™n ·∫£nh x√°m, resize v√† tƒÉng c∆∞·ªùng ·∫£nh b·∫±ng GaussianBlur.\n","    \"\"\"\n","    image = cv2.imread(path)\n","    if image is None:\n","        raise ValueError(f\"Kh√¥ng th·ªÉ ƒë·ªçc ƒë∆∞·ª£c ·∫£nh t·ª´ ƒë∆∞·ªùng d·∫´n: {path}\")\n","    # Chuy·ªÉn BGR sang RGB\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    # C·∫Øt ·∫£nh theo v√πng s√°ng\n","    image = crop_image_from_gray_to_color(image, tol=7)\n","    # Resize ·∫£nh v·ªÅ k√≠ch th∆∞·ªõc mong mu·ªën\n","    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n","    # TƒÉng c∆∞·ªùng ·∫£nh b·∫±ng GaussianBlur v√† weighted addition\n","    image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), sigmaX), -4, 128)\n","    return image\n","\n","# 5. X·ª≠ l√Ω v√† l∆∞u ·∫£nh ƒë√£ x·ª≠ l√Ω v√†o m·ªôt th∆∞ m·ª•c t·∫°m th·ªùi tr√™n Colab\n","train_img_folder = os.path.join(extract_root, \"train_images\")  # Th∆∞ m·ª•c ch·ª©a ·∫£nh g·ªëc\n","processed_folder = \"/content/processed_train_images\"          # Th∆∞ m·ª•c l∆∞u ·∫£nh ƒë√£ x·ª≠ l√Ω\n","os.makedirs(processed_folder, exist_ok=True)\n","\n","processed_ids = []  # L∆∞u l·∫°i id c·ªßa c√°c ·∫£nh ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng\n","\n","for idx, row in df_train.iterrows():\n","    img_filename = f\"{row['id_code']}.png\"\n","    img_path = os.path.join(train_img_folder, img_filename)\n","\n","    try:\n","        proc_img = load_ben_color(img_path, sigmaX=10, IMG_SIZE=244)\n","        # cv2.imwrite l∆∞u ·∫£nh theo ƒë·ªãnh d·∫°ng BGR n√™n chuy·ªÉn t·ª´ RGB sang BGR\n","        proc_img_bgr = cv2.cvtColor(proc_img, cv2.COLOR_RGB2BGR)\n","        save_path = os.path.join(processed_folder, img_filename)\n","        cv2.imwrite(save_path, proc_img_bgr)\n","        processed_ids.append(row['id_code'])\n","    except Exception as e:\n","        print(f\"L·ªói khi x·ª≠ l√Ω ·∫£nh {img_filename}: {e}\")\n","\n","print(f\"ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng {len(processed_ids)} ·∫£nh.\")\n","\n","# 6. C·∫≠p nh·∫≠t DataFrame ch·ªâ v·ªõi c√°c ·∫£nh ƒë√£ x·ª≠ l√Ω th√†nh c√¥ng\n","df_train_processed = df_train[df_train['id_code'].isin(processed_ids)].copy()\n","\n","# 7. Chia d·ªØ li·ªáu th√†nh t·∫≠p train v√† validation d·ª±a tr√™n file CSV\n","x = df_train_processed['id_code']\n","y = df_train_processed['diagnosis']\n","\n","# X√°o tr·ªôn d·ªØ li·ªáu ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh ng·∫´u nhi√™n\n","x, y = shuffle(x, y, random_state=42)\n","\n","# Chia t·∫≠p train+validation v√† test (80% - 20%)\n","x_temp, test_x, y_temp, test_y = train_test_split(x, y, test_size=0.20, stratify=y, random_state=42)\n","\n","# Chia t·∫≠p train v√† validation (85% train, 15% val trong 80% d·ªØ li·ªáu ban ƒë·∫ßu)\n","train_x, valid_x, train_y, valid_y = train_test_split(x_temp, y_temp, test_size=0.15/0.80, stratify=y_temp, random_state=42)\n","\n","# In th√¥ng tin ki·ªÉm tra\n","print(\"Train X size:\", len(train_x))\n","print(\"Train y size:\", len(train_y))\n","print(\"Valid X size:\", len(valid_x))\n","print(\"Valid y size:\", len(valid_y))\n","print(\"Test X size:\", len(test_x))\n","print(\"Test y size:\", len(test_y))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7923,"status":"ok","timestamp":1748584902570,"user":{"displayName":"Resconnect","userId":"00882996269091038835"},"user_tz":-420},"id":"ozYM5kEQ2YC-","outputId":"3f711bbc-4de9-4d18-d095-afccee206d60"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.7'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n","  check_for_updates()\n"]}],"source":["import matplotlib.pyplot as plt\n","import skimage.io\n","from skimage.transform import resize\n","import albumentations as A\n","from tqdm import tqdm\n","import PIL\n","from PIL import Image, ImageOps\n","import cv2\n","from sklearn.utils import class_weight, shuffle\n","from keras.losses import binary_crossentropy, categorical_crossentropy\n","\n","# Ti·ªÅn x·ª≠ l√Ω cho c√°c m√¥ h√¨nh kh√°c nhau\n","from keras.applications.resnet50 import preprocess_input as resnet50_preprocess\n","from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n","from keras.applications.densenet import preprocess_input as densenet_preprocess\n","from keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n","from keras.applications.xception import preprocess_input as xception_preprocess  # ƒê√£ thay th·∫ø\n","\n","import keras.backend as K\n","import tensorflow as tf\n","from sklearn.metrics import f1_score, fbeta_score, cohen_kappa_score, accuracy_score\n","from keras.utils import Sequence, to_categorical\n","from sklearn.model_selection import train_test_split\n","\n","# Import c√°c m√¥ h√¨nh CNN t·ª´ Keras Applications\n","from keras.applications import ResNet50, EfficientNetB0, InceptionV3, DenseNet121, Xception  # ƒê√£ thay th·∫ø MobileNetV2\n","\n","\n","WORKERS = 2\n","CHANNEL = 3\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","SIZE = 244\n","NUM_CLASSES = 5\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I8-k9wBi2YC-"},"outputs":[],"source":["import numpy as np\n","\n","def to_multi_label(target, num_classes=5):\n","    \"\"\" Chuy·ªÉn ƒë·ªïi nh√£n ƒë∆°n th√†nh multi-label (One-vs-All). \"\"\"\n","    return (np.arange(num_classes) < (target[:, None] + 1)).astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RHb02T12YC-"},"outputs":[],"source":["import tensorflow as tf\n","\n","class AdamAccumulate(tf.keras.optimizers.Adam):\n","    def __init__(self, accum_iters=2, **kwargs):\n","        \"\"\"\n","        accum_iters: s·ªë b∆∞·ªõc t√≠ch l≈©y gradient tr∆∞·ªõc khi c·∫≠p nh·∫≠t tr·ªçng s·ªë.\n","        kwargs: c√°c tham s·ªë kh√°c c·ªßa Adam (nh∆∞ learning_rate, beta_1, beta_2, epsilon, decay, v.v.)\n","        \"\"\"\n","        super(AdamAccumulate, self).__init__(**kwargs)\n","        if accum_iters < 1:\n","            raise ValueError('accum_iters ph·∫£i >= 1')\n","        self.accum_iters = accum_iters\n","        # ƒê·∫øm s·ªë batch ƒë√£ t√≠ch l≈©y\n","        self._accum_steps = 0\n","        # B·ªô nh·ªõ ch·ª©a gradient ƒë√£ t√≠ch l≈©y (dictionary: key l√† tham chi·∫øu c·ªßa bi·∫øn)\n","        self._grad_accum = {}\n","\n","    @tf.function\n","    def apply_gradients(self, grads_and_vars, name=None, experimental_aggregate_gradients=True):\n","        # N·∫øu ƒë√¢y l√† l·∫ßn ch·∫°y ƒë·∫ßu ti√™n, kh·ªüi t·∫°o bi·∫øn t√≠ch l≈©y cho m·ªói bi·∫øn\n","        if not self._grad_accum:\n","            for grad, var in grads_and_vars:\n","                if grad is None:\n","                    continue\n","                self._grad_accum[var.ref()] = tf.Variable(tf.zeros_like(var), trainable=False)\n","\n","        # T√≠ch l≈©y gradient cho m·ªói bi·∫øn\n","        for grad, var in grads_and_vars:\n","            if grad is None:\n","                continue\n","            self._grad_accum[var.ref()].assign_add(grad)\n","\n","        self._accum_steps += 1\n","\n","        # Ch·ªâ c·∫≠p nh·∫≠t tr·ªçng s·ªë khi s·ªë b∆∞·ªõc t√≠ch l≈©y ƒë·∫°t ƒë·∫øn accum_iters\n","        if self._accum_steps % self.accum_iters == 0:\n","            # T√≠nh trung b√¨nh gradient v√† t·∫°o danh s√°ch c·∫≠p nh·∫≠t\n","            avg_grads_and_vars = []\n","            for grad, var in grads_and_vars:\n","                if grad is None:\n","                    continue\n","                accumulated_grad = self._grad_accum[var.ref()]\n","                avg_grad = accumulated_grad / tf.cast(self.accum_iters, accumulated_grad.dtype)\n","                avg_grads_and_vars.append((avg_grad, var))\n","                # Reset l·∫°i gradient t√≠ch l≈©y cho bi·∫øn\n","                accumulated_grad.assign(tf.zeros_like(var))\n","            # G·ªçi ph∆∞∆°ng th·ª©c apply_gradients c·ªßa l·ªõp Adam g·ªëc ƒë·ªÉ c·∫≠p nh·∫≠t tr·ªçng s·ªë\n","            super(AdamAccumulate, self).apply_gradients(avg_grads_and_vars, name, experimental_aggregate_gradients)\n","        # N·∫øu ch∆∞a ƒë·ªß s·ªë b∆∞·ªõc, kh√¥ng c·∫≠p nh·∫≠t tr·ªçng s·ªë\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJWwUcTl2YC-"},"outputs":[],"source":["import albumentations as A\n","import numpy as np\n","import cv2\n","import os\n","from sklearn.utils import shuffle\n","from keras.utils import Sequence\n","\n","# Import preprocess_input t·ª´ c√°c m√¥ h√¨nh\n","from tensorflow.keras.applications.resnet50 import preprocess_input as resnet50_preprocess\n","from tensorflow.keras.applications.inception_v3 import preprocess_input as inception_preprocess\n","from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess\n","from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n","from tensorflow.keras.applications.xception import preprocess_input as xception_preprocess\n","\n","class My_Generator(Sequence):\n","    def __init__(self, image_filenames, labels, batch_size, is_train=False,\n","                 mix=False, augment=False, size1=224, size2=299, model_type=\"default\",\n","                 balance_classes=False):\n","        self.image_filenames = np.array(image_filenames)\n","        self.labels = np.array(labels)\n","        self.batch_size = batch_size\n","        self.is_train = is_train\n","        self.is_augment = augment\n","        self.is_mix = mix\n","        self.model_type = str(model_type).lower()\n","        self.n_classes = self.labels.shape[1] if self.labels.ndim > 1 else int(max(self.labels) + 1)\n","\n","        if \"inceptionv3\" in self.model_type:\n","            self.target_size = (size2, size2)\n","        else:\n","            self.target_size = (size1, size1)\n","\n","        self.base_path = \"/content/processed_train_images/\"\n","\n","        if self.is_augment and self.is_train:\n","            self.augmenter = A.Compose([\n","                A.OneOf([\n","                    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0, p=1),\n","                    A.MultiplicativeNoise(multiplier=(0.9, 1.1), per_channel=True, p=1),\n","                    A.RandomBrightnessContrast(brightness_limit=0, contrast_limit=0.1, p=1)\n","                ], p=0.5),\n","                A.HorizontalFlip(p=0.5),\n","                A.VerticalFlip(p=0.5),\n","                A.CropAndPad(percent=(-0.1, 0), p=0.5)\n","            ])\n","\n","        self.rare_augmenter = A.Compose([\n","            A.HorizontalFlip(p=0.5),\n","            A.VerticalFlip(p=0.5),\n","            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7),\n","            A.GaussNoise(p=0.5),\n","            A.Rotate(limit=30, p=0.5),\n","            A.RandomScale(scale_limit=0.2, p=0.5),\n","            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n","            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5)\n","        ])\n","\n","        self.class_counts = self._compute_initial_class_counts()\n","        self.augmented_class_counts = self.class_counts.copy()\n","        self.class_weights = None  # Kh·ªüi t·∫°o tr·ªçng s·ªë l·ªõp l√† None\n","\n","        if self.is_train and balance_classes:\n","            self.balance_classes()\n","\n","        if self.is_train:\n","            self.on_epoch_end()\n","\n","    def _compute_initial_class_counts(self):\n","        labels = np.argmax(self.labels, axis=1) if self.labels.ndim > 1 else self.labels\n","        return np.bincount(labels, minlength=self.n_classes)\n","\n","    def _compute_class_weights(self):\n","        total_samples = np.sum(self.augmented_class_counts)\n","        if total_samples == 0:\n","            return np.ones(self.n_classes)\n","        class_weights = total_samples / (self.n_classes * self.augmented_class_counts)\n","        class_weights = np.where(np.isinf(class_weights) | (self.augmented_class_counts == 0), 1.0, class_weights)\n","        return class_weights / np.min(class_weights[np.isfinite(class_weights)])\n","\n","    def get_class_weights(self):\n","        \"\"\"Tr·∫£ v·ªÅ tr·ªçng s·ªë l·ªõp hi·ªán t·∫°i ƒë·ªÉ s·ª≠ d·ª•ng trong hu·∫•n luy·ªán.\"\"\"\n","        return self.class_weights\n","\n","    def balance_classes(self):\n","        class_counts = self._compute_initial_class_counts()\n","        max_count = class_counts[0]  # S·ª≠ d·ª•ng s·ªë l∆∞·ª£ng m·∫´u c·ªßa l·ªõp 0 l√†m m·ª•c ti√™u\n","\n","        print(f\"S·ªë l∆∞·ª£ng m·∫´u ban ƒë·∫ßu: {class_counts}\")\n","        print(f\"S·ªë l∆∞·ª£ng m·∫´u m·ª•c ti√™u cho m·ªói l·ªõp (d·ª±a tr√™n l·ªõp 0): {max_count}\")\n","\n","        new_filenames = []\n","        new_labels = []\n","        for cls in range(self.n_classes):\n","            current_count = class_counts[cls]\n","            if current_count == 0:\n","                print(f\"L·ªõp {cls} kh√¥ng c√≥ m·∫´u, b·ªè qua.\")\n","                continue\n","            if cls == 3:\n","                target_count = int(max_count * 1.3)  # L·ªõp 3 ƒë∆∞·ª£c tƒÉng th√™m 30%\n","            else:\n","                target_count = max_count\n","            if current_count < target_count:\n","                samples_to_add = target_count - current_count\n","                label_indices = np.argmax(self.labels, axis=1) if self.labels.ndim > 1 else self.labels\n","                class_indices = np.where(label_indices == cls)[0]\n","                for i in range(samples_to_add):\n","                    idx = np.random.choice(class_indices)\n","                    img_id = self.image_filenames[idx]\n","                    label = self.labels[idx]\n","                    img = self._load_image(img_id)\n","                    if img is None:\n","                        continue\n","                    aug_img = self.rare_augmenter(image=img)['image']\n","                    new_img_id = f\"{img_id}_balance_aug_{i}\"\n","                    save_path = os.path.join(self.base_path, f\"{new_img_id}.png\")\n","                    if cv2.imwrite(save_path, cv2.cvtColor(aug_img, cv2.COLOR_RGB2BGR)):\n","                        new_filenames.append(new_img_id)\n","                        new_labels.append(np.array(label, dtype=self.labels.dtype))\n","                        self.augmented_class_counts[cls] += 1\n","                    else:\n","                        print(f\"L·ªói khi l∆∞u ·∫£nh tƒÉng c∆∞·ªùng {new_img_id}\")\n","\n","        if new_labels:\n","            new_labels_array = np.array(new_labels)\n","            if new_labels_array.ndim == 1:\n","                new_labels_array = new_labels_array[:, np.newaxis]\n","            self.image_filenames = np.concatenate([self.image_filenames, new_filenames])\n","            self.labels = np.concatenate([self.labels, new_labels_array])\n","\n","        # Kh√¥ng g·ªçi on_epoch_end() ·ªü ƒë√¢y ƒë·ªÉ tr√°nh t√≠nh tr·ªçng s·ªë l·ªõp ngay l·∫≠p t·ª©c\n","\n","        # Ki·ªÉm tra s·ªë l∆∞·ª£ng m·∫´u sau khi c√¢n b·∫±ng\n","        updated_counts = self._compute_initial_class_counts()\n","        print(f\"S·ªë l∆∞·ª£ng m·∫´u sau khi c√¢n b·∫±ng v√† tƒÉng l·ªõp 3: {updated_counts}\")\n","\n","    def augment_weak_classes(self, weak_classes, augment_factor=2):\n","        new_filenames = []\n","        new_labels = []\n","        for idx, label in enumerate(self.labels):\n","            label_class = np.argmax(label) if label.ndim > 1 else label\n","            if np.isscalar(label_class) and np.isin(label_class, weak_classes):\n","                img_id = self.image_filenames[idx]\n","                img = self._load_image(img_id)\n","                if img is None:\n","                    continue\n","                for i in range(augment_factor):\n","                    aug_img = self.rare_augmenter(image=img)['image']\n","                    new_img_id = f\"{img_id}_weak_aug_{i}\"\n","                    save_path = os.path.join(self.base_path, f\"{new_img_id}.png\")\n","                    if cv2.imwrite(save_path, cv2.cvtColor(aug_img, cv2.COLOR_RGB2BGR)):\n","                        new_filenames.append(new_img_id)\n","                        new_labels.append(np.array(label, dtype=self.labels.dtype))\n","                        self.augmented_class_counts[label_class] += 1\n","                    else:\n","                        print(f\"L·ªói khi l∆∞u ·∫£nh tƒÉng c∆∞·ªùng {new_img_id}\")\n","        if new_labels:\n","            new_labels_array = np.array(new_labels)\n","            if new_labels_array.ndim == 1:\n","                new_labels_array = new_labels_array[:, np.newaxis]\n","            self.image_filenames = np.concatenate([self.image_filenames, new_filenames])\n","            self.labels = np.concatenate([self.labels, new_labels_array])\n","\n","    def __len__(self):\n","        return int(np.ceil(len(self.image_filenames) / self.batch_size))\n","\n","    def __getitem__(self, idx):\n","        batch_x = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        return self._generate_batch(batch_x, batch_y, augment=self.is_train)\n","\n","    def on_epoch_end(self):\n","        if self.is_train:\n","            self.image_filenames, self.labels = shuffle(self.image_filenames, self.labels)\n","            # T√≠nh tr·ªçng s·ªë l·ªõp v√†o cu·ªëi m·ªói epoch\n","            self.class_weights = self._compute_class_weights()\n","            print(f\"Tr·ªçng s·ªë l·ªõp sau epoch: {self.class_weights}\")\n","            print(f\"S·ªë l∆∞·ª£ng m·∫´u tƒÉng c∆∞·ªùng: {self.augmented_class_counts}\")\n","\n","    def _load_image(self, img_id):\n","        img_path = os.path.join(self.base_path, f\"{img_id}.png\")\n","        try:\n","            img = cv2.imread(img_path)\n","            if img is None:\n","                raise ValueError(f\"H√¨nh ·∫£nh kh√¥ng t√¨m th·∫•y ho·∫∑c b·ªã h·ªèng: {img_path}\")\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","            img = cv2.resize(img, self.target_size)\n","            return img\n","        except Exception as e:\n","            print(f\"L·ªói khi t·∫£i h√¨nh ·∫£nh {img_id}: {str(e)}\")\n","            return None\n","\n","    def _generate_batch(self, batch_x, batch_y, augment=False):\n","        batch_images = []\n","        valid_labels = []\n","\n","        for img_id, label in zip(batch_x, batch_y):\n","            img = self._load_image(img_id)\n","            if img is None:\n","                continue\n","            if augment and self.is_augment:\n","                img = self.augmenter(image=img.astype(np.uint8))['image']\n","            img = img.astype(np.float32) / 255.0\n","\n","            if \"resnet50\" in self.model_type:\n","                img = resnet50_preprocess(img)\n","            elif \"efficientnetb0\" in self.model_type:\n","                img = efficientnet_preprocess(img)\n","            elif \"inceptionv3\" in self.model_type:\n","                img = inception_preprocess(img)\n","            elif \"densenet121\" in self.model_type:\n","                img = densenet_preprocess(img)\n","            elif \"xception\" in self.model_type:\n","                img = xception_preprocess(img)\n","\n","            batch_images.append(img)\n","            valid_labels.append(label)\n","\n","        if not batch_images:\n","            return np.zeros((1, *self.target_size, 3), dtype=np.float32), np.zeros((1, *batch_y.shape[1:]), dtype=np.float32)\n","\n","        batch_images = np.array(batch_images)\n","        valid_labels = np.array(valid_labels)\n","\n","        if self.is_mix and len(batch_images) > 1:\n","            batch_images, valid_labels = self._mixup(batch_images, valid_labels)\n","\n","        return batch_images, valid_labels\n","\n","    def _mixup(self, x, y):\n","        lam = np.random.beta(0.2, 0.4)\n","        index = np.random.permutation(len(x))\n","        mixed_x = np.zeros_like(x)\n","        mixed_y = np.zeros_like(y)\n","        for i in range(len(x)):\n","            if np.argmax(y[i]) == np.argmax(y[index[i]]):\n","                mixed_x[i] = lam * x[i] + (1 - lam) * x[index[i]]\n","                mixed_y[i] = y[i]\n","            else:\n","                mixed_x[i] = x[i]\n","                mixed_y[i] = y[i]\n","        return mixed_x, mixed_y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"34tqm68_2YC_"},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential, load_model\n","from keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n","                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D)\n","\n","from keras.applications.resnet50 import ResNet50\n","from keras.applications import EfficientNetB0\n","from keras.applications.inception_v3 import InceptionV3\n","from keras.applications.densenet import DenseNet121\n","from keras.applications.xception import Xception  # üîÑ Thay MobileNetV2 b·∫±ng Xception\n","\n","from keras.callbacks import ModelCheckpoint\n","from keras import metrics\n","from keras.optimizers import Adam\n","from keras import backend as K\n","import keras\n","from keras.models import Model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-W9xFeWB2YC_"},"outputs":[],"source":["def create_model(input_shape, n_out, model_type, weights_path=None, weights=\"imagenet\"):\n","    input_tensor = Input(shape=input_shape)\n","\n","    # Kh·ªüi t·∫°o m√¥ h√¨nh v·ªõi weights ho·∫∑c weights_path\n","    if model_type == \"resnet50\":\n","        base_model = ResNet50(include_top=False, weights=weights if not weights_path else None, input_tensor=input_tensor)\n","    elif model_type == \"efficientnetb0\":\n","        base_model = EfficientNetB0(include_top=False, weights=weights if not weights_path else None, input_tensor=input_tensor)\n","    elif model_type == \"inceptionv3\":\n","        base_model = InceptionV3(include_top=False, weights=weights if not weights_path else None, input_tensor=input_tensor)\n","    elif model_type == \"densenet121\":\n","        base_model = DenseNet121(include_top=False, weights=weights if not weights_path else None, input_tensor=input_tensor)\n","    elif model_type == \"xception\":  # ƒê·ªïi t·ª´ \"mobilenetv2\" th√†nh \"xception\"\n","        base_model = Xception(include_top=False, weights=weights if not weights_path else None, input_tensor=input_tensor)\n","\n","    else:\n","        raise ValueError(f\"Unsupported model type: {model_type}\")\n","\n","    # N·∫øu c√≥ weights_path, t·∫£i tr·ªçng s·ªë t·ª´ file\n","    if weights_path:\n","        try:\n","            base_model.load_weights(weights_path)\n","            print(f\"Loaded weights from {weights_path}\")\n","        except Exception as e:\n","            print(f\"Error loading weights from {weights_path}: {e}\")\n","            raise\n","\n","    x = GlobalAveragePooling2D(name='global_avg_pool')(base_model.output)\n","    x = Dropout(0.5)(x)\n","    x = Dense(1024, activation='relu')(x)\n","    x = Dropout(0.5)(x)\n","    final_output = Dense(n_out, activation=\"softmax\", name='final_output')(x)\n","\n","    model = Model(input_tensor, final_output)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ozVAHa0S2YC_"},"outputs":[],"source":["# C·∫•u h√¨nh m√¥ h√¨nh\n","model_configs = {\n","    # \"xception\": {\n","    #     \"model_type\": \"xception\",\n","    #     \"weights\": \"imagenet\",\n","    #     \"save_path\": \"/content/drive/MyDrive/working/Xception_bestqwk.h5\"\n","    # },\n","    \"resnet50\": {\n","        \"model_type\": \"resnet50\",\n","        \"weights_path\": \"/content/drive/MyDrive/keras_weights/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\",\n","        \"save_path\": \"/content/drive/MyDrive/working/ResNet50_bestqwk.h5\"\n","    },\n","    # \"efficientnetb0\": {\n","    #     \"model_type\": \"efficientnetb0\",\n","    #     \"weights\": \"imagenet\",\n","    #     \"save_path\": \"/content/drive/MyDrive/working/EfficientNetB1_bestqwk.h5\"\n","    # },\n","    # \"inceptionv3\": {\n","    #     \"model_type\": \"inceptionv3\",\n","    #     \"weights_path\": \"/content/drive/MyDrive/keras_weights/inceptionv3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\",\n","    #     \"save_path\": \"/content/drive/MyDrive/working/InceptionV3_bestqwk.h5\"\n","    # },\n","    \"densenet121\": {\n","        \"model_type\": \"densenet121\",\n","        \"weights_path\": \"/content/drive/MyDrive/keras_weights/densenet121/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\",\n","        \"save_path\": \"/content/drive/MyDrive/working/DenseNet121_bestqwk.h5\"\n","    }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":41,"status":"error","timestamp":1748842223055,"user":{"displayName":"Resconnect","userId":"00882996269091038835"},"user_tz":-420},"id":"SgxCVgEY2YC_","outputId":"fc63e827-0a98-48b8-90c8-51f1a1be6a31"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'create_model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-be20847cfa99>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;34m'train_x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'valid_x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_x ho·∫∑c valid_x kh√¥ng ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;34m'train_y'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'valid_y'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_y ho·∫∑c valid_y kh√¥ng ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"create_model kh√¥ng ph·∫£i l√† h√†m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;34m'My_Generator'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"My_Generator kh√¥ng ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'create_model' is not defined"]}],"source":["from sklearn.metrics import f1_score, recall_score, cohen_kappa_score, confusion_matrix\n","from datetime import datetime\n","import json\n","import os\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tensorflow.keras.callbacks import EarlyStopping, Callback\n","from tensorflow.keras.losses import CategoricalCrossentropy\n","import shutil\n","from sklearn.utils import resample\n","import subprocess\n","\n","# Class QWKEvaluation\n","class QWKEvaluation(tf.keras.callbacks.Callback):\n","    def __init__(self, validation_data=(), batch_size=64, interval=1, model_type=None, save_paths=None):\n","        super().__init__()\n","        self.interval = interval\n","        self.batch_size = batch_size\n","        self.valid_generator, self.y_val = validation_data\n","        self.history = []\n","        self.model_type = model_type\n","        self.save_paths = save_paths if save_paths is not None else {}\n","        self.save_path = self.save_paths.get(model_type, None)\n","        self.best_qwk = -float('inf')\n","        self.best_y_true = None\n","        self.best_y_pred = None\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        if epoch % self.interval == 0:\n","            steps = int(np.ceil(len(self.y_val) / self.batch_size))\n","            y_pred = self.model.predict(self.valid_generator, steps=steps, verbose=1)\n","\n","            if len(self.y_val.shape) > 1 and self.y_val.shape[1] > 1:\n","                y_true = np.argmax(self.y_val, axis=1)\n","                y_pred_classes = np.argmax(y_pred, axis=1)\n","            else:\n","                y_true = self.y_val.astype(int)\n","                y_pred_classes = np.argmax(y_pred, axis=1)\n","\n","            score = cohen_kappa_score(y_true, y_pred_classes, labels=[0, 1, 2, 3, 4], weights='quadratic')\n","            print(f\"\\nEpoch {epoch+1} - QWK: {score:.4f}\")\n","\n","            f1 = f1_score(y_true, y_pred_classes, average=None, labels=[0, 1, 2, 3, 4])\n","            sensitivity = recall_score(y_true, y_pred_classes, average=None, labels=[0, 1, 2, 3, 4])\n","            print(f\"F1-score per class: {f1}\")\n","            print(f\"Sensitivity per class: {sensitivity}\")\n","\n","            self.history.append(score)\n","\n","            if score > self.best_qwk:\n","                self.best_qwk = score\n","                self.best_y_true = y_true\n","                self.best_y_pred = y_pred_classes\n","                print(f\"New best QWK: {self.best_qwk:.4f} at Epoch {epoch+1}\")\n","\n","                if self.save_path:\n","                    keras_save_path = self.save_path.replace('.h5', '.keras')\n","                    save_dir = os.path.dirname(keras_save_path)\n","                    os.makedirs(save_dir, exist_ok=True)\n","\n","                    self.model.save(keras_save_path, overwrite=True)\n","                    print(f\"Saved (overwritten) full model to {keras_save_path}\")\n","\n","                    save_dir = self.save_path.replace('.h5', '')\n","                    os.makedirs(save_dir, exist_ok=True)\n","\n","                    model_json = self.model.to_json()\n","                    config_path = os.path.join(save_dir, \"config.json\")\n","                    with open(config_path, \"w\") as json_file:\n","                        json_file.write(model_json)\n","                    print(f\"Saved model architecture to {config_path}\")\n","\n","                    weights_path = os.path.join(save_dir, \"model.weights.h5\")\n","                    self.model.save_weights(weights_path)\n","                    print(f\"Saved model weights to {weights_path}\")\n","\n","                    metadata = {\n","                        \"keras_version\": tf.keras.__version__,\n","                        \"save_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n","                        \"model_type\": self.model_type\n","                    }\n","                    metadata_path = os.path.join(save_dir, \"metadata.json\")\n","                    with open(metadata_path, \"w\") as meta_file:\n","                        json.dump(metadata, meta_file)\n","                    print(f\"Saved metadata to {metadata_path}\")\n","\n","# Class QWKReduceLROnPlateau\n","class QWKReduceLROnPlateau(tf.keras.callbacks.Callback):\n","    def __init__(self, qwk_callback, factor=0.5, patience=3, min_lr=1e-6, verbose=1):\n","        super().__init__()\n","        self.qwk_callback = qwk_callback\n","        self.factor = factor\n","        self.patience = patience\n","        self.min_lr = min_lr\n","        self.verbose = verbose\n","        self.best_qwk = -float('inf')\n","        self.wait = 0\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        current_qwk = self.qwk_callback.history[-1] if self.qwk_callback.history else -float('inf')\n","        if current_qwk > self.best_qwk:\n","            self.best_qwk = current_qwk\n","            self.wait = 0\n","        else:\n","            self.wait += 1\n","            if self.wait >= self.patience:\n","                old_lr = float(self.model.optimizer.learning_rate)\n","                if old_lr > self.min_lr:\n","                    new_lr = max(old_lr * self.factor, self.min_lr)\n","                    self.model.optimizer.learning_rate.assign(new_lr)\n","                    if self.verbose > 0:\n","                        print(f\"\\nEpoch {epoch+1}: QWKReduceLROnPlateau reducing learning rate to {new_lr:.6f}.\")\n","                    self.wait = 0\n","\n","# Class DynamicRareClassAugmentationCallback\n","class DynamicRareClassAugmentationCallback(Callback):\n","    def __init__(self, train_generator, valid_generator, valid_labels, threshold=0.6, augment_factor=2):\n","        super().__init__()\n","        self.train_generator = train_generator\n","        self.valid_generator = valid_generator\n","        self.valid_labels = valid_labels\n","        self.threshold = threshold\n","        self.augment_factor = augment_factor\n","        self.f1_history = []\n","        self.batch_size = self.train_generator.batch_size\n","        self.num_classes = self.train_generator.n_classes\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        steps = int(np.ceil(len(self.valid_labels) / self.batch_size))\n","        y_pred = self.model.predict(self.valid_generator, steps=steps, verbose=1)\n","        y_true = np.argmax(self.valid_labels, axis=1)\n","        y_pred_classes = np.argmax(y_pred, axis=1)\n","\n","        f1_scores = f1_score(y_true, y_pred_classes, average=None, labels=list(range(self.num_classes)))\n","        print(f\"F1-scores at epoch {epoch+1}: {f1_scores}\")\n","\n","        self.f1_history.append(f1_scores)\n","\n","        weak_classes = [i for i, f1 in enumerate(f1_scores) if f1 < self.threshold]\n","        print(f\"Weak classes at epoch {epoch+1} (F1 < {self.threshold}): {weak_classes}\")\n","\n","        if weak_classes:\n","            self.train_generator.augment_weak_classes(weak_classes, augment_factor=self.augment_factor)\n","            print(f\"Augmented {self.augment_factor} samples for weak classes: {weak_classes}\")\n","\n","            self.train_generator.on_epoch_end()\n","            print(f\"Updated class weights: {self.train_generator.get_class_weights()}\")\n","\n","        if len(self.f1_history) > 1:\n","            prev_f1 = self.f1_history[-2]\n","            curr_f1 = self.f1_history[-1]\n","            print(f\"F1-score comparison (epoch {epoch} vs {epoch+1}):\")\n","            for i in range(self.num_classes):\n","                print(f\"Class {i}: {prev_f1[i]:.4f} -> {curr_f1[i]:.4f} (Change: {curr_f1[i] - prev_f1[i]:.4f})\")\n","\n","# Class ƒë·ªÉ thu th·∫≠p v√† v·∫Ω loss\n","class LossHistoryCallback(Callback):\n","    def __init__(self):\n","        super().__init__()\n","        self.losses = []\n","        self.val_losses = []\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.losses.append(logs.get('loss'))\n","        self.val_losses.append(logs.get('val_loss'))\n","\n","    def plot_and_save_loss(self, model_type, save_dir=\"/content/drive/MyDrive/working/\"):\n","        plt.figure(figsize=(10, 6))\n","        plt.plot(self.losses, label='Training Loss', marker='o')\n","        plt.plot(self.val_losses, label='Validation Loss', marker='s')\n","        plt.title(f'Training and Validation Loss - {model_type}')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Loss')\n","        plt.legend()\n","        plt.grid(True)\n","\n","        os.makedirs(save_dir, exist_ok=True)\n","        save_path = os.path.join(save_dir, f'loss_plot_{model_type}.png')\n","        plt.savefig(save_path)\n","        plt.close()\n","        print(f\"Saved loss plot to {save_path}\")\n","\n","# Chuy·ªÉn ƒë·ªïi nh√£n\n","NUM_CLASSES = 5\n","SIZE = 244\n","if len(train_y.shape) == 1 or train_y.shape[1] != NUM_CLASSES:\n","    train_y_multi = tf.keras.utils.to_categorical(train_y, num_classes = NUM_CLASSES)\n","    valid_y_multi = tf.keras.utils.to_categorical(valid_y, num_classes = NUM_CLASSES)\n","else:\n","    train_y_multi = train_y\n","    valid_y_multi = valid_y\n","\n","batch_size = 64\n","resized_train_x = train_x.values\n","resized_valid_x = valid_x.values\n","\n","# ƒê·ªãnh nghƒ©a callback\n","early_stopping = EarlyStopping(monitor='accuracy', patience=7, restore_best_weights=True, verbose=1, mode='max')\n","\n","# C·∫•u h√¨nh m√¥ h√¨nh\n","# C·∫•u h√¨nh m√¥ h√¨nh\n","model_configs = {\n","    \"xception\": {\n","        \"model_type\": \"xception\",\n","        \"weights\": \"imagenet\",\n","        \"save_path\": \"/content/drive/MyDrive/working/Xception_bestqwk_aptos.h5\"\n","    },\n","    \"resnet50\": {\n","        \"model_type\": \"resnet50\",\n","        \"weights_path\": \"/content/drive/MyDrive/keras_weights/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\",\n","        \"save_path\": \"/content/drive/MyDrive/working/ResNet50_bestqwk_aptos.h5\"\n","    },\n","    \"efficientnetb0\": {\n","        \"model_type\": \"efficientnetb0\",\n","        \"weights\": \"imagenet\",\n","        \"save_path\": \"/content/drive/MyDrive/working/EfficientNetB1_bestqwk_aptos.h5\"\n","    },\n","    \"inceptionv3\": {\n","        \"model_type\": \"inceptionv3\",\n","        \"weights_path\": \"/content/drive/MyDrive/keras_weights/inceptionv3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\",\n","        \"save_path\": \"/content/drive/MyDrive/working/InceptionV3_bestqwk_aptos.h5\"\n","    },\n","    \"densenet121\": {\n","        \"model_type\": \"densenet121\",\n","        \"weights_path\": \"/content/drive/MyDrive/keras_weights/densenet121/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\",\n","        \"save_path\": \"/content/drive/MyDrive/working/DenseNet121_bestqwk_aptos.h5\"\n","    }\n","}\n","\n","# Ki·ªÉm tra tr∆∞·ªõc khi hu·∫•n luy·ªán\n","assert 'train_x' in globals() and 'valid_x' in globals(), \"train_x ho·∫∑c valid_x kh√¥ng ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\"\n","assert 'train_y' in globals() and 'valid_y' in globals(), \"train_y ho·∫∑c valid_y kh√¥ng ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\"\n","assert callable(create_model), \"create_model kh√¥ng ph·∫£i l√† h√†m\"\n","assert 'My_Generator' in globals(), \"My_Generator kh√¥ng ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\"\n","\n","# In th√¥ng tin d·ªØ li·ªáu ƒë·ªÉ debug\n","print(\"train_y shape:\", train_y.shape)\n","print(\"valid_y shape:\", valid_y.shape)\n","print(\"train_y_multi shape:\", train_y_multi.shape)\n","print(\"valid_y_multi shape:\", valid_y_multi.shape)\n","\n","# V√≤ng l·∫∑p hu·∫•n luy·ªán\n","for model_name, config in model_configs.items():\n","    print(f\"\\n==> ƒêang hu·∫•n luy·ªán m√¥ h√¨nh {model_name} ...\")\n","\n","    if config[\"model_type\"] == \"inceptionv3\":\n","        model_input_shape = (299, 299, 3)\n","        img_size = 299\n","    else:\n","        model_input_shape = (SIZE, SIZE, 3)\n","        img_size = SIZE\n","\n","    # T·∫°o generator v·ªõi balance_classes\n","    train_generator = My_Generator(\n","        resized_train_x, train_y_multi, batch_size,\n","        is_train=True, mix=False, augment=True,\n","        size1=SIZE, size2=299, model_type=config[\"model_type\"],\n","        balance_classes=True\n","    )\n","\n","    # Ki·ªÉm tra s·ªë l∆∞·ª£ng m·∫´u\n","    try:\n","        print(f\"S·ªë l∆∞·ª£ng m·∫´u: {train_generator.augmented_class_counts}\")\n","    except AttributeError:\n","        print(\"Kh√¥ng truy c·∫≠p ƒë∆∞·ª£c augmented_class_counts, ti·∫øp t·ª•c hu·∫•n luy·ªán...\")\n","\n","    valid_generator = My_Generator(\n","        resized_valid_x, valid_y_multi, batch_size,\n","        is_train=False, size1=SIZE, size2=299, model_type=config[\"model_type\"]\n","    )\n","\n","    # L·∫•y tr·ªçng s·ªë l·ªõp ban ƒë·∫ßu (s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t sau m·ªói epoch)\n","    class_weights = train_generator.get_class_weights()\n","    if class_weights is None:\n","        class_weights = np.ones(NUM_CLASSES)  # M·∫∑c ƒë·ªãnh n·∫øu ch∆∞a t√≠nh\n","    class_weight = {i: float(w) for i, w in enumerate(class_weights)}\n","    print(f\"Tr·ªçng s·ªë l·ªõp ban ƒë·∫ßu: {class_weight}\")\n","\n","    # T·∫°o m√¥ h√¨nh\n","    weights_path = config.get(\"weights_path\", None)\n","    pretrained_weights = config.get(\"weights\", \"imagenet\")\n","    model = create_model(\n","        input_shape=model_input_shape,\n","        n_out=NUM_CLASSES,\n","        model_type=config[\"model_type\"],\n","        weights_path=weights_path,\n","        weights=pretrained_weights\n","    )\n","\n","    # Kh·ªüi t·∫°o QWKEvaluation\n","    qwk_callback = QWKEvaluation(\n","        validation_data=(valid_generator, valid_y_multi),\n","        batch_size=batch_size,\n","        interval=1,\n","        model_type=config[\"model_type\"],\n","        save_paths={config[\"model_type\"]: config[\"save_path\"]}\n","    )\n","\n","    # Kh·ªüi t·∫°o QWKReduceLROnPlateau\n","    qwk_reduce_lr = QWKReduceLROnPlateau(\n","        qwk_callback=qwk_callback,\n","        factor=0.5,\n","        patience=3,\n","        min_lr=1e-6,\n","        verbose=1\n","    )\n","\n","    # Kh·ªüi t·∫°o LossHistoryCallback\n","    loss_history = LossHistoryCallback()\n","\n","    # Giai ƒëo·∫°n kh·ªüi ƒë·ªông (warm-up) - KH√îNG d√πng class_weight\n","    for layer in model.layers[:50]:\n","        layer.trainable = False\n","    for layer in model.layers[50:]:\n","        layer.trainable = True\n","\n","    # S·ª≠ d·ª•ng CategoricalCrossentropy v·ªõi label smoothing\n","    loss_fn = CategoricalCrossentropy(label_smoothing=0.1)\n","    model.compile(\n","        loss=loss_fn,\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n","        metrics=['accuracy']\n","    )\n","\n","    augment_callback = DynamicRareClassAugmentationCallback(\n","        train_generator=train_generator,\n","        valid_generator=valid_generator,\n","        valid_labels=valid_y_multi,\n","        threshold=0.6,\n","        augment_factor=2\n","    )\n","\n","    # Hu·∫•n luy·ªán kh·ªüi ƒë·ªông (Kh√¥ng √°p d·ª•ng class_weight)\n","    model.fit(\n","        train_generator,\n","        steps_per_epoch=int(np.ceil(len(train_generator.image_filenames) / batch_size)),\n","        epochs=5,\n","        validation_data=valid_generator,\n","        validation_steps=int(np.ceil(len(valid_x) / batch_size)),\n","        verbose=1,\n","        callbacks=[qwk_callback, qwk_reduce_lr, early_stopping, augment_callback, loss_history]\n","    )\n","\n","    # Hu·∫•n luy·ªán ƒë·∫ßy ƒë·ªß\n","    for layer in model.layers:\n","        layer.trainable = True\n","\n","    model.compile(\n","        loss=CategoricalCrossentropy(label_smoothing=0.1),\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n","        metrics=['accuracy']\n","    )\n","\n","    train_mixup = My_Generator(\n","        resized_train_x, train_y_multi, batch_size,\n","        is_train=True, mix=True, augment=True,\n","        size1=SIZE, size2=299, model_type=config[\"model_type\"],\n","        balance_classes=True\n","    )\n","\n","    # Ki·ªÉm tra s·ªë l∆∞·ª£ng m·∫´u\n","    try:\n","        print(f\"S·ªë l∆∞·ª£ng m·∫´u (mixup): {train_mixup.augmented_class_counts}\")\n","    except AttributeError:\n","        print(\"Kh√¥ng truy c·∫≠p ƒë∆∞·ª£c augmented_class_counts (mixup), ti·∫øp t·ª•c hu·∫•n luy·ªán...\")\n","\n","    augment_callback = DynamicRareClassAugmentationCallback(\n","        train_generator=train_mixup,\n","        valid_generator=valid_generator,\n","        valid_labels=valid_y_multi,\n","        threshold=0.6,\n","        augment_factor=2\n","    )\n","\n","    # Hu·∫•n luy·ªán ƒë·∫ßy ƒë·ªß v·ªõi v√≤ng l·∫∑p epoch ƒë·ªÉ c·∫≠p nh·∫≠t class_weight\n","    epochs = 30\n","    for epoch in range(epochs):\n","        print(f\"\\nB·∫Øt ƒë·∫ßu epoch {epoch + 1} cho m√¥ h√¨nh {model_name}\")\n","        # L·∫•y tr·ªçng s·ªë l·ªõp cho epoch hi·ªán t·∫°i\n","        class_weights = train_mixup.get_class_weights()\n","        if class_weights is None:\n","            class_weights = np.ones(NUM_CLASSES)  # M·∫∑c ƒë·ªãnh n·∫øu ch∆∞a t√≠nh\n","        class_weight = {i: float(w) for i, w in enumerate(class_weights)}\n","        print(f\"Tr·ªçng s·ªë l·ªõp cho epoch {epoch + 1}: {class_weight}\")\n","\n","        # Hu·∫•n luy·ªán m·ªôt epoch (√Åp d·ª•ng class_weight)\n","        model.fit(\n","            train_mixup,\n","            steps_per_epoch=int(np.ceil(len(train_mixup.image_filenames) / batch_size)),\n","            epochs=1,  # Hu·∫•n luy·ªán t·ª´ng epoch\n","            validation_data=valid_generator,\n","            validation_steps=int(np.ceil(len(valid_x) / batch_size)),\n","            verbose=1,\n","            callbacks=[qwk_callback, qwk_reduce_lr, augment_callback, early_stopping, loss_history],\n","            class_weight=class_weight  # √Åp d·ª•ng class_weight\n","        )\n","\n","    # L∆∞u m√¥ h√¨nh cu·ªëi c√πng\n","    final_save_path = config[\"save_path\"].replace('.h5', '_final.keras')\n","    model.save(final_save_path, overwrite=True)\n","    print(f\"ƒê√£ l∆∞u m√¥ h√¨nh cu·ªëi c√πng t·∫°i {final_save_path}\")\n","\n","    # V·∫Ω v√† l∆∞u bi·ªÉu ƒë·ªì loss\n","    loss_history.plot_and_save_loss(config[\"model_type\"], save_dir=\"/content/drive/MyDrive/working/\")\n","\n","    # V·∫Ω v√† l∆∞u bi·ªÉu ƒë·ªì ma tr·∫≠n nh·∫ßm l·∫´n t·ªët nh·∫•t cho m√¥ h√¨nh n√†y\n","    if qwk_callback.best_y_true is not None and qwk_callback.best_y_pred is not None:\n","        cm = confusion_matrix(qwk_callback.best_y_true, qwk_callback.best_y_pred, labels=[0, 1, 2, 3, 4])\n","        plt.figure(figsize=(8, 6))\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                    xticklabels=[0, 1, 2, 3, 4],\n","                    yticklabels=[0, 1, 2, 3, 4])\n","        plt.title(f'Best Confusion Matrix - QWK: {qwk_callback.best_qwk:.4f} ({config[\"model_type\"]})')\n","        plt.xlabel('Predicted')\n","        plt.ylabel('True')\n","        save_cm_path = f\"/content/drive/MyDrive/working/best_confusion_matrix_{config['model_type']}.png\"\n","        plt.savefig(save_cm_path)\n","        plt.close()  # ƒê√≥ng figure ƒë·ªÉ tr√°nh chi·∫øm b·ªô nh·ªõ\n","        print(f\"Saved best confusion matrix to {save_cm_path}\")\n","    else:\n","        print(f\"Kh√¥ng c√≥ best QWK ƒë∆∞·ª£c ghi nh·∫≠n cho m√¥ h√¨nh {config['model_type']}, kh√¥ng v·∫Ω bi·ªÉu ƒë·ªì.\")\n","\n","    #TEST\n","    import numpy as np\n","    import tensorflow as tf\n","    import seaborn as sns\n","    import matplotlib.pyplot as plt\n","    from sklearn.metrics import cohen_kappa_score, f1_score, recall_score, precision_score, accuracy_score, confusion_matrix\n","\n","    # ... (Gi·∫£ s·ª≠ c√°c bi·∫øn nh∆∞ model, test_x, test_y, batch_size, NUM_CLASSES, config, My_Generator ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a tr∆∞·ªõc ƒë√≥)\n","\n","    print(f\"\\n==> ƒêang ki·ªÉm tra m√¥ h√¨nh {model_name} tr√™n t·∫≠p test ...\")\n","\n","    # Chuy·ªÉn nh√£n test_y sang d·∫°ng one-hot n·∫øu c·∫ßn\n","    if len(test_y.shape) == 1 or test_y.shape[1] != NUM_CLASSES:\n","        test_y_multi = tf.keras.utils.to_categorical(test_y, num_classes=NUM_CLASSES)\n","    else:\n","        test_y_multi = test_y\n","\n","    # T·∫°o generator cho t·∫≠p test\n","    test_generator = My_Generator(\n","        test_x.values, test_y_multi, batch_size,\n","        is_train=False, size1=SIZE, size2=299, model_type=config[\"model_type\"]\n","    )\n","\n","    # D·ª± ƒëo√°n tr√™n t·∫≠p test\n","    steps_test = int(np.ceil(len(test_x) / batch_size))\n","    y_pred_test = model.predict(test_generator, steps=steps_test, verbose=1)\n","\n","    # Chuy·ªÉn d·ª± ƒëo√°n v√† nh√£n th·∫≠t sang d·∫°ng l·ªõp (class indices)\n","    y_true_test = np.argmax(test_y_multi, axis=1)\n","    y_pred_classes_test = np.argmax(y_pred_test, axis=1)\n","\n","    # T√≠nh Quadratic Weighted Kappa (QWK)\n","    qwk_test = cohen_kappa_score(y_true_test, y_pred_classes_test, labels=[0, 1, 2, 3, 4], weights='quadratic')\n","    print(f\"QWK tr√™n t·∫≠p test: {qwk_test:.4f}\")\n","\n","    # T√≠nh ƒë·ªô ch√≠nh x√°c t·ªïng th·ªÉ (Accuracy)\n","    accuracy_test = accuracy_score(y_true_test, y_pred_classes_test)\n","    print(f\"ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p test: {accuracy_test:.4f}\")\n","\n","    # T√≠nh F1-score v√† ƒë·ªô nh·∫°y (Sensitivity/Recall) cho t·ª´ng l·ªõp\n","    f1_test = f1_score(y_true_test, y_pred_classes_test, average=None, labels=[0, 1, 2, 3, 4])\n","    sensitivity_test = recall_score(y_true_test, y_pred_classes_test, average=None, labels=[0, 1, 2, 3, 4])\n","    print(f\"F1-score cho t·ª´ng l·ªõp tr√™n t·∫≠p test: {[f'{f1:.4f}' for f1 in f1_test]}\")\n","    print(f\"ƒê·ªô nh·∫°y cho t·ª´ng l·ªõp tr√™n t·∫≠p test: {[f'{sens:.4f}' for sens in sensitivity_test]}\")\n","\n","    # T√≠nh ƒë·ªô ƒë·∫∑c hi·ªáu (Specificity) cho t·ª´ng l·ªõp\n","    specificity_test = []\n","    cm = confusion_matrix(y_true_test, y_pred_classes_test, labels=[0, 1, 2, 3, 4])\n","    for cls in range(NUM_CLASSES):\n","        # Specificity = TN / (TN + FP)\n","        tn = np.sum(cm) - np.sum(cm[cls, :]) - np.sum(cm[:, cls]) + cm[cls, cls]\n","        fp = np.sum(cm[:, cls]) - cm[cls, cls]\n","        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n","        specificity_test.append(specificity)\n","    print(f\"ƒê·ªô ƒë·∫∑c hi·ªáu cho t·ª´ng l·ªõp tr√™n t·∫≠p test: {[f'{spec:.4f}' for spec in specificity_test]}\")\n","\n","    # T√≠nh ƒë·ªô ch√≠nh x√°c (Precision) cho t·ª´ng l·ªõp\n","    precision_test = precision_score(y_true_test, y_pred_classes_test, average=None, labels=[0, 1, 2, 3, 4])\n","    print(f\"ƒê·ªô ch√≠nh x√°c cho t·ª´ng l·ªõp tr√™n t·∫≠p test: {[f'{prec:.4f}' for prec in precision_test]}\")\n","\n","    # V·∫Ω v√† l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n cho t·∫≠p test\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=[0, 1, 2, 3, 4],\n","                yticklabels=[0, 1, 2, 3, 4])\n","    plt.title(f'Ma tr·∫≠n nh·∫ßm l·∫´n t·∫≠p test - QWK: {qwk_test:.4f} ({config[\"model_type\"]})')\n","    plt.xlabel('D·ª± ƒëo√°n')\n","    plt.ylabel('Th·∫≠t')\n","    save_cm_test_path = f\"/content/drive/MyDrive/working/test_confusion_matrix_{config['model_type']}.png\"\n","    plt.savefig(save_cm_test_path)\n","    plt.close()\n","    print(f\"ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n t·∫≠p test t·∫°i {save_cm_test_path}\")"]},{"cell_type":"markdown","metadata":{"id":"F8ImhblXS1AO"},"source":["**Hu·∫•n luy·ªán meta-learner**"]},{"cell_type":"markdown","source":[],"metadata":{"id":"PmKhX5uMYqQ-"}},{"cell_type":"markdown","metadata":{"id":"YuTdef73gE0e"},"source":["m·∫´u 2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"id":"LblX7kriIIs2","outputId":"ac86ac35-1839-46e9-f5ed-f120c875d53b","executionInfo":{"status":"error","timestamp":1749025283024,"user_tz":-420,"elapsed":253066,"user":{"displayName":"Resconnect","userId":"00882996269091038835"}}},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["T√¨m th·∫•y 3662 ID ·∫£nh trong /content/processed_train_images\n","M·∫´u ID: ['cb2f3c5d71a7', '5c6194562ed2', 'e893e86dde94', '58059e73d2d4', '8ee50c26fc13']...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'loss_scale_optimizer', because it has 436 variables whereas the saved optimizer has 432 variables. \n","  saveable.load_own_variables(weights_store.get(inner_path))\n","/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 432 variables whereas the saved optimizer has 0 variables. \n","  saveable.load_own_variables(weights_store.get(inner_path))\n","ERROR:root:L·ªói trong episode 1: 'NoneType' object has no attribute 'save_weights'\n","ERROR:root:L·ªói trong episode 2: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 550) + inhomogeneous part.\n","ERROR:root:L·ªói trong episode 3: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 550) + inhomogeneous part.\n","ERROR:root:L·ªói trong episode 4: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 550) + inhomogeneous part.\n","ERROR:root:L·ªói trong episode 5: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 550) + inhomogeneous part.\n","ERROR:root:L·ªói trong episode 6: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 550) + inhomogeneous part.\n","ERROR:root:L·ªói trong episode 7: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 550) + inhomogeneous part.\n","ERROR:root:L·ªói trong episode 8: 'NoneType' object has no attribute 'save_weights'\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-6e4e0df9f6b7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2246\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m     \u001b[0;31m# Ch·∫°y pipeline v·ªõi t√πy ch·ªçn hyperparameter tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2248\u001b[0;31m     \u001b[0mtrain_features_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_features_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-6e4e0df9f6b7>\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(tune_hyperparameters, use_gpu)\u001b[0m\n\u001b[1;32m   2201\u001b[0m         \u001b[0;31m# Ch·∫°y pipeline ch√≠nh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ch·∫°y pipeline ch√≠nh...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m         pipeline_results = main_pipeline(\n\u001b[0m\u001b[1;32m   2204\u001b[0m             \u001b[0mbalanced_train_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m             \u001b[0mbalanced_train_y_multi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-6e4e0df9f6b7>\u001b[0m in \u001b[0;36mmain_pipeline\u001b[0;34m(balanced_train_x, balanced_train_y_multi, valid_images, valid_y_multi, test_images, test_y_multi, df_train_processed, train_ids, valid_ids, test_ids, balanced_train_ids, processed_image_ids)\u001b[0m\n\u001b[1;32m   2025\u001b[0m                 \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2026\u001b[0m             )\n\u001b[0;32m-> 2027\u001b[0;31m             meta_model, history = maml_fomaml_train_manual(\n\u001b[0m\u001b[1;32m   2028\u001b[0m                 \u001b[0mmeta_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2029\u001b[0m                 \u001b[0mdata_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_train_processed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-6e4e0df9f6b7>\u001b[0m in \u001b[0;36mmaml_fomaml_train_manual\u001b[0;34m(meta_model, data_df, valid_features, valid_labels, valid_images, input_dim, model_type, n_episodes, inner_lr, outer_lr, fine_tune_lr, support_size, query_size, save_dir, sample_ids, callbacks)\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0;31m# G·ªçi callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m                 \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"qwk\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mqwk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"f1\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0;31m# L∆∞u confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-6e4e0df9f6b7>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0my_pred_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    498\u001b[0m     ):\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# Create an iterator that yields batches of input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         epoch_iterator = TFEpochIterator(\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribute_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tf_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m             dataset = self._distribute_strategy.experimental_distribute_dataset(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001b[0m in \u001b[0;36mget_tf_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             dataset = tf.data.Dataset.zip(\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             )\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensors_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_tensors_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m     \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_tensors_op.py\u001b[0m in \u001b[0;36m_from_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_from_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-private-name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_TensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_tensors_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element, name)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     variant_tensor = gen_dataset_ops.tensor_dataset(\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moutput_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flat_tensor_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mtensor_dataset\u001b[0;34m(components, output_shapes, metadata, name)\u001b[0m\n\u001b[1;32m   7709\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7710\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7711\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   7712\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TensorDataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7713\u001b[0m         output_shapes, \"metadata\", metadata)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import tensorflow as tf\n","import logging\n","\n","# C·∫•u h√¨nh logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","# C·∫•u h√¨nh GPU tr∆∞·ªõc khi TensorFlow kh·ªüi t·∫°o\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    for device in physical_devices:\n","        try:\n","            tf.config.experimental.set_memory_growth(device, True)\n","            logging.info(f\"ƒê√£ b·∫≠t memory growth cho {device}\")\n","        except Exception as e:\n","            logging.error(f\"L·ªói khi c√†i ƒë·∫∑t memory growth cho {device}: {str(e)}\")\n","\n","import numpy as np\n","import tensorflow as tf\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import cohen_kappa_score, confusion_matrix, f1_score, recall_score, classification_report, roc_curve, auc, precision_recall_curve\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.utils import shuffle\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import EfficientNetB1, Xception, InceptionV3, ResNet50, DenseNet121\n","from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n","from tensorflow.keras.applications.xception import preprocess_input as xception_preprocess\n","from tensorflow.keras.applications.inception_v3 import preprocess_input as inceptionv3_preprocess\n","from tensorflow.keras.applications.resnet50 import preprocess_input as resnet50_preprocess\n","from tensorflow.keras.applications.densenet import preprocess_input as densenet121_preprocess\n","from tensorflow.keras.layers import Layer, Input, Conv2D, BatchNormalization, GlobalAveragePooling2D, Dense, Dropout\n","from tensorflow.keras.models import Model, model_from_json  # Th√™m model_from_json\n","from tensorflow.keras.callbacks import Callback\n","from pathlib import Path\n","import os\n","import gc\n","import psutil\n","import logging\n","import cv2\n","import json\n","import time\n","import random\n","from itertools import product\n","from sklearn.calibration import calibration_curve\n","from datetime import datetime\n","import albumentations as A\n","import GPUtil\n","\n","# Thi·∫øt l·∫≠p logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logging.info(f\"Pipeline kh·ªüi ƒë·ªông l√∫c: {datetime.now().strftime('%H:%M %p %z, %d/%m/%Y')}\")\n","\n","# Tham s·ªë c·ªë ƒë·ªãnh\n","FEATURE_SAVE_DIR = \"/content/drive/MyDrive/working\"\n","PROCESSED_FOLDER = \"/content/processed_train_images\"\n","TEMP_AUGMENT_DIR = \"/content/temp_augment\"\n","DRIVE_FOLDER = \"/content/drive/MyDrive/kaggle_data/aptos2019\"\n","SIZE = 244\n","NUM_CLASSES = 5\n","BATCH_SIZE = 16\n","SUPPORT_SET_SIZE = 10\n","QUERY_SET_SIZE = 10\n","\n","# ƒê·ªãnh nghƒ©a model_configs\n","MODEL_CONFIGS = {\n","    \"efficientnetb1\": {\n","        \"model_type\": \"efficientnetb1\",\n","        \"config_path\": \"/content/drive/MyDrive/working/EfficientNetB1_bestqwk_aptos/config.json\",\n","        \"weights_path\": \"/content/drive/MyDrive/working/EfficientNetB1_bestqwk_aptos/model.weights.h5\",\n","        \"preprocess\": efficientnet_preprocess,\n","        \"img_size\": 244,  # S·ª≠a t·ª´ 244 th√†nh 224\n","        \"base_model\": EfficientNetB1\n","    },\n","    \"xception\": {\n","        \"model_type\": \"xception\",\n","        \"config_path\": \"/content/drive/MyDrive/working/Xception_bestqwk_aptos/config.json\",\n","        \"weights_path\": \"/content/drive/MyDrive/working/Xception_bestqwk_aptos/model.weights.h5\",\n","        \"preprocess\": xception_preprocess,\n","        \"img_size\": 244,\n","        \"base_model\": Xception\n","    },\n","    \"inceptionv3\": {\n","        \"model_type\": \"inceptionv3\",\n","        \"config_path\": \"/content/drive/MyDrive/working/InceptionV3_bestqwk_aptos/config.json\",\n","        \"weights_path\": \"/content/drive/MyDrive/working/InceptionV3_bestqwk_aptos/model.weights.h5\",\n","        \"preprocess\": inceptionv3_preprocess,\n","        \"img_size\": 299,\n","        \"base_model\": InceptionV3\n","    },\n","    \"resnet50\": {\n","        \"model_type\": \"resnet50\",\n","        \"config_path\": \"/content/drive/MyDrive/working/ResNet50_bestqwk_aptos/config.json\",\n","        \"weights_path\": \"/content/drive/MyDrive/working/ResNet50_bestqwk_aptos/model.weights.h5\",\n","        \"preprocess\": resnet50_preprocess,\n","        \"img_size\": 244,\n","        \"base_model\": ResNet50\n","    },\n","    \"densenet121\": {\n","        \"model_type\": \"densenet121\",\n","        \"config_path\": \"/content/drive/MyDrive/working/DenseNet121_bestqwk_aptos/config.json\",\n","        \"weights_path\": \"/content/drive/MyDrive/working/DenseNet121_bestqwk_aptos/model.weights.h5\",\n","        \"preprocess\": densenet121_preprocess,\n","        \"img_size\": 244,\n","        \"base_model\": DenseNet121\n","    }\n","}\n","\n","# Ki·ªÉm tra v√† mount Google Drive\n","from google.colab import drive\n","if not os.path.ismount('/content/drive'):\n","    drive.mount('/content/drive')\n","else:\n","    logging.info(\"Google Drive ƒë√£ ƒë∆∞·ª£c mount.\")\n","\n","# T·∫°o th∆∞ m·ª•c l∆∞u tr·ªØ\n","os.makedirs(FEATURE_SAVE_DIR, exist_ok=True)\n","os.makedirs(PROCESSED_FOLDER, exist_ok=True)\n","os.makedirs(TEMP_AUGMENT_DIR, exist_ok=True)\n","\n","def validate_model_configs(model_configs):\n","    for model_name, config in model_configs.items():\n","        config_path = config['config_path']\n","        weights_path = config['weights_path']\n","        if not os.path.exists(config_path):\n","            logging.error(f\"T·ªáp c·∫•u h√¨nh kh√¥ng t·ªìn t·∫°i cho {model_name}: {config_path}\")\n","            raise FileNotFoundError(f\"Thi·∫øu t·ªáp c·∫•u h√¨nh: {config_path}\")\n","        if not os.path.exists(weights_path):\n","            logging.error(f\"T·ªáp tr·ªçng s·ªë kh√¥ng t·ªìn t·∫°i cho {model_name}: {weights_path}\")\n","            raise FileNotFoundError(f\"Thi·∫øu t·ªáp tr·ªçng s·ªë: {weights_path}\")\n","        logging.info(f\"X√°c nh·∫≠n t·ªáp h·ª£p l·ªá cho {model_name}: {config_path}, {weights_path}\")\n","\n","def load_model_from_config(config_path, weights_path, base_model_fn, input_shape):\n","    if not os.path.exists(config_path) or not os.path.exists(weights_path):\n","        logging.error(f\"Thi·∫øu t·ªáp: config={config_path}, weights={weights_path}\")\n","        raise FileNotFoundError(\"Thi·∫øu t·ªáp c·∫•u h√¨nh ho·∫∑c tr·ªçng s·ªë\")\n","    try:\n","        with open(config_path, 'r') as f:\n","            model_config = json.load(f)\n","        model = model_from_json(json.dumps(model_config), custom_objects={\n","            'CustomGridDropout': CustomGridDropout,\n","            'MemoryAugmentedLayer': MemoryAugmentedLayer,\n","            'GradientReversalLayer': GradientReversalLayer\n","        })\n","        model.load_weights(weights_path, skip_mismatch=True)  # B·ªè by_name=True\n","        logging.info(f\"ƒê√£ t·∫£i tr·ªçng s·ªë t·ª´ {weights_path}\")\n","    except Exception as e:\n","        logging.warning(f\"Kh√¥ng t·∫£i ƒë∆∞·ª£c m√¥ h√¨nh: {str(e)}. Kh·ªüi t·∫°o v·ªõi tr·ªçng s·ªë ImageNet...\")\n","        model = base_model_fn(include_top=False, weights='imagenet', input_shape=input_shape)\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(),\n","        loss='categorical_crossentropy',\n","        metrics=['accuracy']\n","    )\n","    return model\n","\n","# H√†m t·∫£i m√¥ h√¨nh t·ª´ config\n","def load_processed_image(image_id, processed_folder, model_type):\n","    try:\n","        if model_type not in MODEL_CONFIGS:\n","            logging.error(f\"model_type kh√¥ng h·ª£p l·ªá: {model_type}\")\n","            raise ValueError(f\"model_type kh√¥ng h·ª£p l·ªá: {model_type}\")\n","        target_size = MODEL_CONFIGS[model_type]['img_size']\n","        img_path = os.path.join(processed_folder, f\"{image_id}.png\")\n","        if not os.path.exists(img_path):\n","            logging.error(f\"·∫¢nh kh√¥ng t·ªìn t·∫°i: {img_path}\")\n","            return None\n","        img = cv2.imread(img_path)\n","        if img is None:\n","            logging.error(f\"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c ·∫£nh: {img_path}\")\n","            return None\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = cv2.resize(img, (target_size, target_size), interpolation=cv2.INTER_AREA)\n","        if img.shape != (target_size, target_size, 3):\n","            logging.error(f\"Shape ·∫£nh kh√¥ng ƒë√∫ng: {img.shape}, k·ª≥ v·ªçng: ({target_size}, {target_size}, 3)\")\n","            return None\n","        img = img.astype(np.float32) / 255.0  # Chu·∫©n h√≥a\n","        logging.debug(f\"ƒê√£ t·∫£i ·∫£nh {image_id} cho {model_type}: shape={img.shape}\")\n","        return img\n","    except Exception as e:\n","        logging.error(f\"L·ªói khi t·∫£i ·∫£nh {image_id} cho {model_type}: {str(e)}\")\n","        return None\n","# Custom layers\n","class CustomGridDropout(Layer):\n","    def __init__(self, ratio=0.3, holes_number=4, p=0.5, **kwargs):\n","        super().__init__(**kwargs)\n","        self.ratio = ratio\n","        self.holes_number = holes_number\n","        self.p = p\n","\n","    def call(self, inputs, training=None):\n","        if not training:\n","            return inputs\n","        inputs = tf.cast(inputs, tf.float16)  # √âp ki·ªÉu sang float16\n","        batch_size = tf.shape(inputs)[0]\n","        height = tf.shape(inputs)[1]\n","        width = tf.shape(inputs)[2]\n","        channels = tf.shape(inputs)[3]\n","\n","        hole_height = tf.maximum(1, tf.cast(tf.cast(height, tf.float32) * self.ratio, tf.int32))\n","        hole_width = tf.maximum(1, tf.cast(tf.cast(width, tf.float32) * self.ratio, tf.int32))\n","\n","        mask = tf.ones_like(inputs, dtype=tf.float16)  # S·ª≠ d·ª•ng float16\n","        random_probs = tf.random.uniform([self.holes_number], 0, 1)\n","        active_holes = tf.cast(random_probs < self.p, tf.int32)\n","\n","        all_indices = []\n","        for i in range(self.holes_number):\n","            should_apply = active_holes[i]\n","            indices = tf.cond(\n","                should_apply > 0,\n","                lambda: self._generate_patch_indices(\n","                    batch_size, height, width, channels, hole_height, hole_width, i\n","                ),\n","                lambda: tf.zeros([0, 4], dtype=tf.int32)\n","            )\n","            all_indices.append(indices)\n","\n","        all_indices = tf.concat(all_indices, axis=0)\n","        updates = tf.zeros([tf.shape(all_indices)[0]], dtype=tf.float16)\n","\n","        if tf.shape(all_indices)[0] > 0:\n","            mask = tf.tensor_scatter_nd_update(mask, all_indices, updates)\n","\n","        return tf.cast(inputs * mask, tf.float16)  # ƒê·∫£m b·∫£o ƒë·∫ßu ra l√† float16\n","\n","    def _generate_patch_indices(self, batch_size, height, width, channels, hole_height, hole_width, hole_idx):\n","        h_start = tf.random.uniform([], 0, height - hole_height + 1, dtype=tf.int32)\n","        w_start = tf.random.uniform([], 0, width - hole_width + 1, dtype=tf.int32)\n","\n","        h_indices = tf.range(h_start, h_start + hole_height)\n","        w_indices = tf.range(w_start, w_start + hole_width)\n","        c_indices = tf.range(channels)\n","\n","        batch_indices = tf.tile(tf.range(batch_size), [hole_height * hole_width * channels])\n","        h_grid, w_grid, c_grid = tf.meshgrid(h_indices, w_indices, c_indices, indexing='ij')\n","        h_grid = tf.reshape(h_grid, [-1])\n","        w_grid = tf.reshape(w_grid, [-1])\n","        c_grid = tf.reshape(c_grid, [-1])\n","\n","        h_indices = tf.tile(h_grid, [batch_size])\n","        w_indices = tf.tile(w_grid, [batch_size])\n","        c_indices = tf.tile(c_grid, [batch_size])\n","\n","        indices = tf.stack([batch_indices, h_indices, w_indices, c_indices], axis=1)\n","        return indices\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\"ratio\": self.ratio, \"holes_number\": self.holes_number, \"p\": self.p})\n","        return config\n","\n","class MemoryAugmentedLayer(Layer):\n","    def __init__(self, memory_size, memory_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.memory_size = memory_size\n","        self.memory_dim = memory_dim\n","        self.memory = self.add_weight(\n","            shape=(memory_size, memory_dim),\n","            initializer='random_normal',\n","            trainable=True,\n","            name='memory'\n","        )\n","\n","    def call(self, inputs):\n","        # Placeholder: Tr·∫£ v·ªÅ inputs m√† kh√¥ng thay ƒë·ªïi\n","        # Thay th·∫ø b·∫±ng tri·ªÉn khai th·ª±c t·∫ø c·ªßa b·∫°n\n","        return inputs\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\"memory_size\": self.memory_size, \"memory_dim\": self.memory_dim})\n","        return config\n","\n","def check_memory():\n","    try:\n","        memory_info = psutil.virtual_memory()\n","        logging.info(f\"Memory usage: Total={memory_info.total / (1024 ** 3):.2f} GB, \"\n","                     f\"Used={memory_info.used / (1024 ** 3):.2f} GB, \"\n","                     f\"Free={memory_info.available / (1024 ** 3):.2f} GB, \"\n","                     f\"Percent={memory_info.percent}%\")\n","\n","        # Check GPU memory usage\n","        try:\n","            gpus = GPUtil.getGPUs()\n","            for gpu in gpus:\n","                logging.info(f\"GPU {gpu.id}: {gpu.name}, \"\n","                            f\"Memory Used={gpu.memoryUsed / 1024:.2f} GB, \"\n","                            f\"Memory Total={gpu.memoryTotal / 1024:.2f} GB, \"\n","                            f\"Utilization={gpu.memoryUtil * 100:.2f}%\")\n","        except:\n","            logging.warning(f\"Failed to get GPU memory information\")\n","\n","        # Perform garbage collection\n","        gc.collect()\n","\n","        # Warn if memory usage is high\n","        memory_threshold = 1024  # Threshold in MB\n","        if memory_info.available < memory_threshold:\n","            logging.warning(f\"Low memory detected: {memory_info.available / (1024 ** 3):.2f} GB available\")\n","            tf.keras.backend.clear_session()\n","\n","            # Free GPU memory\n","            try:\n","                tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n","            except:\n","                logging.warning(f\"Failed to set memory growth\")\n","\n","        return memory_info.available / (1024 ** 3)  # Return available memory in GB\n","\n","    except Exception as e:\n","        logging.error(f\"Error checking memory: {str(e)}\")\n","        return None\n","\n","class GradientReversalLayer(Layer):\n","    def __init__(self, lambda_=1.0, **kwargs):\n","        super().__init__(**kwargs)\n","        self.lambda_ = lambda_\n","\n","    def call(self, inputs, training=None):\n","        inputs = tf.convert_to_tensor(inputs, dtype=tf.float32)\n","        return inputs if not training else tf.math.multiply(-self.lambda_, inputs)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\"lambda_\": self.lambda_})\n","        return config\n","\n","# H√†m random erasing t√πy ch·ªânh\n","def custom_random_erasing(image, scale=(0.01, 0.05), ratio=(0.5, 2.0), p=0.3, value=None):\n","    if np.random.random() > p:\n","        return image\n","    height, width, channels = image.shape\n","    area = height * width\n","    scale_factor = np.random.uniform(scale[0], scale[1])\n","    erase_area = area * scale_factor\n","    aspect_ratio = np.random.uniform(ratio[0], ratio[1])\n","    erase_height = int(np.sqrt(erase_area / aspect_ratio))\n","    erase_width = int(np.sqrt(erase_area * aspect_ratio))\n","    erase_height = min(erase_height, height)\n","    erase_width = min(erase_width, width)\n","    if erase_height < 1 or erase_width < 1:\n","        return image\n","    x = np.random.randint(0, width - erase_width + 1)\n","    y = np.random.randint(0, height - erase_height + 1)\n","    output = image.copy()\n","    if value is None:\n","        value = np.mean(image, axis=(0, 1))\n","    output[y:y+erase_height, x:x+erase_width, :] = value\n","    return output\n","\n","\n","# H√†m c√¢n b·∫±ng v√† tƒÉng c∆∞·ªùng d·ªØ li·ªáu\n","def balance_data(images, labels, target_classes=[0, 1, 2, 3, 4]):\n","    if not isinstance(target_classes, (list, np.ndarray)):\n","        raise TypeError(f\"target_classes must be list or numpy array, got: {type(target_classes)}\")\n","\n","    num_classes = labels.shape[1]\n","    label_indices = np.argmax(labels, axis=1)\n","\n","    # Filter samples belonging to target_classes\n","    keep_indices = np.isin(label_indices, target_classes)\n","    filtered_images = images[keep_indices]\n","    filtered_labels = labels[keep_indices]\n","    filtered_label_indices = label_indices[keep_indices]\n","\n","    # Count samples per class\n","    class_counts = np.bincount(filtered_label_indices, minlength=num_classes)\n","    logging.info(f\"Initial label distribution: {dict(zip(range(num_classes), class_counts))}\")\n","\n","    # Target ~7148 samples (~1430 per class)\n","    total_samples = 7148\n","    base_samples_per_class = total_samples // len(target_classes)  # 1429\n","    extra_samples = total_samples % len(target_classes)  # 3\n","    samples_per_class = [base_samples_per_class] * len(target_classes)\n","    for i in range(extra_samples):\n","        samples_per_class[i] += 1\n","    logging.info(f\"Target samples per class: {dict(zip(target_classes, samples_per_class))}\")\n","\n","    new_images = []\n","    new_labels = []\n","\n","    for cls_idx, cls in enumerate(target_classes):\n","        cls_indices = np.where(filtered_label_indices == cls)[0]\n","        cls_images = filtered_images[cls_indices]\n","        cls_labels = filtered_labels[cls_indices]\n","        current_count = len(cls_indices)\n","        target_count = samples_per_class[cls_idx]\n","\n","        logging.info(f\"Class {cls}: {current_count} initial samples, target {target_count} samples\")\n","\n","        # Add all available samples\n","        new_images.extend(cls_images)\n","        new_labels.extend(cls_labels)\n","\n","        # Oversample if needed\n","        if current_count < target_count:\n","            oversample_count = target_count - current_count\n","            logging.info(f\"Oversampling {oversample_count} samples for class {cls}\")\n","            # Randomly select indices with replacement\n","            oversample_indices = np.random.choice(cls_indices, size=oversample_count, replace=True)\n","            new_images.extend(filtered_images[oversample_indices])\n","            new_labels.extend(filtered_labels[oversample_indices])\n","\n","    new_images = np.array(new_images, dtype=np.float32)\n","    new_labels = np.array(new_labels, dtype=np.float32)\n","\n","    # Verify total samples\n","    if len(new_images) != total_samples:\n","        logging.error(f\"Sample count mismatch: got {len(new_images)}, expected {total_samples}\")\n","        raise ValueError(\"Sample count mismatch after balancing\")\n","\n","    new_images, new_labels = shuffle(new_images, new_labels, random_state=42)\n","\n","    final_class_counts = np.bincount(np.argmax(new_labels, axis=1), minlength=num_classes)\n","    logging.info(f\"Final label distribution: {dict(zip(range(num_classes), final_class_counts))}\")\n","\n","    return new_images, new_labels\n","\n","\n","class My_Generator(tf.keras.utils.Sequence):\n","    def __init__(self, images, labels, batch_size, is_train=False, mix=False, augment=False, model_type=\"default\", preprocess=None, image_paths=None, sample_ids=None, temp_augment_dir=\"/content/temp_augment\"):\n","        self.batch_size = batch_size\n","        self.is_train = is_train\n","        self.is_mix = False  # Disable mixup\n","        self.augment = False  # Disable augmentation\n","        self.model_type = str(model_type).lower()\n","        self.preprocess = preprocess\n","        self.temp_augment_dir = temp_augment_dir\n","        os.makedirs(self.temp_augment_dir, exist_ok=True)\n","        if not os.access(self.temp_augment_dir, os.W_OK):\n","            raise PermissionError(f\"No write permission for {self.temp_augment_dir}\")\n","\n","        # Set target_size dynamically based on model_type\n","        if self.model_type not in MODEL_CONFIGS:\n","            raise ValueError(f\"Invalid model_type: {self.model_type}. Must be in {list(MODEL_CONFIGS.keys())}\")\n","        self.target_size = (MODEL_CONFIGS[self.model_type]['img_size'], MODEL_CONFIGS[self.model_type]['img_size'])\n","        logging.info(f\"Initialized My_Generator for {self.model_type} with target_size={self.target_size}\")\n","\n","        self.image_paths = []\n","        self.labels = []\n","        self.sample_ids = []\n","\n","        if image_paths is not None:\n","            if len(image_paths) != len(labels) or len(image_paths) != len(sample_ids):\n","                raise ValueError(f\"Mismatch: image_paths={len(image_paths)}, labels={len(labels)}, sample_ids={len(sample_ids)}\")\n","            for path, label, sid in zip(image_paths, labels, sample_ids):\n","                if os.path.exists(path):\n","                    self.image_paths.append(path)\n","                    self.labels.append(label)\n","                    self.sample_ids.append(sid)\n","                else:\n","                    logging.warning(f\"Skipping non-existent image: {path}\")\n","        elif isinstance(images, np.ndarray):\n","            if len(images) != len(labels) or len(images) != len(sample_ids):\n","                raise ValueError(f\"Mismatch: images={len(images)}, labels={len(labels)}, sample_ids={len(sample_ids)}\")\n","            for i, (img, label, sid) in enumerate(zip(images, labels, sample_ids)):\n","                img_path = os.path.join(self.temp_augment_dir, f\"img_{i}_{np.random.randint(1000000)}.png\")\n","                try:\n","                    # Resize image to target_size\n","                    img = cv2.resize(img, self.target_size, interpolation=cv2.INTER_AREA)\n","                    if img.dtype != np.uint8:\n","                        if img.max() <= 1.0:\n","                            img = (img * 255).astype(np.uint8)\n","                        else:\n","                            img = img.astype(np.uint8)\n","                    cv2.imwrite(img_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n","                    if os.path.exists(img_path):\n","                        self.image_paths.append(img_path)\n","                        self.labels.append(label)\n","                        self.sample_ids.append(sid)\n","                    else:\n","                        logging.warning(f\"Failed to save image: {img_path}\")\n","                except Exception as e:\n","                    logging.warning(f\"Error saving image {img_path}: {str(e)}\")\n","                    continue\n","        else:\n","            raise ValueError(\"Require valid image_paths or images\")\n","\n","        if not self.image_paths:\n","            raise ValueError(\"No valid image_paths created\")\n","\n","        self.labels = np.array(self.labels, dtype=np.float32)\n","        self.sample_ids = np.array(self.sample_ids, dtype=str)\n","\n","        if len(self.image_paths) != len(self.labels) or len(self.image_paths) != len(self.sample_ids):\n","            raise ValueError(f\"Post-init mismatch: image_paths={len(self.image_paths)}, labels={len(self.labels)}, sample_ids={len(self.sample_ids)}\")\n","\n","        logging.info(f\"Initialized My_Generator: {len(self.image_paths)} samples, batch_size={batch_size}, is_train={is_train}, target_size={self.target_size}\")\n","        self.indices = np.arange(len(self.image_paths))\n","        if self.is_train:\n","            np.random.shuffle(self.indices)\n","\n","    def __len__(self):\n","        num_samples = len(self.image_paths)\n","        if num_samples == 0:\n","            logging.error(\"No samples in generator\")\n","            raise ValueError(\"Generator has no samples\")\n","        num_batches = (num_samples + self.batch_size - 1) // self.batch_size\n","        logging.info(f\"Generator len: {num_samples} samples, {num_batches} batches\")\n","        return num_batches\n","\n","    def __getitem__(self, index):\n","        start_idx = index * self.batch_size\n","        end_idx = min(start_idx + self.batch_size, len(self.image_paths))\n","        batch_indices = self.indices[start_idx:end_idx]\n","\n","        if len(batch_indices) == 0:\n","            logging.warning(f\"Batch {index} has no indices, returning empty batch\")\n","            return np.array([]), np.array([]), []\n","\n","        batch_images = []\n","        batch_labels = []\n","        batch_ids = []\n","\n","        for idx in batch_indices:\n","            try:\n","                img = self._load_image(self.image_paths[idx])\n","                if img is not None and img.size > 0:\n","                    batch_images.append(img)\n","                    batch_labels.append(self.labels[idx])\n","                    batch_ids.append(self.sample_ids[idx])\n","                else:\n","                    logging.warning(f\"Skipping empty or failed image at index {idx}: {self.image_paths[idx]}\")\n","            except Exception as e:\n","                logging.warning(f\"Error loading image at index {idx}: {str(e)}\")\n","                continue\n","\n","        if not batch_images:\n","            logging.warning(f\"Batch {index} empty after processing, returning empty batch\")\n","            return np.array([]), np.array([]), []\n","\n","        batch_images = np.array(batch_images)\n","        batch_labels = np.array(batch_labels)\n","        logging.debug(f\"Batch {index}: images_shape={batch_images.shape}, labels_shape={batch_labels.shape}, ids_count={len(batch_ids)}\")\n","        return batch_images, batch_labels, batch_ids\n","\n","    def _load_image(self, img_path):\n","        img = cv2.imread(img_path)\n","        if img is None:\n","            logging.error(f\"Failed to read image: {img_path}\")\n","            return None\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = cv2.resize(img, self.target_size)\n","        if self.preprocess:\n","            img = self.preprocess(img)\n","        return img\n","\n","    def on_epoch_end(self):\n","        if self.is_train:\n","            np.random.shuffle(self.indices)\n","\n","# Callback ƒë·ªÉ t√≠nh tr·ªçng s·ªë l·ªõp t·ª´ confusion matrix\n","class ConfusionMatrixWeightCallback(tf.keras.callbacks.Callback):\n","    def __init__(self, valid_features, valid_labels, classification_model, num_classes=5, class_counts=None):\n","        super().__init__()\n","        self.valid_features = valid_features\n","        self.valid_labels = valid_labels\n","        self.classification_model = classification_model\n","        self.num_classes = num_classes\n","        self.prev_cm = None\n","        self.class_weights = np.ones(num_classes, dtype=np.float32)\n","        self.class_counts = class_counts\n","        self.history_dir = os.path.join(FEATURE_SAVE_DIR, \"history\")\n","        os.makedirs(self.history_dir, exist_ok=True)\n","        self.weights_history = []\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        y_pred = self.classification_model.predict(self.valid_features, verbose=0, batch_size=32)\n","        y_true = np.argmax(self.valid_labels, axis=1)\n","        y_pred_classes = np.argmax(y_pred, axis=1)\n","\n","        cm = confusion_matrix(y_true, y_pred_classes, labels=list(range(self.num_classes)))\n","        logging.info(f\"Epoch {epoch+1} - Ma tr·∫≠n nh·∫ßm l·∫´n:\\n{cm}\")\n","\n","        errors = np.sum(cm * (1 - np.eye(self.num_classes)), axis=1)\n","        total_samples_per_class = np.sum(cm, axis=1)\n","        total_samples_per_class = np.where(total_samples_per_class == 0, 1, total_samples_per_class)\n","        error_rates = errors / total_samples_per_class\n","\n","        weak_classes = []\n","        if self.class_counts is not None:\n","            min_count = np.min(self.class_counts[self.class_counts > 0])\n","            weak_classes = np.where(self.class_counts <= min_count * 1.5)[0]\n","        high_error_classes = np.where(error_rates >= np.percentile(error_rates, 75))[0]\n","        weak_classes = np.unique(np.concatenate([weak_classes, high_error_classes])).astype(int)\n","\n","        self.class_weights = 1.0 + error_rates\n","        for cls in weak_classes:\n","            self.class_weights[cls] *= 2.0\n","        self.class_weights /= self.class_weights.max()\n","\n","        logging.info(f\"Epoch {epoch+1} - L·ªõp y·∫øu: {weak_classes}\")\n","        logging.info(f\"Epoch {epoch+1} - Tr·ªçng s·ªë l·ªõp: {self.class_weights}\")\n","\n","        self.weights_history.append({\n","            \"epoch\": epoch + 1,\n","            \"class_weights\": self.class_weights.tolist(),\n","            \"weak_classes\": weak_classes.tolist(),\n","            \"confusion_matrix\": cm.tolist()\n","        })\n","        weights_path = os.path.join(self.history_dir, f\"class_weights_epoch_{epoch+1}.json\")\n","        with open(weights_path, 'w') as f:\n","            json.dump(self.weights_history[-1], f, indent=4)\n","        logging.info(f\"ƒê√£ l∆∞u tr·ªçng s·ªë l·ªõp t·∫°i: {weights_path}\")\n","\n","        plt.figure(figsize=(8, 6))\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                    xticklabels=list(range(self.num_classes)),\n","                    yticklabels=list(range(self.num_classes)))\n","        plt.title(f'Ma tr·∫≠n nh·∫ßm l·∫´n - Epoch {epoch+1}')\n","        plt.xlabel('D·ª± ƒëo√°n')\n","        plt.ylabel('Th·ª±c t·∫ø')\n","        cm_path = os.path.join(FEATURE_SAVE_DIR, f'confusion_matrix_epoch_{epoch+1}.png')\n","        plt.savefig(cm_path)\n","        plt.close()\n","        logging.info(f\"ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n t·∫°i: {cm_path}\")\n","\n","        self.prev_cm = cm.copy()\n","\n","    def get_class_weights(self):\n","        return self.class_weights\n","\n","# Callback cho checkpoint d·ª±a tr√™n nhi·ªÅu metrics\n","class MultiMetricCheckpoint(tf.keras.callbacks.Callback):\n","    def __init__(self, filepath, monitor_metrics, mode='max', save_best_only=True):\n","        super().__init__()\n","        self.filepath = filepath\n","        self.monitor_metrics = monitor_metrics\n","        self.mode = mode\n","        self.save_best_only = save_best_only\n","        self.best_metrics = {metric: -float('inf') if mode == 'max' else float('inf') for metric in monitor_metrics}\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        current_metrics = {}\n","        improved = False\n","\n","        for metric in self.monitor_metrics:\n","            current = logs.get(metric, 0.0)\n","            current_metrics[metric] = current\n","            if self.mode == 'max' and current > self.best_metrics[metric]:\n","                self.best_metrics[metric] = current\n","                improved = True\n","            elif self.mode == 'min' and current < self.best_metrics[metric]:\n","                self.best_metrics[metric] = current\n","                improved = True\n","\n","        if improved and self.save_best_only:\n","            self.model.save_weights(self.filepath.format(epoch=epoch + 1), overwrite=True)\n","            logging.info(f\"ƒê√£ l∆∞u m√¥ h√¨nh t·ªët nh·∫•t t·∫°i epoch {epoch + 1} v·ªõi metrics: {current_metrics}\")\n","\n","# H√†m ki·ªÉm tra b·ªô nh·ªõ\n","\n","# H√†m debug layers c·ªßa m√¥ h√¨nh\n","def debug_model_layers(model, model_name=\"model\"):\n","    logging.info(f\"Ki·ªÉm tra layers c·ªßa {model_name}:\")\n","    conv_layers = []\n","    for layer in model.layers:\n","        if hasattr(layer, 'output'):\n","            output_shape = layer.output.shape\n","            if len(output_shape) == 4:\n","                conv_layers.append((layer.name, output_shape))\n","                logging.info(f\"Layer: {layer.name}, Type: {type(layer).__name__}, Output Shape: {output_shape}\")\n","    if not conv_layers:\n","        logging.error(f\"Kh√¥ng t√¨m th·∫•y layer 4D n√†o trong {model_name}\")\n","    else:\n","        logging.info(f\"T√¨m th·∫•y {len(conv_layers)} layers v·ªõi ƒë·∫ßu ra 4D:\")\n","        for name, shape in conv_layers:\n","            logging.info(f\"  - {name}: {shape}\")\n","    return conv_layers\n","\n","# H√†m validate dataset\n","def validate_dataset(image_ids, processed_folder, save_dir):\n","    valid_ids = []\n","    corrupted_files = []\n","\n","    for image_id in image_ids:\n","        img_path = os.path.join(processed_folder, f\"{image_id}.png\")\n","        if not os.path.exists(img_path):\n","            logging.error(f\"·∫¢nh kh√¥ng t·ªìn t·∫°i: {img_path}\")\n","            corrupted_files.append(image_id)\n","            continue\n","        img = cv2.imread(img_path)\n","        if img is None:\n","            logging.error(f\"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c ·∫£nh: {img_path}\")\n","            corrupted_files.append(image_id)\n","            continue\n","        valid_ids.append(image_id)\n","\n","    report = {\n","        \"total_images\": len(image_ids),\n","        \"valid_images\": len(valid_ids),\n","        \"corrupted_images\": len(corrupted_files),\n","        \"corrupted_ids\": corrupted_files\n","    }\n","    report_path = os.path.join(save_dir, \"dataset_validation_report.json\")\n","    with open(report_path, 'w') as f:\n","        json.dump(report, f, indent=4)\n","    logging.info(f\"ƒê√£ l∆∞u b√°o c√°o validation t·∫°i: {report_path}\")\n","\n","    if corrupted_files:\n","        logging.warning(f\"ƒê√£ t√¨m th·∫•y {len(corrupted_files)} ·∫£nh b·ªã l·ªói ho·∫∑c thi·∫øu: {corrupted_files[:5]}...\")\n","\n","    return valid_ids\n","\n","# H√†m ki·ªÉm tra data leakage\n","def check_data_leakage(train_x, valid_x, test_x, metadata_df=None, id_column='id_code'):\n","    train_set = set(train_x)\n","    valid_set = set(valid_x)\n","    test_set = set(test_x)\n","\n","    train_valid_overlap = train_set.intersection(valid_set)\n","    train_test_overlap = train_set.intersection(test_set)\n","    valid_test_overlap = valid_set.intersection(test_set)\n","\n","    leakage_detected = False\n","    if train_valid_overlap:\n","        logging.warning(f\"Ph√°t hi·ªán overlap gi·ªØa train v√† valid: {len(train_valid_overlap)} m·∫´u\")\n","        leakage_detected = True\n","    if train_test_overlap:\n","        logging.warning(f\"Ph√°t hi·ªán overlap gi·ªØa train v√† test: {len(train_test_overlap)} m·∫´u\")\n","        leakage_detected = True\n","    if valid_test_overlap:\n","        logging.warning(f\"Ph√°t hi·ªán overlap gi·ªØa valid v√† test: {len(valid_test_overlap)} m·∫´u\")\n","        leakage_detected = True\n","\n","    if metadata_df is not None and 'patient_id' in metadata_df.columns:\n","        train_patients = set(metadata_df[metadata_df[id_column].isin(train_x)]['patient_id'])\n","        valid_patients = set(metadata_df[metadata_df[id_column].isin(valid_x)]['patient_id'])\n","        test_patients = set(metadata_df[metadata_df[id_column].isin(test_x)]['patient_id'])\n","\n","        patient_overlap = train_patients.intersection(valid_patients, test_patients)\n","        if patient_overlap:\n","            logging.warning(f\"Ph√°t hi·ªán overlap b·ªánh nh√¢n: {patient_overlap}\")\n","            leakage_detected = True\n","\n","    if not leakage_detected:\n","        logging.info(\"Kh√¥ng ph√°t hi·ªán data leakage.\")\n","\n","    return not leakage_detected\n","\n","def get_image_ids(folder_path):\n","    try:\n","        if not os.path.exists(folder_path):\n","            print(f\"ERROR: Th∆∞ m·ª•c {folder_path} kh√¥ng t·ªìn t·∫°i.\")\n","            return []\n","\n","        image_files = [f for f in os.listdir(folder_path) if f.endswith('.png')]\n","        image_ids = [os.path.splitext(f)[0] for f in image_files]\n","\n","        print(f\"T√¨m th·∫•y {len(image_ids)} ID ·∫£nh trong {folder_path}\")\n","        print(f\"M·∫´u ID: {image_ids[:5]}...\")\n","\n","        return image_ids\n","    except Exception as e:\n","        print(f\"ERROR: L·ªói khi li·ªát k√™ ID ·∫£nh trong {folder_path}: {str(e)}\")\n","        return []\n","\n","def extract_features(generator, feature_extractor):\n","    features = []\n","    feature_ids = []\n","    total_samples = len(generator.image_paths)\n","    num_batches = len(generator)\n","    processed_samples = 0\n","    empty_batch_count = 0\n","    max_empty_batches = 10\n","    batch_size = 16\n","\n","    logging.info(f\"Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng cho {total_samples} m·∫´u, {num_batches} batch\")\n","\n","    for batch_idx in range(num_batches):\n","        try:\n","            batch_images, _, batch_ids = generator[batch_idx]\n","        except Exception as e:\n","            logging.warning(f\"L·ªói khi l·∫•y batch {batch_idx}: {str(e)}\")\n","            empty_batch_count += 1\n","            if empty_batch_count >= max_empty_batches:\n","                logging.error(f\"Qu√° nhi·ªÅu batch r·ªóng li√™n ti·∫øp ({empty_batch_count})\")\n","                break\n","            continue\n","\n","        if len(batch_images) == 0 or len(batch_ids) == 0:\n","            logging.warning(f\"Batch {batch_idx} r·ªóng, b·ªè qua\")\n","            empty_batch_count += 1\n","            if empty_batch_count >= max_empty_batches:\n","                logging.error(f\"Qu√° nhi·ªÅu batch r·ªóng li√™n ti·∫øp ({empty_batch_count})\")\n","                break\n","            continue\n","\n","        empty_batch_count = 0\n","        try:\n","            batch_features = feature_extractor.predict(batch_images, batch_size=batch_size, verbose=0)\n","            logging.debug(f\"Batch {batch_idx} feature shape tr∆∞·ªõc pooling: {batch_features.shape}\")\n","            if len(batch_features.shape) > 2:\n","                batch_features = np.mean(batch_features, axis=(1, 2))  # Global average pooling\n","            logging.debug(f\"Batch {batch_idx} feature shape sau pooling: {batch_features.shape}\")\n","            if batch_features.shape[0] != len(batch_ids):\n","                logging.warning(f\"S·ªë ƒë·∫∑c tr∆∞ng ({batch_features.shape[0]}) kh√¥ng kh·ªõp v·ªõi s·ªë ID ({len(batch_ids)})\")\n","                continue\n","            features.append(batch_features)\n","            feature_ids.extend(batch_ids)\n","            processed_samples += len(batch_ids)\n","            logging.info(f\"Batch {batch_idx}: {len(batch_ids)} samples, features_shape={batch_features.shape}\")\n","        except Exception as e:\n","            logging.warning(f\"L·ªói khi x·ª≠ l√Ω batch {batch_idx}: {str(e)}\")\n","            continue\n","\n","    if not features:\n","        logging.error(\"Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c ƒë·∫∑c tr∆∞ng n√†o.\")\n","        raise ValueError(\"Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c ƒë·∫∑c tr∆∞ng.\")\n","\n","    features = np.concatenate(features, axis=0)\n","    if len(features) != len(feature_ids):\n","        logging.error(f\"S·ªë ƒë·∫∑c tr∆∞ng ({len(features)}) kh√¥ng kh·ªõp v·ªõi s·ªë feature_ids ({len(feature_ids)})\")\n","        raise ValueError(\"ƒê·∫∑c tr∆∞ng v√† ID kh√¥ng ƒë·ªìng b·ªô.\")\n","    logging.info(f\"Ho√†n th√†nh tr√≠ch xu·∫•t: {len(features)} ƒë·∫∑c tr∆∞ng\")\n","    return features, feature_ids\n","\n","# H√†m tr√≠ch xu·∫•t v√† l∆∞u ƒë·∫∑c tr∆∞ng\n","def extract_and_save_features(model_name, extractor, generator, save_dir, sample_ids):\n","    if not sample_ids or len(sample_ids) == 0:\n","        logging.error(f\"sample_ids r·ªóng ho·∫∑c kh√¥ng ƒë∆∞·ª£c cung c·∫•p cho {model_name}.\")\n","        raise ValueError(\"sample_ids r·ªóng ho·∫∑c kh√¥ng ƒë∆∞·ª£c cung c·∫•p.\")\n","\n","    logging.info(f\"B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng cho {model_name}\")\n","    features, feature_ids = extract_features(generator, extractor)\n","    logging.info(f\"Tr√≠ch xu·∫•t {len(features)} ƒë·∫∑c tr∆∞ng cho {model_name}\")\n","\n","    if len(features) != len(sample_ids):\n","        logging.warning(f\"S·ªë ƒë·∫∑c tr∆∞ng ({len(features)}) kh√¥ng kh·ªõp v·ªõi s·ªë sample_ids ({len(sample_ids)})\")\n","        missing_ids = set(sample_ids) - set(feature_ids)\n","        logging.warning(f\"ID b·ªã thi·∫øu trong feature_ids: {list(missing_ids)[:5]}...\")\n","\n","    feature_path = os.path.join(save_dir, f\"{model_name}_features.npy\")\n","    np.save(feature_path, features)\n","    logging.info(f\"ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng t·∫°i: {feature_path}\")\n","\n","    ids_path = os.path.join(save_dir, f\"{model_name}_feature_ids.npy\")\n","    np.save(ids_path, np.array(feature_ids, dtype=str))\n","    logging.info(f\"ƒê√£ l∆∞u feature_ids t·∫°i: {ids_path}\")\n","\n","    return features, feature_ids\n","\n","# H√†m k·∫øt h·ª£p v√† gi·∫£m chi·ªÅu ƒë·∫∑c tr∆∞ng\n","def combine_and_reduce_features(features_dict, features_dict_4d, labels, sample_ids, save_dir, n_components=50):\n","    try:\n","        features_2d_list = []\n","        feature_ids_list = []\n","\n","        for model_name, features in features_dict.items():\n","            if features is None:\n","                logging.warning(f\"ƒê·∫∑c tr∆∞ng t·ª´ {model_name} l√† None, b·ªè qua.\")\n","                continue\n","            if len(features.shape) != 2:\n","                logging.warning(f\"ƒê·∫∑c tr∆∞ng t·ª´ {model_name} c√≥ shape {features.shape}, √°p d·ª•ng global average pooling.\")\n","                features = np.mean(features, axis=tuple(range(1, len(features.shape)-1)), keepdims=False)\n","            features_2d_list.append(features)\n","            feature_ids = np.load(os.path.join(save_dir, f\"{model_name}_feature_ids.npy\"), allow_pickle=True)\n","            feature_ids_list.append(feature_ids)\n","            logging.info(f\"ƒê√£ th√™m ƒë·∫∑c tr∆∞ng 2D t·ª´ {model_name}, shape: {features.shape}\")\n","\n","        if not features_2d_list:\n","            logging.error(\"Kh√¥ng c√≥ ƒë·∫∑c tr∆∞ng 2D h·ª£p l·ªá n√†o ƒë·ªÉ k·∫øt h·ª£p.\")\n","            raise ValueError(\"Y√™u c·∫ßu √≠t nh·∫•t m·ªôt t·∫≠p ƒë·∫∑c tr∆∞ng 2D h·ª£p l·ªá.\")\n","\n","        # Ki·ªÉm tra t√≠nh nh·∫•t qu√°n c·ªßa feature_ids\n","        feature_ids_array = np.array(feature_ids_list)\n","        if not np.all(feature_ids_array == feature_ids_array[0]):\n","            logging.warning(\"Feature_ids kh√¥ng nh·∫•t qu√°n, s·ª≠ d·ª•ng feature_ids t·ª´ m√¥ h√¨nh ƒë·∫ßu ti√™n.\")\n","        common_ids = feature_ids_list[0]\n","\n","        # Ki·ªÉm tra shape ƒë·ªìng nh·∫•t\n","        feature_shapes = [f.shape for f in features_2d_list]\n","        if len(set(tuple(s) for s in feature_shapes)) > 1:\n","            logging.error(f\"Shape ƒë·∫∑c tr∆∞ng kh√¥ng ƒë·ªìng nh·∫•t: {feature_shapes}\")\n","            raise ValueError(\"ƒê·∫∑c tr∆∞ng c√≥ shape kh√¥ng ƒë·ªìng nh·∫•t.\")\n","\n","        combined_2d = np.concatenate(features_2d_list, axis=1)\n","        scaler = StandardScaler()\n","        combined_2d = scaler.fit_transform(combined_2d)\n","\n","        indices = []\n","        missing_ids = []\n","        for sample_id in sample_ids:\n","            idx = np.where(common_ids == sample_id)[0]\n","            if len(idx) > 0:\n","                indices.append(idx[0])\n","            else:\n","                missing_ids.append(sample_id)\n","                logging.warning(f\"Kh√¥ng t√¨m th·∫•y sample_id {sample_id} trong feature_ids.\")\n","\n","        if not indices:\n","            logging.error(\"Kh√¥ng t√¨m th·∫•y sample_id n√†o trong feature_ids.\")\n","            raise ValueError(\"Kh√¥ng c√≥ sample_id h·ª£p l·ªá.\")\n","\n","        indices = np.array(indices)\n","        combined_2d = combined_2d[indices]\n","        aligned_labels = labels[indices] if labels is not None else None\n","\n","        if n_components is not None:\n","            pca = PCA(n_components=n_components, random_state=42)\n","            reduced_2d = pca.fit_transform(combined_2d)\n","            explained_variance = np.sum(pca.explained_variance_ratio_)\n","            logging.info(f\"PCA: {n_components} th√†nh ph·∫ßn, gi·∫£i th√≠ch {explained_variance*100:.2f}% ph∆∞∆°ng sai\")\n","        else:\n","            reduced_2d = combined_2d\n","            explained_variance = 1.0\n","            logging.info(\"B·ªè qua PCA, gi·ªØ nguy√™n ƒë·∫∑c tr∆∞ng g·ªëc.\")\n","\n","        save_path = os.path.join(save_dir, \"combined_reduced_features.npy\")\n","        np.save(save_path, reduced_2d)\n","        logging.info(f\"ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng gi·∫£m chi·ªÅu t·∫°i: {save_path}\")\n","\n","        return reduced_2d, aligned_labels, indices, reduced_2d.shape[1]\n","\n","    except Exception as e:\n","        logging.error(f\"L·ªói khi k·∫øt h·ª£p v√† gi·∫£m chi·ªÅu ƒë·∫∑c tr∆∞ng: {str(e)}\")\n","        raise\n","\n","# H√†m apply temperature scaling v√† laplace smoothing\n","def apply_temperature_scaling(logits, temperature=2.0):\n","    logits = tf.convert_to_tensor(logits, dtype=tf.float32)\n","    return tf.nn.softmax(logits / temperature)\n","\n","def laplace_smoothing(probs, epsilon=1e-5):\n","    probs = tf.convert_to_tensor(probs, dtype=tf.float32)\n","    return (probs + epsilon) / (tf.reduce_sum(probs, axis=-1, keepdims=True) + NUM_CLASSES * epsilon)\n","\n","# H√†m l∆∞u confusion matrix\n","def save_confusion_matrix(y_true, y_pred, episode, qwk, save_dir, prefix=''):\n","    cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3, 4])\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=[0, 1, 2, 3, 4], yticklabels=[0, 1, 2, 3, 4])\n","    plt.title(f'Ma tr·∫≠n nh·∫ßm l·∫´n - {prefix}QWK: {qwk:.4f} t·∫°i Episode {episode+1}')\n","    plt.xlabel('D·ª± ƒëo√°n')\n","    plt.ylabel('Th·ª±c t·∫ø')\n","    cm_path = os.path.join(save_dir, f'confusion_matrix_{prefix}qwk_episode_{episode+1}.png')\n","    plt.savefig(cm_path)\n","    plt.close()\n","    logging.info(f\"ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n {prefix}QWK t·∫°i: {cm_path}\")\n","    return cm\n","\n","# H√†m ƒë√°nh gi√° chi ti·∫øt per-class metrics\n","def evaluate_per_class_metrics(y_true, y_pred_probs, y_pred_classes, labels, save_dir, prefix=\"test\"):\n","    report = classification_report(\n","        y_true, y_pred_classes, labels=labels, target_names=[f\"Class {i}\" for i in labels], output_dict=True\n","    )\n","    metrics_df = pd.DataFrame(report).transpose()\n","\n","    metrics_path = Path(save_dir) / f\"{prefix}_classification_report.csv\"\n","    metrics_df.to_csv(metrics_path)\n","    logging.info(f\"ƒê√£ l∆∞u b√°o c√°o ph√¢n lo·∫°i t·∫°i: {metrics_path}\")\n","\n","    plt.figure(figsize=(12, 5))\n","\n","    plt.subplot(1, 2, 1)\n","    for i in labels:\n","        fpr, tpr, _ = roc_curve(y_true == i, y_pred_probs[:, i])\n","        roc_auc = auc(fpr, tpr)\n","        plt.plot(fpr, tpr, label=f\"Class {i} (AUC = {roc_auc:.2f})\")\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlabel(\"False Positive Rate\")\n","    plt.ylabel(\"True Positive Rate\")\n","    plt.title(\"ROC Curves\")\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    for i in labels:\n","        precision, recall, _ = precision_recall_curve(y_true == i, y_pred_probs[:, i])\n","        plt.plot(recall, precision, label=f\"Class {i}\")\n","    plt.xlabel(\"Recall\")\n","    plt.ylabel(\"Precision\")\n","    plt.title(\"Precision-Recall Curves\")\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    curves_path = Path(save_dir) / f\"{prefix}_roc_pr_curves.png\"\n","    plt.savefig(curves_path, bbox_inches=\"tight\")\n","    plt.close()\n","    logging.info(f\"ƒê√£ l∆∞u ROC v√† PR curves t·∫°i: {curves_path}\")\n","\n","    return metrics_df\n","\n","# H√†m ƒë√°nh gi√° calibration\n","def evaluate_calibration(y_true, y_pred_probs, save_dir, prefix=\"test\"):\n","    plt.figure(figsize=(10, 8))\n","\n","    for cls in range(NUM_CLASSES):\n","        prob_true, prob_pred = calibration_curve(\n","            y_true == cls, y_pred_probs[:, cls], n_bins=10, strategy='uniform'\n","        )\n","        plt.plot(prob_pred, prob_true, marker='o', label=f\"Class {cls}\")\n","\n","    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n","    plt.xlabel(\"Mean Predicted Probability\")\n","    plt.ylabel(\"Fraction of Positives\")\n","    plt.title(\"Reliability Diagram\")\n","    plt.legend()\n","\n","    calib_path = Path(save_dir) / f\"{prefix}_calibration_curve.png\"\n","    plt.savefig(calib_path, bbox_inches=\"tight\")\n","    plt.close()\n","    logging.info(f\"ƒê√£ l∆∞u calibration curve t·∫°i: {calib_path}\")\n","\n","# H√†m export predictions\n","def export_predictions(image_ids, y_true, y_pred_probs, y_pred_classes, save_dir, filename=\"predictions.csv\"):\n","    predictions_df = pd.DataFrame({\n","        \"id_code\": image_ids,\n","        \"true_label\": y_true,\n","        \"predicted_label\": y_pred_classes\n","    })\n","    for i in range(y_pred_probs.shape[1]):\n","        predictions_df[f\"prob_class_{i}\"] = y_pred_probs[:, i]\n","\n","    output_path = Path(save_dir) / filename\n","    predictions_df.to_csv(output_path, index=False)\n","    logging.info(f\"ƒê√£ xu·∫•t d·ª± ƒëo√°n t·∫°i: {output_path}\")\n","\n","    return predictions_df\n","\n","\n","\n","# H√†m l∆∞u meta-learner model\n","\n","def save_meta_learner_model(model, save_dir, episode, qwk_scores, training_config):\n","    if model is None:\n","        logging.error(f\"Kh√¥ng th·ªÉ l∆∞u m√¥ h√¨nh t·∫°i episode {episode + 1}: model l√† None\")\n","        raise ValueError(\"M√¥ h√¨nh l√† None\")\n","    try:\n","        os.makedirs(save_dir, exist_ok=True)\n","        model_path = os.path.join(save_dir, f\"meta_model_episode_{episode + 1}.h5\")\n","        model.save_weights(model_path, overwrite=True)  # Ch·ªâ l∆∞u tr·ªçng s·ªë\n","        logging.info(f\"ƒê√£ l∆∞u tr·ªçng s·ªë m√¥ h√¨nh t·∫°i: {model_path}\")\n","        meta_info = {\n","            \"episode\": episode + 1,\n","            \"qwk_scores\": [float(score) for score in qwk_scores],\n","            \"training_config\": training_config\n","        }\n","        info_path = os.path.join(save_dir, f\"meta_info_episode_{episode + 1}.json\")\n","        with open(info_path, 'w') as f:\n","            json.dump(meta_info, f, indent=4)\n","        logging.info(f\"ƒê√£ l∆∞u th√¥ng tin meta t·∫°i: {info_path}\")\n","    except Exception as e:\n","        logging.error(f\"L·ªói khi l∆∞u meta m√¥ h√¨nh t·∫°i episode {episode + 1}: {str(e)}\")\n","        raise\n","\n","# H√†m t·∫°o episode cho meta-learning\n","def create_episode(data_df, support_size, query_size, num_classes, model_type=\"efficientnetb1\"):\n","    config = MODEL_CONFIGS[model_type]\n","    size = config['img_size']\n","    support_images = []\n","    support_labels = []\n","    query_images = []\n","    query_labels = []\n","    included_classes = []\n","\n","    for cls in range(num_classes):\n","        cls_samples = data_df[data_df['diagnosis'] == cls]\n","        required_samples = support_size + query_size\n","        if len(cls_samples) < required_samples:\n","            logging.warning(f\"Kh√¥ng ƒë·ªß m·∫´u cho l·ªõp {cls}: c·∫ßn {required_samples}, c√≥ {len(cls_samples)}\")\n","            continue\n","        cls_samples = cls_samples.sample(n=required_samples, random_state=42)\n","        support_samples = cls_samples.iloc[:support_size]\n","        query_samples = cls_samples.iloc[support_size:support_size + query_size]\n","\n","        for _, row in support_samples.iterrows():\n","            img = load_processed_image(row['id_code'], PROCESSED_FOLDER, model_type=model_type)\n","            if img is not None and img.shape == (size, size, 3):\n","                support_images.append(img)\n","                support_labels.append(cls)\n","            else:\n","                logging.warning(f\"B·ªè qua ·∫£nh support kh√¥ng h·ª£p l·ªá: {row['id_code']}, shape: {getattr(img, 'shape', 'None')}\")\n","\n","        for _, row in query_samples.iterrows():\n","            img = load_processed_image(row['id_code'], PROCESSED_FOLDER, model_type=model_type)\n","            if img is not None and img.shape == (size, size, 3):\n","                query_images.append(img)\n","                query_labels.append(cls)\n","            else:\n","                logging.warning(f\"B·ªè qua ·∫£nh query kh√¥ng h·ª£p l·ªá: {row['id_code']}, shape: {getattr(img, 'shape', 'None')}\")\n","\n","        included_classes.append(cls)\n","\n","    if not included_classes:\n","        logging.error(\"Kh√¥ng c√≥ l·ªõp n√†o c√≥ ƒë·ªß m·∫´u cho episode\")\n","        return np.zeros((0, size, size, 3)), np.zeros((0, num_classes)), np.zeros((0, size, size, 3)), np.zeros((0, num_classes))\n","\n","    if len(support_images) < support_size * len(included_classes) or len(query_images) < query_size * len(included_classes):\n","        logging.error(f\"Kh√¥ng ƒë·ªß ·∫£nh h·ª£p l·ªá: support_images={len(support_images)}, query_images={len(query_images)}\")\n","        return np.zeros((0, size, size, 3)), np.zeros((0, num_classes)), np.zeros((0, size, size, 3)), np.zeros((0, num_classes))\n","\n","    try:\n","        # Ki·ªÉm tra v√† in h√¨nh d·∫°ng c·ªßa t·ª´ng ·∫£nh\n","        for i, img in enumerate(support_images + query_images):\n","            if img.shape != (size, size, 3):\n","                logging.error(f\"·∫¢nh {i} c√≥ shape kh√¥ng h·ª£p l·ªá: {img.shape}\")\n","                raise ValueError(f\"·∫¢nh kh√¥ng h·ª£p l·ªá trong episode: shape {img.shape}\")\n","        support_images = np.array(support_images, dtype=np.float32)\n","        support_labels = tf.keras.utils.to_categorical(support_labels, num_classes)\n","        query_images = np.array(query_images, dtype=np.float32)\n","        query_labels = tf.keras.utils.to_categorical(query_labels, num_classes)\n","    except Exception as e:\n","        logging.error(f\"L·ªói khi chuy·ªÉn ƒë·ªïi m·∫£ng: {str(e)}\")\n","        return np.zeros((0, size, size, 3)), np.zeros((0, num_classes)), np.zeros((0, size, size, 3)), np.zeros((0, num_classes))\n","\n","    logging.info(f\"ƒê√£ t·∫°o episode: support_images={support_images.shape}, support_labels={support_labels.shape}, \"\n","                 f\"query_images={query_images.shape}, query_labels={query_labels.shape}, classes={included_classes}\")\n","    return support_images, support_labels, query_images, query_labels\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization, ReLU, GlobalAveragePooling2D, Dense, Dropout, Concatenate\n","from tensorflow.keras.models import Model\n","import numpy as np\n","import logging\n","import os\n","from sklearn.metrics import cohen_kappa_score, f1_score\n","\n","def build_meta_conv_model(img_shape, feature_2d_shape, num_classes):\n","    \"\"\"\n","    X√¢y d·ª±ng m√¥ h√¨nh meta-learning v·ªõi ki·∫øn tr√∫c ƒë∆°n gi·∫£n h∆°n.\n","    \"\"\"\n","    img_input = Input(shape=img_shape, name='img_input')\n","    x = Conv2D(32, (3, 3), padding='same', activation=None)(img_input)  # Gi·∫£m t·ª´ 64\n","    x = BatchNormalization(trainable=True)(x)\n","    x = ReLU()(x)\n","    x = MaxPooling2D((2, 2))(x)\n","    x = Conv2D(64, (3, 3), padding='same', activation=None)(x)  # Gi·∫£m t·ª´ 128\n","    x = BatchNormalization(trainable=True)(x)\n","    x = ReLU()(x)\n","    x = MaxPooling2D((2, 2))(x)\n","    x = Conv2D(128, (3, 3), padding='same', activation=None)(x)  # Gi·∫£m t·ª´ 256\n","    x = BatchNormalization(trainable=True)(x)\n","    x = ReLU()(x)\n","    x = MaxPooling2D((4, 4))(x)\n","    conv_features = x\n","\n","    x = GlobalAveragePooling2D()(conv_features)\n","\n","    feature_2d_input = Input(shape=feature_2d_shape, name='feature_2d_input')\n","    y = Dense(128, activation='relu')(feature_2d_input)  # Gi·∫£m t·ª´ 256\n","    y = BatchNormalization(trainable=True)(y)\n","    y = Dropout(0.5)(y)\n","\n","    combined = Concatenate()([x, y])\n","    z = Dense(64, activation='relu')(combined)  # Gi·∫£m t·ª´ 128\n","    z = BatchNormalization(trainable=True)(z)\n","    z = Dropout(0.5)(z)\n","    logits = Dense(num_classes, activation='softmax')(z)\n","\n","    model = Model(inputs=[img_input, feature_2d_input], outputs=[conv_features, logits], name='meta_conv_model')\n","    return model\n","\n","def maml_fomaml_train_manual(\n","    meta_model,\n","    data_df,\n","    valid_features,\n","    valid_labels,\n","    valid_images,\n","    input_dim,\n","    model_type=\"efficientnetb1\",\n","    n_episodes=40,\n","    inner_lr=0.01,\n","    outer_lr=0.001,\n","    fine_tune_lr=0.0001,\n","    support_size=16,\n","    query_size=16,\n","    save_dir=\"./features\",\n","    sample_ids=None,\n","    callbacks=None\n","):\n","    logging.info(f\"Kh·ªüi t·∫°o hu·∫•n luy·ªán MAML/FOMAML cho {model_type}\")\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=outer_lr)\n","    qwk_scores = []\n","    best_qwk = -1.0\n","    history = {\"qwk\": [], \"loss\": []}\n","    NUM_CLASSES = valid_labels.shape[1] if valid_labels is not None else 5\n","    BATCH_SIZE = 16\n","\n","    if valid_images.shape[0] != valid_features.shape[0]:\n","        logging.error(f\"S·ªë l∆∞·ª£ng m·∫´u kh√¥ng kh·ªõp: valid_images={valid_images.shape[0]}, valid_features={valid_features.shape[0]}\")\n","        raise ValueError(\"S·ªë l∆∞·ª£ng m·∫´u kh√¥ng kh·ªõp gi·ªØa ·∫£nh v√† ƒë·∫∑c tr∆∞ng.\")\n","    logging.info(f\"D·ªØ li·ªáu validation: images_shape={valid_images.shape}, features_shape={valid_features.shape}, labels_shape={valid_labels.shape}\")\n","\n","    config = MODEL_CONFIGS[model_type]\n","    input_shape = (config['img_size'], config['img_size'], 3)\n","    try:\n","        # Trong v√≤ng l·∫∑p model_name trong main_pipeline\n","        base_model = load_model_from_config(\n","            config['config_path'], config['weights_path'], config['base_model'], input_shape\n","        )\n","        feature_2d_layers = [\n","            layer for layer in base_model.layers\n","            if not isinstance(layer, tf.keras.layers.InputLayer) and len(layer.output.shape) == 2\n","        ]\n","        if not feature_2d_layers:\n","            logging.warning(f\"Kh√¥ng t√¨m th·∫•y t·∫ßng 2D trong {model_name}, th√™m GlobalAveragePooling2D\")\n","            x = base_model.output\n","            x = GlobalAveragePooling2D()(x)\n","            feature_2d_output = x\n","        else:\n","            feature_2d_output = feature_2d_layers[-1].output\n","        feature_extractor = tf.keras.Model(inputs=base_model.input, outputs=feature_2d_output)\n","        logging.info(f\"Feature extractor output shape: {feature_2d_output.shape}\")\n","    except Exception as e:\n","        logging.error(f\"L·ªói t·∫£i base model ho·∫∑c t·∫°o feature extractor cho {model_type}: {str(e)}\")\n","        raise\n","\n","    # Ti·∫øp t·ª•c v·ªõi c√°c b∆∞·ªõc c√≤n l·∫°i\n","    try:\n","        if not isinstance(valid_images, np.ndarray):\n","            logging.warning(f\"valid_images kh√¥ng ph·∫£i np.ndarray, type: {type(valid_images)}. Chuy·ªÉn ƒë·ªïi sang np.ndarray.\")\n","            valid_images = np.array(valid_images)\n","        if len(valid_images.shape) != 4:\n","            logging.error(f\"valid_images c√≥ shape kh√¥ng h·ª£p l·ªá: {valid_images.shape}. K·ª≥ v·ªçng m·∫£ng 4D.\")\n","            raise ValueError(\"valid_images ph·∫£i l√† m·∫£ng 4D.\")\n","\n","        valid_images_preprocessed = config['preprocess'](valid_images)\n","        if not isinstance(valid_images_preprocessed, np.ndarray):\n","            logging.warning(f\"valid_images_preprocessed kh√¥ng ph·∫£i np.ndarray, type: {type(valid_images_preprocessed)}. Chuy·ªÉn ƒë·ªïi sang np.ndarray.\")\n","            valid_images_preprocessed = np.array(valid_images_preprocessed)\n","        logging.info(f\"Valid images preprocessed shape: {valid_images_preprocessed.shape}, dtype: {valid_images_preprocessed.dtype}\")\n","        valid_features_2d = feature_extractor.predict(valid_images_preprocessed, batch_size=BATCH_SIZE, verbose=0)\n","        logging.info(f\"Valid features 2D shape: {valid_features_2d.shape}, dtype: {valid_features_2d.dtype}\")\n","        tf.keras.backend.clear_session()\n","        gc.collect()\n","        check_memory()\n","    except Exception as e:\n","        logging.error(f\"L·ªói ti·ªÅn x·ª≠ l√Ω ho·∫∑c tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng: {str(e)}\")\n","        raise\n","\n","    # ... (ph·∫ßn c√≤n l·∫°i c·ªßa h√†m gi·ªØ nguy√™n)\n","\n","    # X√¢y d·ª±ng meta_conv_model n·∫øu ch∆∞a c√≥\n","    if meta_model is None:\n","        try:\n","            meta_model = build_meta_conv_model(\n","                img_shape=input_shape,\n","                feature_2d_shape=(valid_features_2d.shape[1],),\n","                num_classes=NUM_CLASSES\n","            )\n","            logging.info(\"ƒê√£ x√¢y d·ª±ng meta_conv_model th√†nh c√¥ng.\")\n","        except Exception as e:\n","            logging.error(f\"L·ªói x√¢y d·ª±ng meta_conv_model: {str(e)}\")\n","            raise\n","\n","    # T·∫°o classification model cho tinh ch·ªânh v√† d·ª± ƒëo√°n\n","    img_input = tf.keras.Input(shape=input_shape, name='img_input')\n","    feature_2d_input = tf.keras.Input(shape=(valid_features_2d.shape[1],), name='feature_2d_input')\n","    conv_features, logits = meta_model([img_input, feature_2d_input])\n","    classification_model = tf.keras.Model(\n","        inputs=[img_input, feature_2d_input],\n","        outputs=logits,\n","        name='meta_classification_model'\n","    )\n","\n","    # Callback cho confusion matrix\n","    class_counts = np.sum(valid_labels, axis=0) if valid_labels is not None else None\n","    cm_callback = ConfusionMatrixWeightCallback(\n","        valid_features=[valid_images_preprocessed, valid_features_2d],\n","        valid_labels=valid_labels,\n","        classification_model=classification_model,\n","        num_classes=NUM_CLASSES,\n","        class_counts=class_counts\n","    )\n","    checkpoint_callback = MultiMetricCheckpoint(\n","        filepath=os.path.join(save_dir, f\"meta_model_best_{model_type}_{{epoch}}.weights.h5\"),\n","        monitor_metrics={'qwk': 'max', 'f1': 'max'},\n","        mode='max',\n","        save_best_only=True\n","    )\n","    callbacks = [cm_callback, checkpoint_callback] + (callbacks or [])\n","\n","    # Hu·∫•n luy·ªán t·ª´ng episode\n","    for episode in range(n_episodes):\n","        logging.info(f\"Hu·∫•n luy·ªán episode {episode + 1}/{n_episodes}\")\n","        try:\n","            # T·∫°o support v√† query set\n","            support_images, support_labels, query_images, query_labels = create_episode(\n","                data_df, support_size, query_size, NUM_CLASSES, model_type=model_type\n","            )\n","            if support_images.shape[0] == 0 or query_images.shape[0] == 0:\n","                logging.warning(f\"Episode r·ªóng t·∫°i {episode + 1}, b·ªè qua\")\n","                continue\n","\n","            # Ti·ªÅn x·ª≠ l√Ω v√† tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng\n","            support_preprocessed = config['preprocess'](support_images)\n","            query_preprocessed = config['preprocess'](query_images)\n","            support_features_2d = feature_extractor.predict(support_preprocessed, batch_size=BATCH_SIZE, verbose=0)\n","            query_features_2d = feature_extractor.predict(query_preprocessed, batch_size=BATCH_SIZE, verbose=0)\n","            tf.keras.backend.clear_session()\n","            gc.collect()\n","            check_memory()\n","\n","            logging.info(f\"Support set: images={support_preprocessed.shape}, features={support_features_2d.shape}\")\n","            logging.info(f\"Query set: images={query_preprocessed.shape}, features={query_features_2d.shape}\")\n","\n","            # Ki·ªÉm tra shape\n","            if support_preprocessed.shape[0] != support_features_2d.shape[0]:\n","                logging.error(f\"S·ªë l∆∞·ª£ng m·∫´u kh√¥ng kh·ªõp trong support set: images={support_preprocessed.shape[0]}, features={support_features_2d.shape[0]}\")\n","                raise ValueError(\"S·ªë l∆∞·ª£ng m·∫´u kh√¥ng kh·ªõp trong support set.\")\n","            if query_preprocessed.shape[0] != query_features_2d.shape[0]:\n","                logging.error(f\"S·ªë l∆∞·ª£ng m·∫´u kh√¥ng kh·ªõp trong query set: images={query_preprocessed.shape[0]}, features={query_features_2d.shape[0]}\")\n","                raise ValueError(\"S·ªë l∆∞·ª£ng m·∫´u kh√¥ng kh·ªõp trong query set.\")\n","\n","            # MAML inner loop (FOMAML style)\n","            fast_weights = [w.numpy() for w in meta_model.weights]  # L∆∞u tr·ªçng s·ªë g·ªëc\n","            with tf.GradientTape() as inner_tape:\n","                support_conv_features, support_logits = meta_model(\n","                    [support_preprocessed, support_features_2d], training=True\n","                )\n","                support_loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(support_labels, support_logits))\n","            inner_grads = inner_tape.gradient(support_loss, meta_model.trainable_variables)\n","\n","            # C·∫≠p nh·∫≠t tr·ªçng s·ªë nhanh\n","            fast_weights_updated = fast_weights.copy()\n","            trainable_indices = [i for i, w in enumerate(meta_model.weights) if w.trainable]\n","            grad_idx = 0\n","            for i in trainable_indices:\n","                if grad_idx < len(inner_grads) and inner_grads[grad_idx] is not None:\n","                    fast_weights_updated[i] = fast_weights[i] - inner_lr * inner_grads[grad_idx]\n","                grad_idx += 1\n","\n","            meta_model.set_weights(fast_weights_updated)  # C·∫≠p nh·∫≠t tr·ªçng s·ªë nhanh\n","\n","            # MAML outer loop\n","            with tf.GradientTape() as outer_tape:\n","                query_conv_features, query_logits = meta_model(\n","                    [query_preprocessed, query_features_2d], training=True\n","                )\n","                query_loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(query_labels, query_logits))\n","                query_logits = apply_temperature_scaling(query_logits, temperature=2.0)\n","                query_logits = laplace_smoothing(query_logits)\n","            outer_grads = outer_tape.gradient(query_loss, meta_model.trainable_variables)\n","            valid_grads_and_vars = [(g, v) for g, v in zip(outer_grads, meta_model.trainable_variables) if g is not None]\n","            if not valid_grads_and_vars:\n","                logging.error(\"Kh√¥ng c√≥ gradient ngo√†i h·ª£p l·ªá ƒë·ªÉ √°p d·ª•ng.\")\n","                raise ValueError(\"Kh√¥ng c√≥ gradient ngo√†i h·ª£p l·ªá.\")\n","            optimizer.apply_gradients(valid_grads_and_vars)\n","\n","            # Kh√¥i ph·ª•c tr·ªçng s·ªë g·ªëc\n","            meta_model.set_weights(fast_weights)\n","\n","            # ƒê√°nh gi√°\n","            y_true = np.argmax(query_labels, axis=1)\n","            y_pred = np.argmax(query_logits, axis=1)\n","            qwk = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n","            qwk_scores.append(qwk)\n","            history[\"qwk\"].append(float(qwk))\n","            history[\"loss\"].append(float(query_loss))\n","            logging.info(f\"Episode {episode + 1}: QWK={qwk:.4f}, Loss={query_loss:.4f}\")\n","\n","            # G·ªçi callbacks\n","            for callback in callbacks:\n","                callback.on_epoch_end(episode, logs={\"qwk\": qwk, \"f1\": f1_score(y_true, y_pred, average='weighted')})\n","\n","            # L∆∞u confusion matrix\n","            save_confusion_matrix(y_true, y_pred, episode, qwk, save_dir, prefix=f'meta_{model_type}_')\n","\n","            # L∆∞u m√¥ h√¨nh t·ªët nh·∫•t\n","            if qwk > best_qwk:\n","                best_qwk = qwk\n","                training_config = {\n","                    \"inner_lr\": float(inner_lr),\n","                    \"outer_lr\": float(outer_lr),\n","                    \"fine_tune_lr\": float(fine_tune_lr),\n","                    \"train_config\": {\n","                        \"support_size\": support_size,\n","                        \"query_size\": query_size,\n","                        \"n_episodes\": n_episodes,\n","                        \"model_type\": model_type\n","                    }\n","                }\n","                save_meta_learner_model(meta_model, save_dir, episode, qwk_scores, training_config)\n","                logging.info(f\"ƒê√£ l∆∞u m√¥ h√¨nh t·ªët nh·∫•t t·∫°i episode {episode + 1}\")\n","\n","            # Qu·∫£n l√Ω b·ªô nh·ªõ\n","            tf.keras.backend.clear_session()\n","            gc.collect()\n","            check_memory()\n","\n","        except Exception as e:\n","            logging.error(f\"L·ªói trong episode {episode + 1}: {str(e)}\")\n","            continue\n","\n","    # Tinh ch·ªânh classification_model\n","    try:\n","        classification_model.compile(\n","            optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tune_lr),\n","            loss='categorical_crossentropy',\n","            metrics=['accuracy']\n","        )\n","        classification_model.fit(\n","            [valid_images_preprocessed, valid_features_2d],\n","            valid_labels,\n","            batch_size=BATCH_SIZE,\n","            epochs=5,\n","            verbose=1,\n","            callbacks=callbacks\n","        )\n","        logging.info(\"Ho√†n th√†nh tinh ch·ªânh classification_model.\")\n","    except Exception as e:\n","        logging.error(f\"L·ªói khi tinh ch·ªânh classification_model: {str(e)}\")\n","        raise\n","\n","    return meta_model, classification_model, history\n","\n","# H√†m cross-validation\n","def cross_validate_pipeline(\n","    x, y, processed_folder, model_configs, feature_save_dir, n_splits=5, n_episodes=20, **kwargs\n","):\n","    \"\"\"\n","    Performs k-fold cross-validation for the meta-learning pipeline.\n","\n","    Args:\n","        x (pd.Series): Series of image IDs (id_code).\n","        y (np.ndarray): Array of labels (diagnosis).\n","        processed_folder (str): Path to folder containing processed images.\n","        model_configs (dict): Dictionary of model configurations (MODEL_CONFIGS).\n","        feature_save_dir (str): Directory to save features and results.\n","        n_splits (int): Number of folds for cross-validation.\n","        n_episodes (int): Number of episodes for meta-learning.\n","        **kwargs: Additional arguments for maml_fomaml_train_manual.\n","\n","    Returns:\n","        dict: Cross-validation results with QWK, F1, and recall scores.\n","    \"\"\"\n","    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n","    cv_results = {\"qwk\": [], \"f1\": [], \"recall\": []}\n","\n","    for fold, (train_idx, valid_idx) in enumerate(kf.split(x)):\n","        logging.info(f\"Processing fold {fold + 1}/{n_splits}\")\n","\n","        # Split data\n","        train_x, valid_x = x.iloc[train_idx].values, x.iloc[valid_idx].values\n","        train_y, valid_y = y[train_idx], y[valid_idx]\n","\n","        # Prepare image paths and filter missing images\n","        train_image_paths = [os.path.join(processed_folder, f\"{id_code}.png\") for id_code in train_x]\n","        valid_image_paths = [os.path.join(processed_folder, f\"{id_code}.png\") for id_code in valid_x]\n","        train_sample_ids = []\n","        train_y_filtered = []\n","        valid_sample_ids = []\n","        valid_y_filtered = []\n","\n","        # Filter train data\n","        for path, id_code, label in zip(train_image_paths, train_x, train_y):\n","            if os.path.exists(path):\n","                train_sample_ids.append(id_code)\n","                train_y_filtered.append(label)\n","            else:\n","                logging.warning(f\"Skipping non-existent train image: {path}\")\n","        train_y_filtered = np.array(train_y_filtered)\n","\n","        # Filter valid data\n","        for path, id_code, label in zip(valid_image_paths, valid_x, valid_y):\n","            if os.path.exists(path):\n","                valid_sample_ids.append(id_code)\n","                valid_y_filtered.append(label)\n","            else:\n","                logging.warning(f\"Skipping non-existent valid image: {path}\")\n","        valid_y_filtered = np.array(valid_y_filtered)\n","\n","        if len(train_sample_ids) == 0 or len(valid_sample_ids) == 0:\n","            logging.error(f\"Fold {fold + 1}: Empty train or valid set after filtering.\")\n","            continue\n","\n","        # Convert labels to one-hot encoding\n","        train_y_multi = tf.keras.utils.to_categorical(train_y_filtered, num_classes=NUM_CLASSES)\n","        valid_y_multi = tf.keras.utils.to_categorical(valid_y_filtered, num_classes=NUM_CLASSES)\n","\n","        # Load train and valid images\n","        train_images = np.array([\n","            img for img in [load_processed_image(id_code, processed_folder, model_type=\"efficientnetb1\") for id_code in train_sample_ids]\n","            if img is not None\n","        ])\n","        valid_images = np.array([\n","            img for img in [load_processed_image(id_code, processed_folder, model_type=\"efficientnetb1\") for id_code in valid_sample_ids]\n","            if img is not None\n","        ])\n","        train_y_multi = train_y_multi[:len(train_images)]  # Adjust labels to match images\n","        valid_y_multi = valid_y_multi[:len(valid_images)]\n","\n","        # Balance training data\n","        balanced_train_x, balanced_train_y_multi = balance_data(\n","            train_images, train_y_multi, target_classes=[0, 1, 2, 3, 4]\n","        )\n","\n","        # Initialize feature dictionaries\n","        train_features_dict, valid_features_dict = {}, {}\n","\n","        # Extract features for each model\n","        for model_name, config in model_configs.items():\n","            logging.info(f\"Extracting features for {model_name} in fold {fold + 1}\")\n","\n","            # Create generators\n","            train_generator = My_Generator(\n","                images=None,\n","                labels=balanced_train_y_multi,\n","                batch_size=BATCH_SIZE,\n","                is_train=True,\n","                mix=False,\n","                augment=False,\n","                model_type=config['model_type'],\n","                preprocess=config['preprocess'],\n","                image_paths=[os.path.join(processed_folder, f\"{id_code}.png\") for id_code in train_sample_ids],\n","                sample_ids=train_sample_ids,\n","                temp_augment_dir=TEMP_AUGMENT_DIR\n","            )\n","            valid_generator = My_Generator(\n","                images=None,\n","                labels=valid_y_multi,\n","                batch_size=BATCH_SIZE,\n","                is_train=False,\n","                mix=False,\n","                augment=False,\n","                model_type=config['model_type'],\n","                preprocess=config['preprocess'],\n","                image_paths=[os.path.join(processed_folder, f\"{id_code}.png\") for id_code in valid_sample_ids],\n","                sample_ids=valid_sample_ids,\n","                temp_augment_dir=TEMP_AUGMENT_DIR\n","            )\n","\n","            # Load model and create feature extractor\n","            base_model = load_model_from_config(\n","                config['config_path'],\n","                config['weights_path'],\n","                config['base_model'],\n","                input_shape=(config['img_size'], config['img_size'], 3)\n","            )\n","            feature_layer = base_model.layers[-2].output\n","            feature_extractor = Model(inputs=base_model.input, outputs=feature_layer)\n","\n","            # Extract and save features\n","            train_features, train_features_ids = extract_and_save_features(\n","                model_name, feature_extractor, train_generator, feature_save_dir, sample_ids=train_sample_ids\n","            )\n","            valid_features, valid_features_ids = extract_and_save_features(\n","                model_name, feature_extractor, valid_generator, feature_save_dir, sample_ids=valid_sample_ids\n","            )\n","\n","            train_features_dict[model_name] = train_features\n","            valid_features_dict[model_name] = valid_features\n","\n","            # Clean up\n","            del base_model, feature_extractor\n","            tf.keras.backend.clear_session()\n","            gc.collect()\n","\n","        # Combine and reduce features (skip PCA to keep 1024 dims)\n","        train_combined, train_labels, train_indices, input_dim = combine_and_reduce_features(\n","            train_features_dict, {}, balanced_train_y_multi, train_sample_ids, feature_save_dir, n_components=None\n","        )\n","        valid_combined, valid_labels, valid_indices, _ = combine_and_reduce_features(\n","            valid_features_dict, {}, valid_y_multi, valid_sample_ids, feature_save_dir, n_components=None\n","        )\n","\n","        # Create feature extractor for validation\n","        img_size = model_configs[\"efficientnetb1\"][\"img_size\"]\n","        base_model = load_model_from_config(\n","            model_configs[\"efficientnetb1\"][\"config_path\"],\n","            model_configs[\"efficientnetb1\"][\"weights_path\"],\n","            model_configs[\"efficientnetb1\"][\"base_model\"],\n","            input_shape=(img_size, img_size, 3)\n","        )\n","        feature_layer = base_model.layers[-2].output\n","        feature_extractor = Model(inputs=base_model.input, outputs=feature_layer)\n","        valid_images_preprocessed = model_configs[\"efficientnetb1\"][\"preprocess\"](valid_images)\n","        valid_features_2d = feature_extractor.predict(valid_images_preprocessed, batch_size=BATCH_SIZE)\n","        del base_model, feature_extractor\n","        tf.keras.backend.clear_session()\n","        gc.collect()\n","\n","        # Build meta-model\n","        meta_model = build_meta_conv_model(\n","            img_shape=(img_size, img_size, 3),\n","            feature_2d_shape=(1024,),\n","            num_classes=NUM_CLASSES\n","        )\n","\n","        # Train meta-model\n","        meta_model, meta_classification_model, history = maml_fomaml_train_manual(\n","            meta_model=meta_model,\n","            data_df=pd.DataFrame({'id_code': np.array(train_sample_ids)[train_indices], 'diagnosis': np.argmax(train_labels[train_indices], axis=1)}),\n","            valid_features=valid_features_2d,\n","            valid_labels=valid_y_multi[valid_indices],\n","            valid_images=valid_images[valid_indices],\n","            input_dim=input_dim,\n","            model_type=\"efficientnetb1\",\n","            n_episodes=n_episodes,\n","            sample_ids=np.array(valid_sample_ids)[valid_indices],\n","            **kwargs\n","        )\n","\n","        # Predict and evaluate\n","        logging.info(f\"Valid images shape: {valid_images_preprocessed.shape}\")\n","        logging.info(f\"Valid features 2D shape: {valid_features_2d.shape}\")\n","        if valid_images_preprocessed.shape[0] != valid_features_2d.shape[0]:\n","            logging.error(f\"Sample count mismatch: images={valid_images_preprocessed.shape[0]}, features={valid_features_2d.shape[0]}\")\n","            raise ValueError(\"Sample count mismatch\")\n","\n","        _, y_pred = meta_classification_model.predict(\n","            [valid_images_preprocessed[valid_indices], valid_features_2d[valid_indices]], batch_size=BATCH_SIZE\n","        )\n","        y_pred_classes = np.argmax(y_pred, axis=1)\n","        y_true = np.argmax(valid_y_multi[valid_indices], axis=1)\n","\n","        qwk = cohen_kappa_score(y_true, y_pred_classes, labels=[0, 1, 2, 3, 4], weights='quadratic')\n","        f1 = f1_score(y_true, y_pred_classes, average='weighted')\n","        recall = recall_score(y_true, y_pred_classes, average='weighted')\n","\n","        cv_results[\"qwk\"].append(qwk)\n","        cv_results[\"f1\"].append(f1)\n","        cv_results[\"recall\"].append(recall)\n","\n","        logging.info(f\"Fold {fold + 1}: QWK={qwk:.4f}, F1={f1:.4f}, Recall={recall:.4f}\")\n","\n","        # Save confusion matrix\n","        save_confusion_matrix(y_true, y_pred_classes, fold, qwk, feature_save_dir, prefix=f'fold_{fold + 1}_')\n","\n","        # Clean up\n","        del meta_model, meta_classification_model\n","        tf.keras.backend.clear_session()\n","        gc.collect()\n","\n","    # Summarize results\n","    cv_summary = {\n","        \"mean_qwk\": np.mean(cv_results[\"qwk\"]) if cv_results[\"qwk\"] else 0.0,\n","        \"std_qwk\": np.std(cv_results[\"qwk\"]) if cv_results[\"qwk\"] else 0.0,\n","        \"mean_f1\": np.mean(cv_results[\"f1\"]) if cv_results[\"f1\"] else 0.0,\n","        \"std_f1\": np.std(cv_results[\"f1\"]) if cv_results[\"f1\"] else 0.0,\n","        \"mean_recall\": np.mean(cv_results[\"recall\"]) if cv_results[\"recall\"] else 0.0,\n","        \"std_recall\": np.std(cv_results[\"recall\"]) if cv_results[\"recall\"] else 0.0\n","    }\n","\n","    summary_path = Path(feature_save_dir) / \"cross_validation_summary.json\"\n","    with open(summary_path, 'w') as f:\n","        json.dump(cv_summary, f, indent=4)\n","    logging.info(f\"Saved cross-validation summary at: {summary_path}\")\n","\n","    return cv_results\n","# H√†m hyperparameter tuning\n","def hyperparameter_tuning(\n","    x, y, processed_folder, model_configs, feature_save_dir, param_grid, n_iter=10, n_episodes=20\n","):\n","    if n_iter is None:\n","        param_combinations = list(product(*param_grid.values()))\n","    else:\n","        param_combinations = [\n","            {k: random.choice(v) for k, v in param_grid.items()} for _ in range(n_iter)\n","        ]\n","\n","    best_qwk = -float('inf')\n","    best_params = None\n","\n","    for params in param_combinations:\n","        logging.info(f\"Th·ª≠ tham s·ªë: {params}\")\n","\n","        cv_results = cross_validate_pipeline(\n","            x, y, processed_folder, model_configs, feature_save_dir, n_splits=3, n_episodes=n_episodes, **params\n","        )\n","\n","        mean_qwk = np.mean(cv_results[\"qwk\"])\n","        if mean_qwk > best_qwk:\n","            best_qwk = mean_qwk\n","            best_params = params\n","\n","        logging.info(f\"Mean QWK: {mean_qwk:.4f}\")\n","\n","    logging.info(f\"Tham s·ªë t·ªët nh·∫•t: {best_params}, QWK t·ªët nh·∫•t: {best_qwk:.4f}\")\n","\n","    return best_params, best_qwk\n","\n","# H√†m t·∫°o Grad-CAM heatmap\n","def generate_gradcam_heatmap(\n","    meta_conv_model,\n","    img_array,\n","    feature_2d,\n","    class_idx,\n","    conv_layer_output='conv2d_3'  # T√™n t·∫ßng cu·ªëi c√πng trong build_meta_conv_model\n","):\n","    try:\n","        # T·∫°o m√¥ h√¨nh trung gian ƒë·ªÉ l·∫•y conv_features\n","        intermediate_model = Model(\n","            inputs=meta_conv_model.input,\n","            outputs=meta_conv_model.get_layer(conv_layer_output).output\n","        )\n","        with tf.GradientTape() as tape:\n","            conv_output = intermediate_model([img_array, feature_2d], training=False)\n","            tape.watch(conv_output)\n","            _, predictions = meta_conv_model([img_array, feature_2d], training=False)\n","            loss = predictions[:, class_idx]\n","        grads = tape.gradient(loss, conv_output)\n","        if grads is None:\n","            logging.error(f\"Gradient tr·∫£ v·ªÅ None cho class {class_idx}\")\n","            return None\n","        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","        conv_output = conv_output[0]\n","        heatmap = tf.reduce_mean(tf.multiply(conv_output, pooled_grads), axis=-1)\n","        heatmap = tf.maximum(heatmap, 0) / (tf.reduce_max(heatmap) + 1e-10)\n","        heatmap = tf.image.resize(\n","            heatmap[..., tf.newaxis],\n","            [img_array.shape[1], img_array.shape[2]],\n","            method='bilinear'\n","        )\n","        heatmap = tf.squeeze(heatmap).numpy()\n","        return heatmap\n","    except Exception as e:\n","        logging.error(f\"L·ªói khi t·∫°o Grad-CAM: {str(e)}\")\n","        return None\n","\n","# H√†m ensemble predictions\n","def ensemble_predictions(model_configs, generators, test_ids, feature_save_dir, weights=None, meta_classification_model=None):\n","    predictions = []\n","\n","    for model_name, config in model_configs.items():\n","        logging.info(f\"Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng cho {model_name}...\")\n","        base_model = load_model_from_config(\n","            config['config_path'], config['weights_path'], config['base_model']\n","        )\n","        feature_layer = base_model.layers[-2].output\n","        feature_extractor = Model(inputs=base_model.input, outputs=feature_layer)\n","\n","        features_2d, _ = extract_and_save_features(\n","            model_name,\n","            feature_extractor,\n","            generators[model_name],\n","            feature_save_dir,\n","            sample_ids=test_ids\n","        )\n","\n","        probs = meta_classification_model.predict(features_2d, batch_size=32)\n","        predictions.append(probs)\n","\n","        del base_model, feature_extractor\n","        tf.keras.backend.clear_session()\n","        gc.collect()\n","        logging.info(f\"ƒê√£ tr√≠ch xu·∫•t {len(features_2d)} ƒë·∫∑c tr∆∞ng cho {model_name}\")\n","\n","    weights = weights or [1.0 / len(predictions)] * len(predictions)\n","    ensemble_probs = np.average(predictions, axis=0, weights=weights)\n","    return ensemble_probs\n","\n","# H√†m t·∫°o pipeline report\n","def generate_pipeline_report(\n","    dataset_stats, cv_results, test_metrics, runtime, save_dir, filename=\"pipeline_report.json\"\n","):\n","    report = {\n","        \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n","        \"dataset_statistics\": dataset_stats,\n","        \"cross_validation_results\": cv_results,\n","        \"test_metrics\": test_metrics,\n","        \"runtime_seconds\": runtime,\n","        \"model_configurations\": list(MODEL_CONFIGS.keys()),\n","        \"num_classes\": NUM_CLASSES,\n","        \"image_size\": SIZE\n","    }\n","\n","    report_path = Path(save_dir) / filename\n","    with open(report_path, 'w') as f:\n","        json.dump(report, f, indent=4)\n","    logging.info(f\"ƒê√£ l∆∞u b√°o c√°o pipeline t·∫°i: {report_path}\")\n","\n","\n","def prepare_data():\n","    try:\n","        df_train = pd.read_csv(os.path.join(DRIVE_FOLDER, \"train.csv\"))\n","        logging.info(f\"ƒê√£ ƒë·ªçc train.csv: {len(df_train)} m·∫´u\")\n","    except Exception as e:\n","        logging.error(f\"L·ªói khi ƒë·ªçc train.csv: {str(e)}\")\n","        return None, None, None, None, None, None, None, None, None, None, None, None\n","\n","    if not {'id_code', 'diagnosis'}.issubset(df_train.columns):\n","        logging.error(\"File train.csv thi·∫øu c·ªôt 'id_code' ho·∫∑c 'diagnosis'.\")\n","        return None, None, None, None, None, None, None, None, None, None, None, None\n","\n","    # X√°c th·ª±c dataset m·ªôt l·∫ßn\n","    processed_image_ids = validate_dataset(get_image_ids(PROCESSED_FOLDER), PROCESSED_FOLDER, FEATURE_SAVE_DIR)\n","    if not processed_image_ids:\n","        logging.error(\"Kh√¥ng t√¨m th·∫•y ·∫£nh trong PROCESSED_FOLDER.\")\n","        return None, None, None, None, None, None, None, None, None, None, None, None\n","    logging.info(f\"T√¨m th·∫•y {len(processed_image_ids)} ID ·∫£nh trong PROCESSED_FOLDER: {processed_image_ids[:5]}...\")\n","\n","    # L·ªçc df_train\n","    df_train_processed = df_train[df_train['id_code'].isin(processed_image_ids)].copy()\n","    logging.info(f\"S·ªë m·∫´u sau khi l·ªçc v·ªõi PROCESSED_FOLDER: {len(df_train_processed)}\")\n","\n","    if df_train_processed.empty:\n","        logging.error(\"Kh√¥ng c√≥ m·∫´u h·ª£p l·ªá sau khi l·ªçc.\")\n","        return None, None, None, None, None, None, None, None, None, None, None, None\n","\n","    x = df_train_processed['id_code']\n","    y = df_train_processed['diagnosis']\n","    x, y = shuffle(x, y, random_state=42)\n","\n","    # Chia d·ªØ li·ªáu\n","    x_temp, test_x, y_temp, test_y = train_test_split(x, y, test_size=0.20, stratify=y, random_state=42)\n","    train_x, valid_x, train_y, valid_y = train_test_split(x_temp, y_temp, test_size=0.15/0.80, stratify=y_temp, random_state=42)\n","\n","    train_ids = np.array(train_x)\n","    valid_ids = np.array(valid_x)\n","    test_ids = np.array(test_x)\n","\n","    # Ki·ªÉm tra r√≤ r·ªâ d·ªØ li·ªáu\n","    if not check_data_leakage(train_ids, valid_ids, test_ids, df_train_processed):\n","        logging.error(\"Ph√°t hi·ªán data leakage.\")\n","        raise ValueError(\"Data leakage ƒë∆∞·ª£c ph√°t hi·ªán.\")\n","\n","    logging.info(f\"S·ªë m·∫´u: Train={len(train_x)}, Valid={len(valid_x)}, Test={len(test_x)}\")\n","\n","    # T·∫£i ·∫£nh train\n","    train_images = []\n","    train_ids_filtered = []\n","    train_y_filtered = []\n","    train_error_ids = []\n","    for id_code, label in zip(train_x, train_y):\n","        img = load_processed_image(id_code, PROCESSED_FOLDER, model_type=\"efficientnetb1\")\n","        if img is not None:\n","            train_images.append(img)\n","            train_ids_filtered.append(id_code)\n","            train_y_filtered.append(label)\n","        else:\n","            train_error_ids.append(id_code)\n","            logging.warning(f\"Kh√¥ng t·∫£i ƒë∆∞·ª£c ·∫£nh train: {id_code}\")\n","    if not train_images:\n","        logging.error(\"Kh√¥ng t·∫£i ƒë∆∞·ª£c ·∫£nh train n√†o.\")\n","        raise ValueError(\"T·∫≠p train r·ªóng sau khi l·ªçc.\")\n","    train_images = np.array(train_images)\n","    train_y_filtered = np.array(train_y_filtered)\n","    train_ids_filtered = np.array(train_ids_filtered)\n","    logging.info(f\"Train: {len(train_images)} ·∫£nh, {len(train_ids_filtered)} IDs, {len(train_y_filtered)} nh√£n, {len(train_error_ids)} l·ªói\")\n","\n","    # T·∫£i ·∫£nh valid\n","    valid_images = []\n","    valid_ids_filtered = []\n","    valid_y_filtered = []\n","    valid_error_ids = []\n","    for id_code, label in zip(valid_x, valid_y):\n","        img = load_processed_image(id_code, PROCESSED_FOLDER, model_type=\"efficientnetb1\")\n","        if img is not None:\n","            valid_images.append(img)\n","            valid_ids_filtered.append(id_code)\n","            valid_y_filtered.append(label)\n","        else:\n","            valid_error_ids.append(id_code)\n","            logging.warning(f\"Kh√¥ng t·∫£i ƒë∆∞·ª£c ·∫£nh valid: {id_code}\")\n","    valid_images = np.array(valid_images)\n","    valid_y_filtered = np.array(valid_y_filtered)\n","    valid_ids_filtered = np.array(valid_ids_filtered)\n","    logging.info(f\"Valid: {len(valid_images)} ·∫£nh, {len(valid_ids_filtered)} IDs, {len(valid_y_filtered)} nh√£n, {len(valid_error_ids)} l·ªói\")\n","\n","    # T·∫£i ·∫£nh test\n","    test_images = []\n","    test_ids_filtered = []\n","    test_y_filtered = []\n","    test_error_ids = []\n","    for id_code, label in zip(test_x, test_y):\n","        img = load_processed_image(id_code, PROCESSED_FOLDER, model_type=\"efficientnetb1\")\n","        if img is not None:\n","            test_images.append(img)\n","            test_ids_filtered.append(id_code)\n","            test_y_filtered.append(label)\n","        else:\n","            test_error_ids.append(id_code)\n","            logging.warning(f\"Kh√¥ng t·∫£i ƒë∆∞·ª£c ·∫£nh test: {id_code}\")\n","    test_images = np.array(test_images)\n","    test_y_filtered = np.array(test_y_filtered)\n","    test_ids_filtered = np.array(test_ids_filtered)\n","    logging.info(f\"Test: {len(test_images)} ·∫£nh, {len(test_ids_filtered)} IDs, {len(test_y_filtered)} nh√£n, {len(test_error_ids)} l·ªói\")\n","\n","    # L∆∞u b√°o c√°o l·ªói\n","    error_report = {\n","        \"train_error_ids\": train_error_ids,\n","        \"valid_error_ids\": valid_error_ids,\n","        \"test_error_ids\": test_error_ids,\n","        \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    }\n","    error_report_path = os.path.join(FEATURE_SAVE_DIR, \"image_load_errors.json\")\n","    with open(error_report_path, 'w') as f:\n","        json.dump(error_report, f, indent=4)\n","    logging.info(f\"ƒê√£ l∆∞u b√°o c√°o ·∫£nh l·ªói t·∫°i: {error_report_path}\")\n","\n","    # Chuy·ªÉn nh√£n sang one-hot encoding\n","    train_y_multi = tf.keras.utils.to_categorical(train_y_filtered, num_classes=NUM_CLASSES)\n","    valid_y_multi = tf.keras.utils.to_categorical(valid_y_filtered, num_classes=NUM_CLASSES)\n","    test_y_multi = tf.keras.utils.to_categorical(test_y_filtered, num_classes=NUM_CLASSES)\n","\n","    # Kh√¥ng c√¢n b·∫±ng d·ªØ li·ªáu\n","    balanced_train_x = train_images\n","    balanced_train_y_multi = train_y_multi\n","    balanced_train_ids = train_ids_filtered\n","\n","    # B√°o c√°o ph√¢n b·ªë nh√£n\n","    label_indices = np.argmax(balanced_train_y_multi, axis=1)\n","    class_counts = np.bincount(label_indices, minlength=NUM_CLASSES)\n","    logging.info(f\"Ph√¢n b·ªë nh√£n t·∫≠p train: {dict(zip(range(NUM_CLASSES), class_counts))}\")\n","\n","    return (balanced_train_x, balanced_train_y_multi, valid_images, valid_y_multi, test_images, test_y_multi,\n","            df_train_processed, train_ids_filtered, valid_ids_filtered, test_ids_filtered, balanced_train_ids, processed_image_ids)\n","\n","def main_pipeline(\n","    balanced_train_x, balanced_train_y_multi, valid_images, valid_y_multi, test_images, test_y_multi,\n","    df_train_processed, train_ids, valid_ids, test_ids, balanced_train_ids, processed_image_ids\n","):\n","    start_time = time.time()\n","    logging.info(\"Starting pipeline...\")\n","\n","    # Initialize dictionaries for features\n","    train_features_dict = {}\n","    valid_features_dict = {}\n","    test_features_dict = {}\n","    meta_models = {}\n","    BATCH_SIZE = 16  # Reduced for memory efficiency\n","\n","    # Filter test data to match processed IDs\n","    test_indices = [i for i, sid in enumerate(test_ids) if sid in processed_image_ids]\n","    if not test_indices:\n","        logging.error(\"No test samples match processed_image_ids.\")\n","        raise ValueError(\"No valid test samples.\")\n","    test_images = test_images[test_indices]\n","    test_y_multi = test_y_multi[test_indices]\n","    test_ids = [test_ids[i] for i in test_indices]\n","    logging.info(f\"Filtered test data: {len(test_indices)} samples\")\n","\n","    # Process each model\n","    for model_name in MODEL_CONFIGS:\n","        logging.info(f\"Processing {model_name}...\")\n","        try:\n","            config = MODEL_CONFIGS[model_name]\n","            input_shape = (config['img_size'], config['img_size'], 3)\n","\n","            # Load base model and create feature extractor\n","            base_model = load_model_from_config(\n","                config['config_path'], config['weights_path'], config['base_model'], input_shape\n","            )\n","            feature_2d_layers = [\n","                layer for layer in base_model.layers\n","                if not isinstance(layer, tf.keras.layers.InputLayer) and len(layer.output.shape) == 2\n","            ]\n","            if not feature_2d_layers:\n","                logging.error(f\"No 2D layers found in {model_name}\")\n","                continue\n","            selected_layer = feature_2d_layers[-1]\n","            feature_extractor = tf.keras.Model(inputs=base_model.input, outputs=selected_layer.output)\n","            logging.info(f\"Feature extractor output shape: {selected_layer.output.shape}\")\n","\n","            # Create generators\n","            train_generator = My_Generator(\n","                images=balanced_train_x, labels=balanced_train_y_multi, batch_size=BATCH_SIZE,\n","                is_train=True, model_type=model_name, preprocess=config['preprocess'],\n","                sample_ids=balanced_train_ids\n","            )\n","            valid_generator = My_Generator(\n","                images=valid_images, labels=valid_y_multi, batch_size=BATCH_SIZE,\n","                model_type=model_name, preprocess=config['preprocess'], sample_ids=valid_ids\n","            )\n","            test_generator = My_Generator(\n","                images=test_images, labels=test_y_multi, batch_size=BATCH_SIZE,\n","                model_type=model_name, preprocess=config['preprocess'], sample_ids=test_ids\n","            )\n","\n","            # Extract features\n","            def extract_features(generator, feature_extractor):\n","                features = []\n","                feature_ids = []\n","                for batch_idx in range(len(generator)):\n","                    batch_images, _, batch_ids = generator[batch_idx]\n","                    if len(batch_images) == 0:\n","                        logging.warning(f\"Empty batch {batch_idx}, skipping\")\n","                        continue\n","                    batch_features = feature_extractor.predict(\n","                        batch_images, batch_size=BATCH_SIZE, verbose=0\n","                    )\n","                    if len(batch_features.shape) > 2:\n","                        batch_features = np.mean(batch_features, axis=(1, 2))\n","                    features.append(batch_features)\n","                    feature_ids.extend(batch_ids)\n","                    tf.keras.backend.clear_session()\n","                    gc.collect()\n","                if not features:\n","                    raise ValueError(\"No features extracted.\")\n","                return np.concatenate(features, axis=0), feature_ids\n","\n","            train_features, train_feature_ids = extract_features(train_generator, feature_extractor)\n","            valid_features, valid_feature_ids = extract_features(valid_generator, feature_extractor)\n","            test_features, test_feature_ids = extract_features(test_generator, feature_extractor)\n","\n","            # Verify feature consistency\n","            if len(train_features) != len(balanced_train_ids) or len(valid_features) != len(valid_ids) or len(test_features) != len(test_ids):\n","                logging.error(f\"Feature count mismatch for {model_name}\")\n","                continue\n","\n","            train_features_dict[model_name] = train_features\n","            valid_features_dict[model_name] = valid_features\n","            test_features_dict[model_name] = test_features\n","            logging.info(f\"Extracted features for {model_name}: train={len(train_features)}, valid={len(valid_features)}, test={len(test_features)}\")\n","\n","            # Save features\n","            os.makedirs(FEATURE_SAVE_DIR, exist_ok=True)\n","            np.save(os.path.join(FEATURE_SAVE_DIR, f\"{model_name}_train_features.npy\"), train_features)\n","            np.save(os.path.join(FEATURE_SAVE_DIR, f\"{model_name}_valid_features.npy\"), valid_features)\n","            np.save(os.path.join(FEATURE_SAVE_DIR, f\"{model_name}_test_features.npy\"), test_features)\n","\n","            # Train meta-model\n","            meta_model = build_meta_conv_model(\n","                img_shape=input_shape,\n","                feature_2d_shape=(train_features.shape[1],),\n","                num_classes=NUM_CLASSES\n","            )\n","            cm_callback = ConfusionMatrixWeightCallback(\n","                valid_features=[config['preprocess'](valid_images), valid_features],\n","                valid_labels=valid_y_multi,\n","                classification_model=meta_model,\n","                num_classes=NUM_CLASSES,\n","                class_counts=np.sum(valid_y_multi, axis=0)\n","            )\n","            checkpoint_callback = MultiMetricCheckpoint(\n","                filepath=os.path.join(FEATURE_SAVE_DIR, f\"meta_model_best_{model_name}_{{epoch}}.weights.h5\"),\n","                monitor_metrics={'qwk': 'max', 'f1': 'max'},\n","                mode='max',\n","                save_best_only=True\n","            )\n","            meta_model, history = maml_fomaml_train_manual(\n","                meta_model=meta_model,\n","                data_df=df_train_processed,\n","                valid_features=valid_features,\n","                valid_labels=valid_y_multi,\n","                valid_images=valid_images,\n","                input_dim=train_features.shape[1],\n","                model_type=model_name,\n","                support_size=16,\n","                query_size=16,\n","                save_dir=FEATURE_SAVE_DIR,\n","                sample_ids=valid_ids,\n","                callbacks=[cm_callback, checkpoint_callback]\n","            )\n","            meta_models[model_name] = meta_model\n","            logging.info(f\"Trained meta-model for {model_name}\")\n","\n","            # Clear memory\n","            del base_model, feature_extractor, train_generator, valid_generator, test_generator\n","            tf.keras.backend.clear_session()\n","            gc.collect()\n","            check_memory()\n","\n","        except Exception as e:\n","            logging.error(f\"Error processing {model_name}: {str(e)}\")\n","            continue\n","\n","    # Ensemble predictions\n","    logging.info(\"Ensembling predictions...\")\n","    ensemble_probs = []\n","    successful_models = []\n","    for model_name in meta_models:\n","        try:\n","            test_features = test_features_dict[model_name]\n","            test_images_preprocessed = MODEL_CONFIGS[model_name]['preprocess'](test_images)\n","            probs = meta_models[model_name].predict(\n","                [test_images_preprocessed, test_features],\n","                batch_size=BATCH_SIZE,\n","                verbose=0\n","            )[1]  # Get softmax probabilities\n","            probs = apply_temperature_scaling(probs, temperature=0.1)\n","            probs = laplace_smoothing(probs)\n","            ensemble_probs.append(probs)\n","            successful_models.append(model_name)\n","            logging.info(f\"Predicted for {model_name}: shape={probs.shape}\")\n","        except Exception as e:\n","            logging.error(f\"Prediction error for {model_name}: {str(e)}\")\n","            continue\n","\n","    # Compute final predictions\n","    if not ensemble_probs:\n","        logging.warning(\"No valid predictions. Using default zero probabilities.\")\n","        final_probs = np.zeros((len(test_y_multi), NUM_CLASSES))\n","        y_pred_classes = np.zeros(len(test_y_multi), dtype=np.int32)\n","        y_true = np.argmax(test_y_multi, axis=1)\n","    else:\n","        weights = np.array([1.0 / len(ensemble_probs)] * len(ensemble_probs))\n","        final_probs = np.average(ensemble_probs, axis=0, weights=weights)\n","        y_pred_classes = np.argmax(final_probs, axis=1)\n","        y_true = np.argmax(test_y_multi, axis=1)\n","\n","    # Evaluate\n","    qwk = cohen_kappa_score(y_true, y_pred_classes, labels=range(NUM_CLASSES), weights='quadratic')\n","    f1 = f1_score(y_true, y_pred_classes, average='weighted')\n","    recall = recall_score(y_true, y_pred_classes, average='weighted')\n","    logging.info(f\"Ensemble results: QWK={qwk:.4f}, F1={f1:.4f}, Recall={recall:.2f}\")\n","\n","    # Generate report\n","    test_metrics = {\n","        \"qwk\": float(qwk),\n","        \"f1_score\": float(f1),\n","        \"recall\": float(recall),\n","        \"successful_models\": successful_models\n","    }\n","    dataset_stats = {\n","        \"train_samples\": len(balanced_train_x),\n","        \"valid_samples\": len(valid_images),\n","        \"test_samples\": len(test_images),\n","        \"class_distribution\": np.sum(test_y_multi, axis=0).tolist()\n","    }\n","    runtime = time.time() - start_time\n","    generate_pipeline_report(dataset_stats, {}, test_metrics, runtime, FEATURE_SAVE_DIR)\n","\n","    logging.info(f\"Pipeline completed in {runtime:.2f} seconds.\")\n","    return {\n","        \"train_features_dict\": train_features_dict,\n","        \"valid_features_dict\": valid_features_dict,\n","        \"test_features_dict\": test_features_dict,\n","        \"meta_models\": meta_models\n","    }\n","\n","def run_pipeline(tune_hyperparameters=False, use_gpu=True):\n","    start_time = time.time()\n","    logging.info(f\"B·∫Øt ƒë·∫ßu pipeline t·∫°i: {datetime.now().strftime('%H:%M:%S %d/%m/%Y')}\")\n","\n","    try:\n","        # Ki·ªÉm tra m√¥i tr∆∞·ªùng\n","        if not os.path.ismount('/content/drive'):\n","            logging.error(\"Google Drive ch∆∞a ƒë∆∞·ª£c mount.\")\n","            raise RuntimeError(\"Y√™u c·∫ßu mount Google Drive.\")\n","        for folder in [FEATURE_SAVE_DIR, PROCESSED_FOLDER, DRIVE_FOLDER]:\n","            if not os.path.exists(folder):\n","                logging.error(f\"Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i: {folder}\")\n","                raise FileNotFoundError(f\"Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i: {folder}\")\n","            logging.info(f\"Th∆∞ m·ª•c t·ªìn t·∫°i: {folder}\")\n","\n","        # C·∫•u h√¨nh GPU ho·∫∑c CPU\n","        if use_gpu:\n","            physical_devices = tf.config.list_physical_devices('GPU')\n","            if physical_devices:\n","                for device in physical_devices:\n","                    tf.config.experimental.set_memory_growth(device, True)\n","                    logging.info(f\"ƒê√£ b·∫≠t memory growth cho {device}\")\n","            else:\n","                logging.warning(\"Kh√¥ng t√¨m th·∫•y GPU, chuy·ªÉn sang CPU.\")\n","                use_gpu = False\n","        else:\n","            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n","            logging.info(\"Ch·∫°y tr√™n CPU theo y√™u c·∫ßu.\")\n","\n","        os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'\n","        logging.info(\"ƒê√£ t·∫Øt XLA compilation.\")\n","        from tensorflow.keras.mixed_precision import set_global_policy\n","        set_global_policy('mixed_float16')\n","        logging.info(\"ƒê√£ b·∫≠t mixed precision.\")\n","\n","        # Chu·∫©n b·ªã d·ªØ li·ªáu\n","        logging.info(\"Chu·∫©n b·ªã d·ªØ li·ªáu...\")\n","        data = prepare_data()\n","        if data is None or any(x is None for x in data):\n","            logging.error(\"Chu·∫©n b·ªã d·ªØ li·ªáu th·∫•t b·∫°i.\")\n","            raise ValueError(\"Chu·∫©n b·ªã d·ªØ li·ªáu th·∫•t b·∫°i.\")\n","\n","        (balanced_train_x, balanced_train_y_multi, valid_images, valid_y_multi, test_images, test_y_multi,\n","         df_train_processed, train_ids, valid_ids, test_ids, balanced_train_ids, processed_image_ids) = data\n","\n","        logging.info(f\"K√≠ch th∆∞·ªõc dataset: train={len(balanced_train_x)}, valid={len(valid_images)}, test={len(test_images)}\")\n","\n","        # Ki·ªÉm tra r√≤ r·ªâ d·ªØ li·ªáu\n","        if not check_data_leakage(train_ids, valid_ids, test_ids, df_train_processed):\n","            logging.error(\"Ph√°t hi·ªán r√≤ r·ªâ d·ªØ li·ªáu.\")\n","            raise ValueError(\"R√≤ r·ªâ d·ªØ li·ªáu ƒë∆∞·ª£c ph√°t hi·ªán.\")\n","        logging.info(\"Kh√¥ng ph√°t hi·ªán r√≤ r·ªâ d·ªØ li·ªáu.\")\n","\n","        # Ki·ªÉm tra c·∫•u h√¨nh m√¥ h√¨nh\n","        validate_model_configs(MODEL_CONFIGS)\n","        logging.info(\"X√°c th·ª±c c·∫•u h√¨nh m√¥ h√¨nh ho√†n t·∫•t.\")\n","\n","        # T·ªëi ∆∞u si√™u tham s·ªë\n","        best_params = {'inner_lr': 0.01, 'outer_lr': 0.001, 'batch_size': 8}\n","        if tune_hyperparameters:\n","            logging.info(\"T·ªëi ∆∞u si√™u tham s·ªë...\")\n","            param_grid = {\n","                'inner_lr': [0.01, 0.001],\n","                'outer_lr': [0.001, 0.0001],\n","                'batch_size': [8, 16]\n","            }\n","            best_params, best_qwk = hyperparameter_tuning(\n","                df_train_processed['id_code'],\n","                df_train_processed['diagnosis'].values,\n","                PROCESSED_FOLDER,\n","                MODEL_CONFIGS,\n","                FEATURE_SAVE_DIR,\n","                param_grid,\n","                n_iter=5,\n","                n_episodes=10\n","            )\n","            logging.info(f\"Tham s·ªë t·ªët nh·∫•t: {best_params}, QWK t·ªët nh·∫•t: {best_qwk:.4f}\")\n","\n","        # Ki·ªÉm tra b·ªô nh·ªõ\n","        available_memory = check_memory()\n","        if available_memory is None or available_memory < 1.0:\n","            logging.warning(f\"B·ªô nh·ªõ th·∫•p: {available_memory} GB.\")\n","\n","        # Ch·∫°y pipeline ch√≠nh\n","        logging.info(\"Ch·∫°y pipeline ch√≠nh...\")\n","        pipeline_results = main_pipeline(\n","            balanced_train_x,\n","            balanced_train_y_multi,\n","            valid_images,\n","            valid_y_multi,\n","            test_images,\n","            test_y_multi,\n","            df_train_processed,\n","            train_ids,\n","            valid_ids,\n","            test_ids,\n","            balanced_train_ids,\n","            processed_image_ids\n","        )\n","\n","        train_features_dict = pipeline_results['train_features_dict']\n","        valid_features_dict = pipeline_results['valid_features_dict']\n","        test_features_dict = pipeline_results['test_features_dict']\n","\n","        # T·∫°o b√°o c√°o\n","        dataset_stats = {\n","            'train_samples': len(balanced_train_x),\n","            'valid_samples': len(valid_images),\n","            'test_samples': len(test_images),\n","            'class_distribution': np.bincount(np.argmax(balanced_train_y_multi, axis=1)).tolist()\n","        }\n","        cv_results = {}\n","        test_metrics = {}\n","        runtime = time.time() - start_time\n","        generate_pipeline_report(dataset_stats, cv_results, test_metrics, runtime, FEATURE_SAVE_DIR)\n","\n","        logging.info(f\"Pipeline ho√†n t·∫•t sau {runtime:.2f} gi√¢y!\")\n","        return train_features_dict, valid_features_dict, test_features_dict\n","\n","    except Exception as e:\n","        logging.error(f\"Pipeline th·∫•t b·∫°i: {str(e)}\")\n","        raise\n","    finally:\n","        logging.info(\"D·ªçn d·∫πp t√†i nguy√™n...\")\n","        tf.keras.backend.clear_session()\n","        gc.collect()\n","        logging.info(f\"Pipeline k·∫øt th√∫c t·∫°i: {datetime.now().strftime('%H:%M:%S %d/%m/%Y')}\")\n","\n","if __name__ == \"__main__\":\n","    # Ch·∫°y pipeline v·ªõi t√πy ch·ªçn hyperparameter tuning\n","    train_features_dict, valid_features_dict, test_features_dict = run_pipeline(tune_hyperparameters=False)"]},{"cell_type":"code","source":["!pip install GPUtil"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6WqhZ6UtDZ6v","executionInfo":{"status":"ok","timestamp":1749022094058,"user_tz":-420,"elapsed":3988,"user":{"displayName":"Resconnect","userId":"00882996269091038835"}},"outputId":"e9beb35b-0e86-4e09-dd27-ac544e2684b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting GPUtil\n","  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: GPUtil\n","  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=9eb9bb1c2a4699c385210cbaf09bca6ca453f02d1af6995b7b6ea6fe967e853b\n","  Stored in directory: /root/.cache/pip/wheels/2b/4d/8f/55fb4f7b9b591891e8d3f72977c4ec6c7763b39c19f0861595\n","Successfully built GPUtil\n","Installing collected packages: GPUtil\n","Successfully installed GPUtil-1.4.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53,"status":"ok","timestamp":1748749944958,"user":{"displayName":"Resconnect","userId":"00882996269091038835"},"user_tz":-420},"id":"oVrwhQPC9CYm","outputId":"39e0c091-967a-467b-8b99-afd1863c3830"},"outputs":[{"name":"stdout","output_type":"stream","text":["B·∫Øt ƒë·∫ßu so s√°nh id_code v·ªõi PROCESSED_FOLDER...\n","ƒê√£ ƒë·ªçc file CSV: 3662 m·∫´u\n","S·ªë id_code trong df_train: 3662\n","M·∫´u id_code: ['2fde69f20585', '8dc22e65c06f', 'ea05c22d92e9', 'a721efb1e049', '0eced86c9db8']...\n","S·ªë t·ªáp ·∫£nh trong PROCESSED_FOLDER: 3662\n","M·∫´u processed_ids: ['2fde69f20585', '8dc22e65c06f', 'ea05c22d92e9', 'a721efb1e049', '0eced86c9db8']...\n","S·ªë ID kh·ªõp: 3662\n","S·ªë ID trong df_train nh∆∞ng kh√¥ng c√≥ trong PROCESSED_FOLDER: 0\n","S·ªë t·ªáp ·∫£nh trong PROCESSED_FOLDER nh∆∞ng kh√¥ng c√≥ trong df_train: 0\n","ƒê√£ l∆∞u b√°o c√°o t·∫°i: /content/drive/MyDrive/working/id_code_comparison_report.json\n","Ho√†n t·∫•t so s√°nh.\n"]}],"source":["import pandas as pd\n","import os\n","import json\n","from pathlib import Path\n","\n","def compare_id_code_with_processed_folder(csv_path, processed_folder, output_dir):\n","    \"\"\"\n","    So s√°nh df_train['id_code'] v·ªõi c√°c t·ªáp ·∫£nh trong PROCESSED_FOLDER.\n","\n","    Args:\n","        csv_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file train.csv\n","        processed_folder (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c PROCESSED_FOLDER ch·ª©a ·∫£nh .png\n","        output_dir (str): Th∆∞ m·ª•c ƒë·ªÉ l∆∞u b√°o c√°o JSON\n","    \"\"\"\n","    try:\n","        # Ki·ªÉm tra v√† ƒë·ªçc file CSV\n","        if not os.path.exists(csv_path):\n","            print(f\"ERROR: File CSV kh√¥ng t·ªìn t·∫°i: {csv_path}\")\n","            return\n","        df_train = pd.read_csv(csv_path)\n","        if 'id_code' not in df_train.columns:\n","            print(\"ERROR: File CSV kh√¥ng c√≥ c·ªôt 'id_code'\")\n","            return\n","        print(f\"ƒê√£ ƒë·ªçc file CSV: {len(df_train)} m·∫´u\")\n","\n","        # L·∫•y danh s√°ch id_code t·ª´ DataFrame\n","        df_ids = set(df_train['id_code'].astype(str).values)\n","        print(f\"S·ªë id_code trong df_train: {len(df_ids)}\")\n","        print(f\"M·∫´u id_code: {list(df_ids)[:5]}...\")\n","\n","        # L·∫•y danh s√°ch t·ªáp ·∫£nh t·ª´ PROCESSED_FOLDER\n","        if not os.path.exists(processed_folder):\n","            print(f\"ERROR: Th∆∞ m·ª•c PROCESSED_FOLDER kh√¥ng t·ªìn t·∫°i: {processed_folder}\")\n","            return\n","        image_files = [f for f in os.listdir(processed_folder) if f.endswith('.png')]\n","        processed_ids = set(os.path.splitext(f)[0] for f in image_files)\n","        print(f\"S·ªë t·ªáp ·∫£nh trong PROCESSED_FOLDER: {len(processed_ids)}\")\n","        print(f\"M·∫´u processed_ids: {list(processed_ids)[:5]}...\")\n","\n","        # So s√°nh id_code v√† processed_ids\n","        matching_ids = df_ids.intersection(processed_ids)\n","        missing_in_processed = df_ids - processed_ids\n","        missing_in_df = processed_ids - df_ids\n","\n","        # T·∫°o b√°o c√°o\n","        report = {\n","            \"timestamp\": pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n","            \"csv_path\": csv_path,\n","            \"processed_folder\": processed_folder,\n","            \"total_df_ids\": len(df_ids),\n","            \"total_processed_ids\": len(processed_ids),\n","            \"matching_ids_count\": len(matching_ids),\n","            \"matching_ids_sample\": list(matching_ids)[:10],  # L·∫•y m·∫´u 10 ID kh·ªõp\n","            \"missing_in_processed_count\": len(missing_in_processed),\n","            \"missing_in_processed\": list(missing_in_processed),\n","            \"missing_in_df_count\": len(missing_in_df),\n","            \"missing_in_df\": list(missing_in_df)\n","        }\n","\n","        # In k·∫øt qu·∫£\n","        print(f\"S·ªë ID kh·ªõp: {len(matching_ids)}\")\n","        print(f\"S·ªë ID trong df_train nh∆∞ng kh√¥ng c√≥ trong PROCESSED_FOLDER: {len(missing_in_processed)}\")\n","        if missing_in_processed:\n","            print(f\"WARNING: M·∫´u ID thi·∫øu trong PROCESSED_FOLDER: {list(missing_in_processed)[:5]}...\")\n","        print(f\"S·ªë t·ªáp ·∫£nh trong PROCESSED_FOLDER nh∆∞ng kh√¥ng c√≥ trong df_train: {len(missing_in_df)}\")\n","        if missing_in_df:\n","            print(f\"WARNING: M·∫´u ID thi·∫øu trong df_train: {list(missing_in_df)[:5]}...\")\n","\n","        # L∆∞u b√°o c√°o v√†o file JSON\n","        os.makedirs(output_dir, exist_ok=True)\n","        report_path = os.path.join(output_dir, \"id_code_comparison_report.json\")\n","        with open(report_path, 'w', encoding='utf-8') as f:\n","            json.dump(report, f, indent=4)\n","        print(f\"ƒê√£ l∆∞u b√°o c√°o t·∫°i: {report_path}\")\n","\n","    except Exception as e:\n","        print(f\"ERROR: L·ªói trong qu√° tr√¨nh so s√°nh: {str(e)}\")\n","        raise\n","\n","if __name__ == \"__main__\":\n","    # Thay ƒë·ªïi c√°c ƒë∆∞·ªùng d·∫´n n√†y theo c·∫•u h√¨nh c·ªßa b·∫°n\n","    CSV_PATH = \"/content/drive/MyDrive/kaggle_data/aptos2019/train.csv\"\n","    PROCESSED_FOLDER = \"/content/processed_train_images\"\n","    OUTPUT_DIR = \"/content/drive/MyDrive/working\"\n","\n","    print(\"B·∫Øt ƒë·∫ßu so s√°nh id_code v·ªõi PROCESSED_FOLDER...\")\n","    compare_id_code_with_processed_folder(CSV_PATH, PROCESSED_FOLDER, OUTPUT_DIR)\n","    print(\"Ho√†n t·∫•t so s√°nh.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1748707097938,"user":{"displayName":"Resconnect","userId":"00882996269091038835"},"user_tz":-420},"id":"kC9liHzBaRHO","outputId":"e5a56d2d-ddd0-4615-ed6f-fce514400002"},"outputs":[{"name":"stdout","output_type":"stream","text":["Missing images (0): []...\n"]}],"source":["import os\n","\n","PROCESSED_DIR = \"/content/processed_train_images\"\n","sample_ids = df_train_processed['id_code'].values\n","missing_images = []\n","for sample_id in sample_ids:\n","    img_path = os.path.join(PROCESSED_FOLDER, f\"{sample_id}.png\")\n","    if not os.path.exists(img_path):\n","        missing_images.append(sample_id)\n","print(f\"Missing images ({len(missing_images)}): {missing_images[:5]}...\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1748276574990,"user":{"displayName":"Resconnect","userId":"00882996269091038835"},"user_tz":-420},"id":"uZPTMEW3yut3","outputId":"6f0fd3a2-0922-4163-ec8a-fec3d2de2729"},"outputs":[{"name":"stdout","output_type":"stream","text":["N·ªôi dung metadata:\n","{\n","    \"model_name\": \"efficientnetb1\",\n","    \"conv_layer_name\": \"top_conv\",\n","    \"features_2d_path\": \"/content/drive/MyDrive/working/efficientnetb1_features_2d.npy\",\n","    \"features_4d_path\": \"/content/drive/MyDrive/working/efficientnetb1_features_4d.npy\",\n","    \"sample_ids\": [\n","        \"189cbbc9e5e3\",\n","        \"d2901144070c\",\n","        \"b9519abce0c1\",\n","        \"b16dd4483ca5\",\n","        \"82bb8a01935f\",\n","        \"0c55d58bebaf\",\n","        \"80dbeb0fdc75\",\n","        \"2fdfb80ea53c\",\n","        \"239f2c348ea4\",\n","        \"041f09eec1e8\",\n","        \"5712e2aa73a2\",\n","        \"7ccf9d25dc48\",\n","        \"e4d3d437b0a8\",\n","        \"9be71d6d7e59\",\n","        \"9519a590985d\",\n","        \"165cd2070ebd\",\n","        \"437900a99871\",\n","        \"e599151ca14b\",\n","        \"89ed6a0dd53f\",\n","        \"d3e884109b45\",\n","        \"a11bf2edd470\",\n","        \"36a1e3c780a0\",\n","        \"1e8a1fdee5b9\",\n","        \"69df7ade0575\",\n","        \"5efa24b03d5e\",\n","        \"50915e2329a1\",\n","        \"e811f39a1243\",\n","        \"af87d48ffe01\",\n","        \"857230f64a2e\",\n","        \"cd54d022e37d\",\n","        \"780f9daaa24b\",\n","        \"595446774178\",\n","        \"b9b6ee2b9453\",\n","        \"8f10e41a2f02\",\n","        \"1fb455685328\",\n","        \"753b14c27c83\",\n","        \"178412895d5e\",\n","        \"b0c9a492e068\",\n","        \"2bbd1f99ecc3\",\n","        \"b960142a8de7\",\n","        \"de6210f88536\",\n","        \"405b4f78658f\",\n","        \"0790515cf5af\",\n","        \"cd01672507c9\",\n","        \"ae5d31979f19\",\n","        \"43bc7c066dfb\",\n","        \"cae51154e1ce\",\n","        \"a1edf0e66592\",\n","        \"851e40a21f81\",\n","        \"d2fb715b0c41\",\n","        \"6ba5ed791444\",\n","        \"a76b69e443ce\",\n","        \"c334f8688b77\",\n","        \"d6803e467592\",\n","        \"6bf26b777e3a\",\n","        \"4360a112db10\",\n","        \"435d900fa7b2\",\n","        \"d4bc001f7224\",\n","        \"f55e1d2a19e4\",\n","        \"cf8f1bc7a215\",\n","        \"63c0eafd6aa9\",\n","        \"5995321563b7\",\n","        \"03b373718013\",\n","        \"36e4b704b905\",\n","        \"2ef955d6d9ff\",\n","        \"906d02fb822d\",\n","        \"f72ef9ceeaa8\",\n","        \"dee687c6e88a\",\n","        \"10f6ef37fe43\",\n","        \"461fa5292fda\",\n","        \"46923eea9a4e\",\n","        \"a4d41c495666\",\n","        \"369229040a34\",\n","        \"f36cb007a1ef\",\n","        \"a06e5ac695ce\",\n","        \"941d874c8afb\",\n","        \"767d777ee889\",\n","        \"a80dab8eddf4\",\n","        \"35cd9832fc0a\",\n","        \"86baef833ae0\",\n","        \"b9fe7da14a32\",\n","        \"64c6c6ee0d98\",\n","        \"1541226c5d72\",\n","        \"1891698febce\",\n","        \"e42d9a94a66d\",\n","        \"5265dc9acdf8\",\n","        \"807135cbc438\",\n","        \"756b0d6488bb\",\n","        \"4dbce359d0e1\",\n","        \"9a326446c431\",\n","        \"db690e2d02f8\",\n","        \"d3e56584a481\",\n","        \"daff5427c9b2\",\n","        \"8846b09384a4\",\n","        \"857002ed4e49\",\n","        \"b6bfe9db60e5\",\n","        \"57469423a012\",\n","        \"0e75d51152fc\",\n","        \"f9d52509c571\",\n","        \"77e15f213b04\",\n","        \"891329021e12\",\n","        \"2967e578939f\",\n","        \"4da6e2089d57\",\n","        \"4da2961e62fe\",\n","        \"3823acc4e464\",\n","        \"ca0f1a17c8e5\",\n","        \"cbf0394039f8\",\n","        \"248dec89b3a2\",\n","        \"cd941e5bc659\",\n","        \"5b644a403e1f\",\n","        \"ddb222ff7c1d\",\n","        \"be197b663520\",\n","        \"88e5051f65bd\",\n","        \"75238d945315\",\n","        \"79d44db3da2d\",\n","        \"12e6e66c80a7\",\n","        \"086d41d17da8\",\n","        \"1db18bdd43aa\",\n","        \"2a8a9e957a6c\",\n","        \"aa8a1e814811\",\n","        \"3461dc601cc2\",\n","        \"384db24ebbd7\",\n","        \"8bf2d925dc0c\",\n","        \"93d6d20a5ee3\",\n","        \"d25b8a8ad3c4\",\n","        \"b310bd564329\",\n","        \"157d17349cc6\",\n","        \"a2d349f567a6\",\n","        \"0c7e82daf5a0\",\n","        \"ae57c8630249\",\n","        \"1d29cb2f4296\",\n","        \"92d8a7c8e718\",\n","        \"20404ec7b518\",\n","        \"49a4765f8822\",\n","        \"d5ad3362424c\",\n","        \"08c60c647673\",\n","        \"61f403fdb434\",\n","        \"d952dbfb0fe4\",\n","        \"ecb4500285ed\",\n","        \"15f8d769935c\",\n","        \"82e5bc01f8a4\",\n","        \"05339950962e\",\n","        \"a15590a7d774\",\n","        \"3b2b91590590\",\n","        \"d2ffe9287dc7\",\n","        \"260a455692b5\",\n","        \"18cde9649e90\",\n","        \"4c5ab774a381\",\n","        \"830e297965a1\",\n","        \"4b422b48d0d4\",\n","        \"51a1d162e223\",\n","        \"eeaea2c5ff34\",\n","        \"5b32ece9c627\",\n","        \"26453eb7e989\",\n","        \"052d9a3fe55a\",\n","        \"ca891d37a43c\",\n","        \"cfdbaef73a8b\",\n","        \"c1896142a20a\",\n","        \"ffc04fed30e6\",\n","        \"677f087cd697\",\n","        \"cd3fd04d72f5\",\n","        \"ed3ce1674761\",\n","        \"15cc2aef772a\",\n","        \"435414ccccf7\",\n","        \"441848e0f308\",\n","        \"2cacdb0dffae\",\n","        \"5e7630f8438e\",\n","        \"7a06ea127e02\",\n","        \"7a6e384a0846\",\n","        \"df8365d6ac33\",\n","        \"521b5377a727\",\n","        \"7131bf4c9e6f\",\n","        \"08bef347f40d\",\n","        \"a3475dc3ac80\",\n","        \"c21eb81de9fc\",\n","        \"215d2b7c3fde\",\n","        \"66bfec8d6bcd\",\n","        \"2700754f71e9\",\n","        \"00e4ddff966a\",\n","        \"242fc19be06f\",\n","        \"d51b3fe0fa1b\",\n","        \"b4b04d81acbb\",\n","        \"aeab0a63bcaf\",\n","        \"96c699221180\",\n","        \"cc9270f06b65\",\n","        \"61c2fbd16e38\",\n","        \"9ac41b9a809e\",\n","        \"8be6629a6039\",\n","        \"dc6fa1b38b83\",\n","        \"803120c5d287\",\n","        \"62b4be2799ca\",\n","        \"51d780864365\",\n","        \"2a47e5b21791\",\n","        \"9d74428188bb\",\n","        \"8dba09a4e5ed\",\n","        \"32d7d360d891\",\n","        \"4c3c1ed09771\",\n","        \"9785805af1b8\",\n","        \"8c4ceddeb1c6\",\n","        \"aeccef0bdc26\",\n","        \"89d9c071a56f\",\n","        \"7a238a1d3cf3\",\n","        \"3f0d3629d69e\",\n","        \"3796af4d987a\",\n","        \"1ec95179cdfe\",\n","        \"12ab2f6397f0\",\n","        \"e580676516b0\",\n","        \"6d259b5b4c76\",\n","        \"d271d3a2b552\",\n","        \"849a91e9ab28\",\n","        \"283c3aeba594\",\n","        \"25dc1b41ed9c\",\n","        \"6504b703c429\",\n","        \"e5f332efcbc7\",\n","        \"3a1ecf5e2839\",\n","        \"f68690db78d3\",\n","        \"57ce57a8cfb0\",\n","        \"3694e8c8e09a\",\n","        \"8201cab8322d\",\n","        \"d88c4843aec3\",\n","        \"155e2df6bfcf\",\n","        \"a0a0cd8af5a6\",\n","        \"c06024f05a16\",\n","        \"7b29e3783919\",\n","        \"7a77c3eb468c\",\n","        \"3abac0961bfd\",\n","        \"f0a2dc580009\",\n","        \"7269a1d84a57\",\n","        \"bda91b76095b\",\n","        \"6c6505a0c637\",\n","        \"873fe0404d6e\",\n","        \"2221cf5c7935\",\n","        \"f1a761c68559\",\n","        \"1e143fa3de57\",\n","        \"b1197f2cc9b3\",\n","        \"1ca35d483772\",\n","        \"2017cd92c63d\",\n","        \"6f0463c1ff18\",\n","        \"315c1a0d87fd\",\n","        \"0da09e3ce8f1\",\n","        \"35ac70c0d08f\",\n","        \"6a2c3f4ef329\",\n","        \"18b99159a14f\",\n","        \"907aaff827e5\",\n","        \"9a159d4674cd\",\n","        \"207a580de0ea\",\n","        \"26463a5fb949\",\n","        \"789434d095d1\",\n","        \"4c17e85686f0\",\n","        \"190a309f2cc5\",\n","        \"b085caa513a8\",\n","        \"e265c870f9b3\",\n","        \"388279491b5d\",\n","        \"c9e697117f3f\",\n","        \"b66f23ffa730\",\n","        \"576e189d23d4\",\n","        \"3f49f8d100e9\",\n","        \"0097f532ac9f\",\n","        \"4d1cf360b2d7\",\n","        \"e8ddfc9709ce\",\n","        \"e68746d426b2\",\n","        \"1c578b72d7b3\",\n","        \"e019b3e0f33d\",\n","        \"73ba798fee25\",\n","        \"0a85a1e8f9e9\",\n","        \"b0619ca93a5f\",\n","        \"d74ccc796517\",\n","        \"e1900014dabf\",\n","        \"bb9a3d835a94\",\n","        \"4aa07d720638\",\n","        \"9faad91b6578\",\n","        \"58184d6fd087\",\n","        \"76cab26493f1\",\n","        \"16060f05d047\",\n","        \"61bbe8db6f3a\",\n","        \"02cd34a85b24\",\n","        \"9858cc2ae073\",\n","        \"db4ed1e07aa3\",\n","        \"13d411c85ffd\",\n","        \"8d9516ea3587\",\n","        \"3f5b4c2948e8\",\n","        \"76be29bb30b2\",\n","        \"5077cdb88aed\",\n","        \"f61bf44c677c\",\n","        \"f64214bed40e\",\n","        \"23175b7ef453\",\n","        \"e9129ce55fd7\",\n","        \"966c07831334\",\n","        \"094858f005ab\",\n","        \"f5c953bee7cd\",\n","        \"3254e48c8aa0\",\n","        \"c1ebe785503a\",\n","        \"b351ae99413a\",\n","        \"501c319f7a9f\",\n","        \"98e8adcf085c\",\n","        \"c4a8f2fcf6e8\",\n","        \"789c60cba801\",\n","        \"80e6e425f966\",\n","        \"0ef4c61dc056\",\n","        \"f09cfc6a4dbd\",\n","        \"03c85870824c\",\n","        \"d29b37d110f3\",\n","        \"48c72dec46e5\",\n","        \"d81338217fc5\",\n","        \"66cd9c28e636\",\n","        \"3f73c91b7e32\",\n","        \"44e0d56e9d42\",\n","        \"ee6e39319b39\",\n","        \"5b068765e846\",\n","        \"1d37f1c8b6d8\",\n","        \"b87f9c59748b\",\n","        \"4ccfa0b4e96c\",\n","        \"d10d315f123f\",\n","        \"26999ebc21de\",\n","        \"e93394175a19\",\n","        \"f5a8c6426a71\",\n","        \"d968a983d4d2\",\n","        \"73d40ce06a67\",\n","        \"a8c9fcdbc0be\",\n","        \"fd62bd0db4f1\",\n","        \"e540d2e35d15\",\n","        \"aa31bc6b8f4d\",\n","        \"0b8bdec9d869\",\n","        \"848e66b9e199\",\n","        \"47b756014447\",\n","        \"8e3b79e1f1f7\",\n","        \"47d1603a555b\",\n","        \"188a9323be03\",\n","        \"276b14f72328\",\n","        \"27bab1432f61\",\n","        \"3aa2b1ce6700\",\n","        \"fa9f1bc03f21\",\n","        \"63a03880939c\",\n","        \"35df2bc6ae95\",\n","        \"1a19f2ef4472\",\n","        \"dd02d60bef14\",\n","        \"d5c63a8d9e94\",\n","        \"38487e1a5b1f\",\n","        \"be21d8b60e2a\",\n","        \"a688f20f8895\",\n","        \"51cfeccaf40d\",\n","        \"cc12453ea915\",\n","        \"09237bf783a4\",\n","        \"85fce24084da\",\n","        \"ce887b196c23\",\n","        \"d26bb2ed6e71\",\n","        \"e3a7671f787b\",\n","        \"d29096bd94aa\",\n","        \"7b211d8bd249\",\n","        \"6bcce181be65\",\n","        \"6cd606dc52e9\",\n","        \"1c4f3aa4df06\",\n","        \"0cb14014117d\",\n","        \"1f4fb37e0854\",\n","        \"9b57e43b44e7\",\n","        \"ed88faaa325a\",\n","        \"8cb6b5b2f19c\",\n","        \"8bf05909e1e1\",\n","        \"c52bb7343387\",\n","        \"dde43aa22ae6\",\n","        \"360832d84ce0\",\n","        \"d8da9de62743\",\n","        \"48a45619d1a3\",\n","        \"8a3eb86ae4bd\",\n","        \"75ed83dbccce\",\n","        \"80ed04a84a16\",\n","        \"b3a994760537\",\n","        \"0c2e2369dfff\",\n","        \"b99afe7137fb\",\n","        \"80feb1f7ca5e\",\n","        \"c1e6fa1ad314\",\n","        \"d0079cc188e9\",\n","        \"15f440753916\",\n","        \"d473f6fafba0\",\n","        \"034cb07a550f\",\n","        \"e322acd46152\",\n","        \"207dd0487264\",\n","        \"c24bcf7a1bc4\",\n","        \"b13d72ceea26\",\n","        \"5ac7a414560e\",\n","        \"9df31421cdd2\",\n","        \"e4e343eaae2a\",\n","        \"ac5b5dddf91b\",\n","        \"8958a4d17b7e\",\n","        \"735836b1ffa6\",\n","        \"a9e3d186cd1b\",\n","        \"2532613a584a\",\n","        \"cd66754e1b3b\",\n","        \"d26bc6e1230d\",\n","        \"73881f55a3ec\",\n","        \"61da799bf0aa\",\n","        \"af345c68e836\",\n","        \"12025b34deb8\",\n","        \"1844a039b4ea\",\n","        \"5905a9b06a73\",\n","        \"6e092b306fe1\",\n","        \"a06b353e7bed\",\n","        \"da0a83f074f3\",\n","        \"060e00d1e2ab\",\n","        \"8ed586c43023\",\n","        \"5a5d3798c357\",\n","        \"23d7ca170bdb\",\n","        \"3c78bfca247b\",\n","        \"6c30dd481717\",\n","        \"51af8c112682\",\n","        \"e2a47a74e6e1\",\n","        \"afc345cc9145\",\n","        \"7e9081e95bf6\",\n","        \"ec0c9f817b03\",\n","        \"4c60b10a3a6a\",\n","        \"1b862fb6f65d\",\n","        \"423abbaa5fad\",\n","        \"47536db39f00\",\n","        \"2408799a09b2\",\n","        \"573ea80a53be\",\n","        \"9232dc06cfdc\",\n","        \"9f285b3e57ed\",\n","        \"e06cccc08c59\",\n","        \"cf603a9ef2d5\",\n","        \"61e301bd3c25\",\n","        \"e9faf0296643\",\n","        \"6e92b1c5ac8e\",\n","        \"8a81f62320d6\",\n","        \"14515b8f19b6\",\n","        \"a76132e79688\",\n","        \"7877be80901c\",\n","        \"13ab8db8c700\",\n","        \"0a09aa7356c0\",\n","        \"ce6f33a81ad5\",\n","        \"12e3f5f2cb17\",\n","        \"5090917a2676\",\n","        \"21037f5c7790\",\n","        \"d774692d9919\",\n","        \"29f44aea93a4\",\n","        \"8f318a978844\",\n","        \"880edb2cdb69\",\n","        \"9eaf735cf01f\",\n","        \"7e77b61e1639\",\n","        \"7f2cce721e19\",\n","        \"2fdffb6160a6\",\n","        \"709784f7fcc2\",\n","        \"41960d5f58c2\",\n","        \"61ac9b0dc6b9\",\n","        \"d85d052900b4\",\n","        \"3206171db5be\",\n","        \"0ada12c0e78f\",\n","        \"9bd008aab548\",\n","        \"348598d01e18\",\n","        \"6c9c902a97de\",\n","        \"5188a8afa879\",\n","        \"07929d32b5b3\",\n","        \"f18abfa690ab\",\n","        \"2b4c7b5f1f1e\",\n","        \"c96f743915b5\",\n","        \"7d94a000c2d0\",\n","        \"682312e82ee3\",\n","        \"523ff163211b\",\n","        \"e135d7ba9a0e\",\n","        \"1d0b93317aa8\",\n","        \"7aabd768abff\",\n","        \"cc839823755b\",\n","        \"83fda7c0500b\",\n","        \"4036471a1bb7\",\n","        \"1b398c0494d1\",\n","        \"1623e8e3adc4\",\n","        \"821789e9053f\",\n","        \"757e39293591\",\n","        \"89d2a7403a06\",\n","        \"3732de8b416f\",\n","        \"00a8624548a9\",\n","        \"a9a28c37c8c4\",\n","        \"57a710de68a4\",\n","        \"54b322c66d01\",\n","        \"222d0ac042b4\",\n","        \"4b5ffea77373\",\n","        \"bab776139279\",\n","        \"992599744a23\",\n","        \"4fa26d065ad3\",\n","        \"454792eb6e05\",\n","        \"c0a117de7d0a\",\n","        \"fd079d2e93a2\",\n","        \"10f10fd30718\",\n","        \"b191ba0a2b12\",\n","        \"26cd40b57ad1\",\n","        \"ef26625121b3\",\n","        \"4ef7144e24ff\",\n","        \"2b07790a2422\",\n","        \"28503940d10b\",\n","        \"83038ca49b6d\",\n","        \"4246ed634f25\",\n","        \"3601dac9bed7\",\n","        \"27e2be850a99\",\n","        \"82088c6734e6\",\n","        \"1ee1eb7943db\",\n","        \"c67117c6ab3b\",\n","        \"4abca30b676b\",\n","        \"ac1667fac512\",\n","        \"7e9458de5707\",\n","        \"7af4d8704032\",\n","        \"3c1efa38d0da\",\n","        \"4bd5d0b30198\",\n","        \"084c02cf077f\",\n","        \"3b9817a39adf\",\n","        \"a5c9a8c726b2\",\n","        \"ab78a66dee6a\",\n","        \"200d947f75db\",\n","        \"d3de0d313d61\",\n","        \"099021fac3c9\",\n","        \"50d8a8fb7737\",\n","        \"cca626a0e19a\",\n","        \"2bd4d4fbed5c\",\n","        \"4ecd1fdd1435\",\n","        \"97d02a9b94f7\",\n","        \"495255c7492f\",\n","        \"541db13517e2\",\n","        \"b7a1bb106051\",\n","        \"61d9c88a3a4b\",\n","        \"4d7d6928534a\",\n","        \"467b7d9d811c\",\n","        \"4ad8d3ec8789\",\n","        \"a9e984b57556\",\n","        \"46d3316c4857\",\n","        \"16ce555748d8\",\n","        \"d9c9b9786da3\",\n","        \"a0cd7bffdaa0\",\n","        \"7adfb8fc0621\",\n","        \"5b3d41626ec5\",\n","        \"482088e6be44\",\n","        \"cd1c98ec48b1\",\n","        \"b92eacd1392a\",\n","        \"b86fb2d5be1a\",\n","        \"5bf3357a2823\",\n","        \"f5e9a307288c\",\n","        \"6531070bf03c\",\n","        \"2c2aa057afc5\",\n","        \"52ddde91a349\",\n","        \"ec01f0862669\",\n","        \"5e52c9fe676f\",\n","        \"103f97a2ab15\",\n","        \"8b568d47a1fd\",\n","        \"6f923b60934b\",\n","        \"2f143453bb71\",\n","        \"51e656e5a541\",\n","        \"0edadb2aa127\",\n","        \"8273fdb4405e\",\n","        \"7f2123bc89a3\",\n","        \"0af296d2f04a\",\n","        \"45693d027798\",\n","        \"a11c62cb3f86\",\n","        \"10eefba568dd\"\n","    ],\n","    \"timestamp\": \"2025-05-26 15:50:57\"\n","}\n","features_4d_path: /content/drive/MyDrive/working/efficientnetb1_features_4d.npy\n"]}],"source":["import json\n","import os\n","\n","metadata_path = \"/content/drive/MyDrive/working/efficientnetb1_features_metadata.json\"\n","if os.path.exists(metadata_path):\n","    with open(metadata_path, 'r') as f:\n","        metadata = json.load(f)\n","    print(\"N·ªôi dung metadata:\")\n","    print(json.dumps(metadata, indent=4))\n","    print(f\"features_4d_path: {metadata.get('features_4d_path', 'Kh√¥ng t√¨m th·∫•y')}\")\n","else:\n","    print(f\"Metadata kh√¥ng t·ªìn t·∫°i t·∫°i: {metadata_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNChub69POZo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749054793974,"user_tz":-420,"elapsed":132002,"user":{"displayName":"Resconnect","userId":"00882996269091038835"}},"outputId":"42595081-aeb8-4420-dd56-64016d5e85cb"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["ƒê√£ k√≠ch ho·∫°t tƒÉng tr∆∞·ªüng b·ªô nh·ªõ GPU\n","Google Drive ƒë√£ ƒë∆∞·ª£c mount.\n","S·ªë m·∫´u l·ªõp 0: 1534\n","Ph√¢n b·ªë nh√£n ban ƒë·∫ßu: {0: np.int64(0), 1: np.int64(314), 2: np.int64(849), 3: np.int64(164), 4: np.int64(251)}\n","S·ªë m·∫´u m·ª•c ti√™u m·ªói l·ªõp: 1534\n","L·ªõp 1: 314 m·∫´u ban ƒë·∫ßu\n","TƒÉng c∆∞·ªùng 1220 m·∫´u cho l·ªõp 1\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["<ipython-input-8-0b667d742405>:178: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n","  A.GaussNoise(var_limit=(10.0, 20.0), p=0.2),\n"]},{"output_type":"stream","name":"stdout","text":["L·ªõp 2: 849 m·∫´u ban ƒë·∫ßu\n","TƒÉng c∆∞·ªùng 685 m·∫´u cho l·ªõp 2\n","L·ªõp 3: 164 m·∫´u ban ƒë·∫ßu\n","TƒÉng c∆∞·ªùng 1370 m·∫´u cho l·ªõp 3\n","L·ªõp 4: 251 m·∫´u ban ƒë·∫ßu\n","TƒÉng c∆∞·ªùng 1283 m·∫´u cho l·ªõp 4\n","Ph√¢n b·ªë nh√£n sau c√¢n b·∫±ng: {0: np.int64(0), 1: np.int64(1534), 2: np.int64(1534), 3: np.int64(1534), 4: np.int64(1534)}\n","Ph√¢n b·ªë nh√£n sau khi th√™m l·ªõp 0: {0: np.int64(1534), 1: np.int64(1534), 2: np.int64(1534), 3: np.int64(1534), 4: np.int64(1534)}\n","balanced_train_x shape: (7670, 244, 244, 3)\n","balanced_train_y_multi shape: (7670, 5)\n","X·ª≠ l√Ω m√¥ h√¨nh: efficientnetb1\n","Kh·ªüi t·∫°o My_Generator: 7670 m·∫´u, target_size=(244, 244), is_train=True\n","Kh·ªüi t·∫°o My_Generator: 550 m·∫´u, target_size=(244, 244), is_train=False\n","Kh·ªüi t·∫°o My_Generator: 733 m·∫´u, target_size=(244, 244), is_train=False\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D t·∫°i: /content/drive/MyDrive/working/efficientnetb1_features_2d.npy, shape=(7670, 1024)\n","ƒê√£ l∆∞u metadata t·∫°i: /content/drive/MyDrive/working/efficientnetb1_features_metadata.json\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D t·∫°i: /content/drive/MyDrive/working/efficientnetb1_features_2d.npy, shape=(550, 1024)\n","ƒê√£ l∆∞u metadata t·∫°i: /content/drive/MyDrive/working/efficientnetb1_features_metadata.json\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D t·∫°i: /content/drive/MyDrive/working/efficientnetb1_features_2d.npy, shape=(733, 1024)\n","ƒê√£ l∆∞u metadata t·∫°i: /content/drive/MyDrive/working/efficientnetb1_features_metadata.json\n","X·ª≠ l√Ω m√¥ h√¨nh: xception\n","Kh·ªüi t·∫°o My_Generator: 7670 m·∫´u, target_size=(244, 244), is_train=True\n","Kh·ªüi t·∫°o My_Generator: 550 m·∫´u, target_size=(244, 244), is_train=False\n","Kh·ªüi t·∫°o My_Generator: 733 m·∫´u, target_size=(244, 244), is_train=False\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D t·∫°i: /content/drive/MyDrive/working/xception_features_2d.npy, shape=(7670, 1024)\n","ƒê√£ l∆∞u metadata t·∫°i: /content/drive/MyDrive/working/xception_features_metadata.json\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D t·∫°i: /content/drive/MyDrive/working/xception_features_2d.npy, shape=(550, 1024)\n","ƒê√£ l∆∞u metadata t·∫°i: /content/drive/MyDrive/working/xception_features_metadata.json\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D t·∫°i: /content/drive/MyDrive/working/xception_features_2d.npy, shape=(733, 1024)\n","ƒê√£ l∆∞u metadata t·∫°i: /content/drive/MyDrive/working/xception_features_metadata.json\n","X·ª≠ l√Ω m√¥ h√¨nh: inceptionv3\n","Kh·ªüi t·∫°o My_Generator: 7670 m·∫´u, target_size=(299, 299), is_train=True\n","Kh·ªüi t·∫°o My_Generator: 550 m·∫´u, target_size=(299, 299), is_train=False\n","Kh·ªüi t·∫°o My_Generator: 733 m·∫´u, target_size=(299, 299), is_train=False\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D t·∫°i: /content/drive/MyDrive/working/inceptionv3_features_2d.npy, shape=(7670, 1024)\n","ƒê√£ l∆∞u metadata t·∫°i: /content/drive/MyDrive/working/inceptionv3_features_metadata.json\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D t·∫°i: /content/drive/MyDrive/working/inceptionv3_features_2d.npy, shape=(550, 1024)\n","ƒê√£ l∆∞u metadata t·∫°i: /content/drive/MyDrive/working/inceptionv3_features_metadata.json\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D t·∫°i: /content/drive/MyDrive/working/inceptionv3_features_2d.npy, shape=(733, 1024)\n","ƒê√£ l∆∞u metadata t·∫°i: /content/drive/MyDrive/working/inceptionv3_features_metadata.json\n","X·ª≠ l√Ω m√¥ h√¨nh: resnet50\n","Kh·ªüi t·∫°o My_Generator: 7670 m·∫´u, target_size=(244, 244), is_train=True\n","Kh·ªüi t·∫°o My_Generator: 550 m·∫´u, target_size=(244, 244), is_train=False\n","Kh·ªüi t·∫°o My_Generator: 733 m·∫´u, target_size=(244, 244), is_train=False\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D t·∫°i: /content/drive/MyDrive/working/resnet50_features_2d.npy, shape=(7670, 1024)\n","ƒê√£ l∆∞u metadata t·∫°i: /content/drive/MyDrive/working/resnet50_features_metadata.json\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D t·∫°i: /content/drive/MyDrive/working/resnet50_features_2d.npy, shape=(550, 1024)\n","ƒê√£ l∆∞u metadata t·∫°i: /content/drive/MyDrive/working/resnet50_features_metadata.json\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D t·∫°i: /content/drive/MyDrive/working/resnet50_features_2d.npy, shape=(733, 1024)\n","ƒê√£ l∆∞u metadata t·∫°i: /content/drive/MyDrive/working/resnet50_features_metadata.json\n","X·ª≠ l√Ω m√¥ h√¨nh: densenet121\n","Kh·ªüi t·∫°o My_Generator: 7670 m·∫´u, target_size=(244, 244), is_train=True\n","Kh·ªüi t·∫°o My_Generator: 550 m·∫´u, target_size=(244, 244), is_train=False\n","Kh·ªüi t·∫°o My_Generator: 733 m·∫´u, target_size=(244, 244), is_train=False\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D t·∫°i: /content/drive/MyDrive/working/densenet121_features_2d.npy, shape=(7670, 1024)\n","ƒê√£ l∆∞u metadata t·∫°i: /content/drive/MyDrive/working/densenet121_features_metadata.json\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D t·∫°i: /content/drive/MyDrive/working/densenet121_features_2d.npy, shape=(550, 1024)\n","ƒê√£ l∆∞u metadata t·∫°i: /content/drive/MyDrive/working/densenet121_features_metadata.json\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D t·∫°i: /content/drive/MyDrive/working/densenet121_features_2d.npy, shape=(733, 1024)\n","ƒê√£ l∆∞u metadata t·∫°i: /content/drive/MyDrive/working/densenet121_features_metadata.json\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D chu·∫©n h√≥a cho efficientnetb1 t·∫°i: /content/drive/MyDrive/working/efficientnetb1_normalized_features_2d.npy, shape=(3112, 512)\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D chu·∫©n h√≥a cho xception t·∫°i: /content/drive/MyDrive/working/xception_normalized_features_2d.npy, shape=(3112, 512)\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D chu·∫©n h√≥a cho inceptionv3 t·∫°i: /content/drive/MyDrive/working/inceptionv3_normalized_features_2d.npy, shape=(3112, 512)\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D chu·∫©n h√≥a cho resnet50 t·∫°i: /content/drive/MyDrive/working/resnet50_normalized_features_2d.npy, shape=(3112, 512)\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D chu·∫©n h√≥a cho densenet121 t·∫°i: /content/drive/MyDrive/working/densenet121_normalized_features_2d.npy, shape=(3112, 512)\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D k·∫øt h·ª£p (tr∆∞·ªõc PCA) t·∫°i: /content/drive/MyDrive/working/combined_features_2d_before_pca.npy, shape=(3112, 2560)\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D k·∫øt h·ª£p (sau PCA) t·∫°i: /content/drive/MyDrive/working/combined_features_2d_after_pca.npy, shape=(3112, 50)\n","ƒê√£ l∆∞u metadata t·∫°i: /content/drive/MyDrive/working/combined_features_metadata.json\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D chu·∫©n h√≥a cho efficientnetb1 t·∫°i: /content/drive/MyDrive/working/efficientnetb1_normalized_features_2d.npy, shape=(550, 512)\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D chu·∫©n h√≥a cho xception t·∫°i: /content/drive/MyDrive/working/xception_normalized_features_2d.npy, shape=(550, 512)\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D chu·∫©n h√≥a cho inceptionv3 t·∫°i: /content/drive/MyDrive/working/inceptionv3_normalized_features_2d.npy, shape=(550, 512)\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D chu·∫©n h√≥a cho resnet50 t·∫°i: /content/drive/MyDrive/working/resnet50_normalized_features_2d.npy, shape=(550, 512)\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D chu·∫©n h√≥a cho densenet121 t·∫°i: /content/drive/MyDrive/working/densenet121_normalized_features_2d.npy, shape=(550, 512)\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D k·∫øt h·ª£p (tr∆∞·ªõc PCA) t·∫°i: /content/drive/MyDrive/working/combined_features_2d_before_pca.npy, shape=(550, 2560)\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D k·∫øt h·ª£p (sau PCA) t·∫°i: /content/drive/MyDrive/working/combined_features_2d_after_pca.npy, shape=(550, 50)\n","ƒê√£ l∆∞u metadata t·∫°i: /content/drive/MyDrive/working/combined_features_metadata.json\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D chu·∫©n h√≥a cho efficientnetb1 t·∫°i: /content/drive/MyDrive/working/efficientnetb1_normalized_features_2d.npy, shape=(733, 512)\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D chu·∫©n h√≥a cho xception t·∫°i: /content/drive/MyDrive/working/xception_normalized_features_2d.npy, shape=(733, 512)\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D chu·∫©n h√≥a cho inceptionv3 t·∫°i: /content/drive/MyDrive/working/inceptionv3_normalized_features_2d.npy, shape=(733, 512)\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D chu·∫©n h√≥a cho resnet50 t·∫°i: /content/drive/MyDrive/working/resnet50_normalized_features_2d.npy, shape=(733, 512)\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D chu·∫©n h√≥a cho densenet121 t·∫°i: /content/drive/MyDrive/working/densenet121_normalized_features_2d.npy, shape=(733, 512)\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D k·∫øt h·ª£p (tr∆∞·ªõc PCA) t·∫°i: /content/drive/MyDrive/working/combined_features_2d_before_pca.npy, shape=(733, 2560)\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D k·∫øt h·ª£p (sau PCA) t·∫°i: /content/drive/MyDrive/working/combined_features_2d_after_pca.npy, shape=(733, 50)\n","ƒê√£ l∆∞u metadata t·∫°i: /content/drive/MyDrive/working/combined_features_metadata.json\n","Hu·∫•n luy·ªán meta-model v·ªõi MAML/FO-MAML...\n","L∆∞u ƒë·∫∑c tr∆∞ng 2D c·ªßa meta-learner...\n","ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D c·ªßa meta-learner t·∫°i: /content/drive/MyDrive/working/meta_learner_features_2d.npy, shape=(550, 128)\n","ƒê√£ l∆∞u metadata meta-learner t·∫°i: /content/drive/MyDrive/working/meta_learner_features_metadata.json\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 1 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_1.png\n","Episode 1/20:\n","  Support Loss: 1.1591, Accuracy: 0.5167\n","  Support QWK: 0.3498, F1: 0.5045, Precision: 0.5259, Recall: 0.5167\n","  Support Confusion Matrix:\n","[[5 0 1 3 1]\n"," [0 8 1 1 0]\n"," [0 1 7 2 0]\n"," [2 5 3 8 2]\n"," [0 2 2 3 3]]\n","  Query Loss: 1.5671, Accuracy: 0.2500\n","  Valid QWK (Ensemble): -0.0308, Precision: 0.3294, Recall: 0.1745\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","ƒê√£ l∆∞u tr·ªçng s·ªë t·ªët nh·∫•t t·∫°i episode 1 v·ªõi QWK: -0.0308\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n best_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_best_episode_1.png\n","Ma tr·∫≠n nh·∫ßm l·∫´n cho QWK t·ªët nh·∫•t t·∫°i Episode 1:\n","[[ 74   1  31  69  96]\n"," [  3   0   1  46   6]\n"," [ 21   5   2 111  11]\n"," [  3   3   0  19   4]\n"," [ 17   1   0  25   1]]\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 2 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_2.png\n","Episode 2/20:\n","  Support Loss: 1.1488, Accuracy: 0.5500\n","  Support QWK: 0.4364, F1: 0.5483, Precision: 0.5702, Recall: 0.5500\n","  Support Confusion Matrix:\n","[[6 0 3 1 0]\n"," [0 6 1 2 1]\n"," [2 0 7 1 0]\n"," [5 3 3 8 1]\n"," [0 1 1 2 6]]\n","  Query Loss: 1.4751, Accuracy: 0.2667\n","  Valid QWK (Ensemble): 0.1283, Precision: 0.2232, Recall: 0.0927\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","ƒê√£ l∆∞u tr·ªçng s·ªë t·ªët nh·∫•t t·∫°i episode 2 v·ªõi QWK: 0.1283\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n best_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_best_episode_2.png\n","Ma tr·∫≠n nh·∫ßm l·∫´n cho QWK t·ªët nh·∫•t t·∫°i Episode 2:\n","[[ 29  10 189  19  24]\n"," [  3   0   4  36  13]\n"," [ 10   0   3  55  82]\n"," [  5   1   0  11  12]\n"," [ 21   0   1  14   8]]\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 3 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_3.png\n","Episode 3/20:\n","  Support Loss: 1.2000, Accuracy: 0.6000\n","  Support QWK: 0.4819, F1: 0.5885, Precision: 0.6707, Recall: 0.6000\n","  Support Confusion Matrix:\n","[[ 4  0  4  0  2]\n"," [ 1  8  0  1  0]\n"," [ 3  0  7  0  0]\n"," [ 5  2  3  7  3]\n"," [ 0  0  0  0 10]]\n","  Query Loss: 1.4192, Accuracy: 0.2167\n","  Valid QWK (Ensemble): 0.2487, Precision: 0.4647, Recall: 0.2945\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","ƒê√£ l∆∞u tr·ªçng s·ªë t·ªët nh·∫•t t·∫°i episode 3 v·ªõi QWK: 0.2487\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n best_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_best_episode_3.png\n","Ma tr·∫≠n nh·∫ßm l·∫´n cho QWK t·ªët nh·∫•t t·∫°i Episode 3:\n","[[113   1 133  11  13]\n"," [  1   1   3  49   2]\n"," [ 10   7  30  82  21]\n"," [  3   7   2  16   1]\n"," [ 11  12   0  19   2]]\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 4 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_4.png\n","Episode 4/20:\n","  Support Loss: 1.1440, Accuracy: 0.7833\n","  Support QWK: 0.5556, F1: 0.7758, Precision: 0.7928, Recall: 0.7833\n","  Support Confusion Matrix:\n","[[ 5  0  2  2  1]\n"," [ 0  9  0  1  0]\n"," [ 0  0 10  0  0]\n"," [ 1  1  1 16  1]\n"," [ 0  3  0  0  7]]\n","  Query Loss: 1.4091, Accuracy: 0.2500\n","  Valid QWK (Ensemble): 0.0494, Precision: 0.2211, Recall: 0.1400\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 5 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_5.png\n","Episode 5/20:\n","  Support Loss: 1.1260, Accuracy: 0.6500\n","  Support QWK: 0.4088, F1: 0.6158, Precision: 0.7179, Recall: 0.6500\n","  Support Confusion Matrix:\n","[[ 7  0  2  1  0]\n"," [ 1  8  0  1  0]\n"," [ 2  0  7  1  0]\n"," [ 1  2  1 16  0]\n"," [ 1  6  0  2  1]]\n","  Query Loss: 1.3782, Accuracy: 0.2333\n","  Valid QWK (Ensemble): 0.1297, Precision: 0.3925, Recall: 0.2364\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 6 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_6.png\n","Episode 6/20:\n","  Support Loss: 1.0181, Accuracy: 0.7000\n","  Support QWK: 0.6741, F1: 0.7077, Precision: 0.7870, Recall: 0.7000\n","  Support Confusion Matrix:\n","[[ 8  0  2  0  0]\n"," [ 1  7  2  0  0]\n"," [ 2  0  8  0  0]\n"," [ 6  0  3 10  1]\n"," [ 0  0  0  1  9]]\n","  Query Loss: 1.3696, Accuracy: 0.2167\n","  Valid QWK (Ensemble): 0.2832, Precision: 0.4893, Recall: 0.3491\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","ƒê√£ l∆∞u tr·ªçng s·ªë t·ªët nh·∫•t t·∫°i episode 6 v·ªõi QWK: 0.2832\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n best_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_best_episode_6.png\n","Ma tr·∫≠n nh·∫ßm l·∫´n cho QWK t·ªët nh·∫•t t·∫°i Episode 6:\n","[[130   6 130   2   3]\n"," [  2   1  11  42   0]\n"," [ 10   6  50  63  21]\n"," [  4   6   9   8   2]\n"," [ 11  18   4   8   3]]\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 7 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_7.png\n","Episode 7/20:\n","  Support Loss: 1.0708, Accuracy: 0.5500\n","  Support QWK: 0.2410, F1: 0.5012, Precision: 0.5165, Recall: 0.5500\n","  Support Confusion Matrix:\n","[[ 8  0  1  1  0]\n"," [ 0  9  1  0  0]\n"," [ 3  0  6  1  0]\n"," [ 5  3  2 10  0]\n"," [ 1  6  3  0  0]]\n","  Query Loss: 1.4058, Accuracy: 0.2333\n","  Valid QWK (Ensemble): 0.2726, Precision: 0.4782, Recall: 0.2491\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 8 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_8.png\n","Episode 8/20:\n","  Support Loss: 1.0854, Accuracy: 0.5667\n","  Support QWK: 0.2840, F1: 0.5765, Precision: 0.7232, Recall: 0.5667\n","  Support Confusion Matrix:\n","[[10  0  0  0  0]\n"," [ 1  6  0  2  1]\n"," [ 5  0  5  0  0]\n"," [10  1  0  9  0]\n"," [ 2  4  0  0  4]]\n","  Query Loss: 1.3491, Accuracy: 0.3167\n","  Valid QWK (Ensemble): 0.1314, Precision: 0.3142, Recall: 0.1909\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 9 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_9.png\n","Episode 9/20:\n","  Support Loss: 1.2382, Accuracy: 0.4833\n","  Support QWK: 0.1581, F1: 0.4792, Precision: 0.6909, Recall: 0.4833\n","  Support Confusion Matrix:\n","[[ 8  1  0  0  1]\n"," [ 1  8  0  1  0]\n"," [ 5  1  4  0  0]\n"," [11  2  0  7  0]\n"," [ 1  7  0  0  2]]\n","  Query Loss: 1.4969, Accuracy: 0.2000\n","  Valid QWK (Ensemble): 0.0892, Precision: 0.4664, Recall: 0.3382\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 10 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_10.png\n","Episode 10/20:\n","  Support Loss: 1.1493, Accuracy: 0.5500\n","  Support QWK: 0.4052, F1: 0.5461, Precision: 0.5996, Recall: 0.5500\n","  Support Confusion Matrix:\n","[[7 1 1 1 0]\n"," [2 4 1 3 0]\n"," [1 0 9 0 0]\n"," [6 4 2 8 0]\n"," [1 1 1 2 5]]\n","  Query Loss: 1.5172, Accuracy: 0.1667\n","  Valid QWK (Ensemble): 0.3156, Precision: 0.5271, Recall: 0.4164\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","ƒê√£ l∆∞u tr·ªçng s·ªë t·ªët nh·∫•t t·∫°i episode 10 v·ªõi QWK: 0.3156\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n best_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_best_episode_10.png\n","Ma tr·∫≠n nh·∫ßm l·∫´n cho QWK t·ªët nh·∫•t t·∫°i Episode 10:\n","[[140  30  74  14  13]\n"," [  2   0  32  20   2]\n"," [ 10   7  81  10  42]\n"," [  3   7  11   3   5]\n"," [ 12   7  12   8   5]]\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 11 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_11.png\n","Episode 11/20:\n","  Support Loss: 1.0822, Accuracy: 0.5167\n","  Support QWK: 0.2519, F1: 0.4761, Precision: 0.6749, Recall: 0.5167\n","  Support Confusion Matrix:\n","[[9 0 0 0 1]\n"," [1 9 0 0 0]\n"," [3 0 6 0 1]\n"," [8 4 3 4 1]\n"," [1 6 0 0 3]]\n","  Query Loss: 1.4840, Accuracy: 0.2167\n","  Valid QWK (Ensemble): 0.1758, Precision: 0.4997, Recall: 0.2436\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 12 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_12.png\n","Episode 12/20:\n","  Support Loss: 1.1152, Accuracy: 0.6000\n","  Support QWK: 0.4156, F1: 0.5860, Precision: 0.7767, Recall: 0.6000\n","  Support Confusion Matrix:\n","[[10  0  0  0  0]\n"," [ 0  9  1  0  0]\n"," [ 5  0  5  0  0]\n"," [ 8  5  1  6  0]\n"," [ 1  3  0  0  6]]\n","  Query Loss: 1.3773, Accuracy: 0.2667\n","  Valid QWK (Ensemble): 0.2886, Precision: 0.4738, Recall: 0.2927\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 13 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_13.png\n","Episode 13/20:\n","  Support Loss: 1.1347, Accuracy: 0.5833\n","  Support QWK: 0.4480, F1: 0.5550, Precision: 0.6640, Recall: 0.5833\n","  Support Confusion Matrix:\n","[[10  0  0  0  0]\n"," [ 0  9  0  0  1]\n"," [ 3  0  6  1  0]\n"," [ 5  8  1  5  1]\n"," [ 0  4  0  1  5]]\n","  Query Loss: 1.3359, Accuracy: 0.3667\n","  Valid QWK (Ensemble): 0.3764, Precision: 0.5526, Recall: 0.3618\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","ƒê√£ l∆∞u tr·ªçng s·ªë t·ªët nh·∫•t t·∫°i episode 13 v·ªõi QWK: 0.3764\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n best_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_best_episode_13.png\n","Ma tr·∫≠n nh·∫ßm l·∫´n cho QWK t·ªët nh·∫•t t·∫°i Episode 13:\n","[[135  38  82   2  14]\n"," [  1  10  21  18   6]\n"," [  3  15  45   9  78]\n"," [  0   9   4   1  15]\n"," [  7  20   3   6   8]]\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 14 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_14.png\n","Episode 14/20:\n","  Support Loss: 1.0907, Accuracy: 0.7833\n","  Support QWK: 0.5721, F1: 0.7730, Precision: 0.8329, Recall: 0.7833\n","  Support Confusion Matrix:\n","[[ 9  0  0  1  0]\n"," [ 0 10  0  0  0]\n"," [ 0  0 10  0  0]\n"," [ 3  2  1 14  0]\n"," [ 0  5  0  1  4]]\n","  Query Loss: 1.5028, Accuracy: 0.1667\n","  Valid QWK (Ensemble): 0.2097, Precision: 0.4771, Recall: 0.2473\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 15 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_15.png\n","Episode 15/20:\n","  Support Loss: 1.1721, Accuracy: 0.6000\n","  Support QWK: 0.2622, F1: 0.5973, Precision: 0.7216, Recall: 0.6000\n","  Support Confusion Matrix:\n","[[ 6  2  1  0  1]\n"," [ 0  9  0  0  1]\n"," [ 0  0 10  0  0]\n"," [ 2  7  1  9  1]\n"," [ 0  8  0  0  2]]\n","  Query Loss: 1.4403, Accuracy: 0.2000\n","  Valid QWK (Ensemble): 0.2821, Precision: 0.4600, Recall: 0.2236\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 16 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_16.png\n","Episode 16/20:\n","  Support Loss: 1.1821, Accuracy: 0.6667\n","  Support QWK: 0.4072, F1: 0.6700, Precision: 0.7778, Recall: 0.6667\n","  Support Confusion Matrix:\n","[[9 0 0 0 1]\n"," [1 8 0 0 1]\n"," [1 0 8 0 1]\n"," [7 4 0 9 0]\n"," [0 4 0 0 6]]\n","  Query Loss: 1.4174, Accuracy: 0.2000\n","  Valid QWK (Ensemble): 0.4237, Precision: 0.5729, Recall: 0.3000\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","ƒê√£ l∆∞u tr·ªçng s·ªë t·ªët nh·∫•t t·∫°i episode 16 v·ªõi QWK: 0.4237\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n best_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_best_episode_16.png\n","Ma tr·∫≠n nh·∫ßm l·∫´n cho QWK t·ªët nh·∫•t t·∫°i Episode 16:\n","[[103 105  28   3  32]\n"," [  1   9  20  19   7]\n"," [  5  15  34   6  90]\n"," [  1   3   2   6  17]\n"," [  3   8   5  15  13]]\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 17 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_17.png\n","Episode 17/20:\n","  Support Loss: 1.1603, Accuracy: 0.6500\n","  Support QWK: 0.4273, F1: 0.6564, Precision: 0.7933, Recall: 0.6500\n","  Support Confusion Matrix:\n","[[ 8  0  1  1  0]\n"," [ 0 10  0  0  0]\n"," [ 2  1  7  0  0]\n"," [ 2  8  0 10  0]\n"," [ 0  6  0  0  4]]\n","  Query Loss: 1.4453, Accuracy: 0.2000\n","  Valid QWK (Ensemble): 0.3002, Precision: 0.3685, Recall: 0.1545\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 18 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_18.png\n","Episode 18/20:\n","  Support Loss: 1.1229, Accuracy: 0.5167\n","  Support QWK: 0.3727, F1: 0.4913, Precision: 0.7130, Recall: 0.5167\n","  Support Confusion Matrix:\n","[[8 2 0 0 0]\n"," [0 9 0 0 1]\n"," [5 0 5 0 0]\n"," [5 8 3 4 0]\n"," [0 5 0 0 5]]\n","  Query Loss: 1.4878, Accuracy: 0.1833\n","  Valid QWK (Ensemble): 0.3183, Precision: 0.4531, Recall: 0.1891\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 19 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_19.png\n","Episode 19/20:\n","  Support Loss: 1.0847, Accuracy: 0.6500\n","  Support QWK: 0.4248, F1: 0.6349, Precision: 0.7849, Recall: 0.6500\n","  Support Confusion Matrix:\n","[[ 9  1  0  0  0]\n"," [ 0 10  0  0  0]\n"," [ 1  0  9  0  0]\n"," [ 4  6  1  8  1]\n"," [ 0  7  0  0  3]]\n","  Query Loss: 1.3603, Accuracy: 0.2500\n","  Valid QWK (Ensemble): 0.2655, Precision: 0.5066, Recall: 0.2182\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","Episode distribution - Support: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode distribution - Query: {0: np.int64(10), 1: np.int64(10), 2: np.int64(10), 3: np.int64(20), 4: np.int64(10)}\n","Episode 20 - Tr·ªçng s·ªë l·ªõp: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n support_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_support_episode_20.png\n","Episode 20/20:\n","  Support Loss: 1.1570, Accuracy: 0.5500\n","  Support QWK: 0.4266, F1: 0.5281, Precision: 0.6682, Recall: 0.5500\n","  Support Confusion Matrix:\n","[[8 2 0 0 0]\n"," [0 8 1 1 0]\n"," [2 0 7 0 1]\n"," [5 9 1 4 1]\n"," [0 4 0 0 6]]\n","  Query Loss: 1.4693, Accuracy: 0.2000\n","  Valid QWK (Ensemble): 0.2651, Precision: 0.4061, Recall: 0.1927\n","  Inner LR: 0.001000, Outer LR: 0.001000, Fine-tune LR: 0.000100\n","Fine-tuning tr√™n t·∫≠p valid...\n","Epoch 1/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 261ms/step - accuracy: 0.2059 - loss: 61.6283 - precision: 0.1651 - recall: 0.0148 - val_accuracy: 0.2309 - val_loss: 59.5195 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-04\n","Epoch 2/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3222 - loss: 58.9407 - precision: 0.5042 - recall: 0.0497 - val_accuracy: 0.3855 - val_loss: 57.0166 - val_precision: 1.0000 - val_recall: 0.0018 - learning_rate: 1.0000e-04\n","Epoch 3/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4058 - loss: 56.4690 - precision: 0.5170 - recall: 0.0699 - val_accuracy: 0.6473 - val_loss: 54.6249 - val_precision: 1.0000 - val_recall: 0.0036 - learning_rate: 1.0000e-04\n","Epoch 4/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4650 - loss: 54.2175 - precision: 0.6384 - recall: 0.0970 - val_accuracy: 0.7564 - val_loss: 52.3275 - val_precision: 1.0000 - val_recall: 0.0491 - learning_rate: 1.0000e-04\n","Epoch 5/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5936 - loss: 51.9122 - precision: 0.7379 - recall: 0.1639 - val_accuracy: 0.7800 - val_loss: 50.1516 - val_precision: 0.9903 - val_recall: 0.1855 - learning_rate: 1.0000e-04\n","Epoch 6/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6216 - loss: 49.7980 - precision: 0.8782 - recall: 0.2412 - val_accuracy: 0.8000 - val_loss: 48.0833 - val_precision: 0.9842 - val_recall: 0.3400 - learning_rate: 1.0000e-04\n","Epoch 7/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6455 - loss: 47.7337 - precision: 0.8997 - recall: 0.3115 - val_accuracy: 0.8091 - val_loss: 46.0973 - val_precision: 0.9670 - val_recall: 0.4800 - learning_rate: 1.0000e-04\n","Epoch 8/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6820 - loss: 45.7820 - precision: 0.8917 - recall: 0.3620 - val_accuracy: 0.8127 - val_loss: 44.1917 - val_precision: 0.9605 - val_recall: 0.5745 - learning_rate: 1.0000e-04\n","Epoch 9/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6746 - loss: 43.9207 - precision: 0.8828 - recall: 0.3985 - val_accuracy: 0.8145 - val_loss: 42.3576 - val_precision: 0.9423 - val_recall: 0.6236 - learning_rate: 1.0000e-04\n","Epoch 10/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7217 - loss: 42.0953 - precision: 0.8917 - recall: 0.4385 - val_accuracy: 0.8236 - val_loss: 40.6064 - val_precision: 0.9278 - val_recall: 0.6545 - learning_rate: 1.0000e-04\n","Epoch 11/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7476 - loss: 40.3737 - precision: 0.8772 - recall: 0.5117 - val_accuracy: 0.8273 - val_loss: 38.9188 - val_precision: 0.9225 - val_recall: 0.6709 - learning_rate: 1.0000e-04\n","Epoch 12/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7605 - loss: 38.6506 - precision: 0.8932 - recall: 0.5685 - val_accuracy: 0.8327 - val_loss: 37.3012 - val_precision: 0.9207 - val_recall: 0.6964 - learning_rate: 1.0000e-04\n","Epoch 13/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7731 - loss: 37.0557 - precision: 0.8949 - recall: 0.5524 - val_accuracy: 0.8345 - val_loss: 35.7531 - val_precision: 0.9204 - val_recall: 0.7145 - learning_rate: 1.0000e-04\n","Epoch 14/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8077 - loss: 35.4904 - precision: 0.9014 - recall: 0.6056 - val_accuracy: 0.8364 - val_loss: 34.2729 - val_precision: 0.9171 - val_recall: 0.7236 - learning_rate: 1.0000e-04\n","Epoch 15/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7813 - loss: 34.0534 - precision: 0.9007 - recall: 0.6196 - val_accuracy: 0.8418 - val_loss: 32.8533 - val_precision: 0.9159 - val_recall: 0.7327 - learning_rate: 1.0000e-04\n","Epoch 16/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7990 - loss: 32.6147 - precision: 0.8886 - recall: 0.6544 - val_accuracy: 0.8455 - val_loss: 31.4852 - val_precision: 0.9148 - val_recall: 0.7418 - learning_rate: 1.0000e-04\n","Epoch 17/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8066 - loss: 31.2332 - precision: 0.9211 - recall: 0.6982 - val_accuracy: 0.8455 - val_loss: 30.1703 - val_precision: 0.9081 - val_recall: 0.7545 - learning_rate: 1.0000e-04\n","Epoch 18/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7958 - loss: 29.9735 - precision: 0.9222 - recall: 0.6803 - val_accuracy: 0.8455 - val_loss: 28.9048 - val_precision: 0.9048 - val_recall: 0.7600 - learning_rate: 1.0000e-04\n","Epoch 19/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8129 - loss: 28.7047 - precision: 0.8954 - recall: 0.7032 - val_accuracy: 0.8455 - val_loss: 27.6850 - val_precision: 0.9017 - val_recall: 0.7673 - learning_rate: 1.0000e-04\n","Epoch 20/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8182 - loss: 27.5072 - precision: 0.9011 - recall: 0.7162 - val_accuracy: 0.8509 - val_loss: 26.5212 - val_precision: 0.9002 - val_recall: 0.7709 - learning_rate: 1.0000e-04\n","Epoch 21/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8393 - loss: 26.3208 - precision: 0.8951 - recall: 0.7213 - val_accuracy: 0.8491 - val_loss: 25.4061 - val_precision: 0.8987 - val_recall: 0.7745 - learning_rate: 1.0000e-04\n","Epoch 22/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8334 - loss: 25.2087 - precision: 0.8965 - recall: 0.7343 - val_accuracy: 0.8491 - val_loss: 24.3345 - val_precision: 0.8971 - val_recall: 0.7764 - learning_rate: 1.0000e-04\n","Epoch 23/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8068 - loss: 24.1696 - precision: 0.8793 - recall: 0.7003 - val_accuracy: 0.8545 - val_loss: 23.3073 - val_precision: 0.8975 - val_recall: 0.7800 - learning_rate: 1.0000e-04\n","Epoch 24/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8048 - loss: 23.1908 - precision: 0.8892 - recall: 0.7275 - val_accuracy: 0.8545 - val_loss: 22.3193 - val_precision: 0.8926 - val_recall: 0.7855 - learning_rate: 1.0000e-04\n","Epoch 25/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8517 - loss: 22.1286 - precision: 0.9316 - recall: 0.7669 - val_accuracy: 0.8564 - val_loss: 21.3726 - val_precision: 0.8914 - val_recall: 0.7909 - learning_rate: 1.0000e-04\n","Epoch 26/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8485 - loss: 21.1795 - precision: 0.9241 - recall: 0.7807 - val_accuracy: 0.8564 - val_loss: 20.4616 - val_precision: 0.8921 - val_recall: 0.7964 - learning_rate: 1.0000e-04\n","Epoch 27/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8553 - loss: 20.2961 - precision: 0.8961 - recall: 0.7646 - val_accuracy: 0.8564 - val_loss: 19.5872 - val_precision: 0.8855 - val_recall: 0.8018 - learning_rate: 1.0000e-04\n","Epoch 28/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8280 - loss: 19.4756 - precision: 0.8815 - recall: 0.7450 - val_accuracy: 0.8582 - val_loss: 18.7498 - val_precision: 0.8855 - val_recall: 0.8018 - learning_rate: 1.0000e-04\n","Epoch 29/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8251 - loss: 18.6379 - precision: 0.8770 - recall: 0.7387 - val_accuracy: 0.8564 - val_loss: 17.9467 - val_precision: 0.8865 - val_recall: 0.8091 - learning_rate: 1.0000e-04\n","Epoch 30/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8344 - loss: 17.8205 - precision: 0.8848 - recall: 0.7795 - val_accuracy: 0.8545 - val_loss: 17.1800 - val_precision: 0.8891 - val_recall: 0.8164 - learning_rate: 1.0000e-04\n","Epoch 31/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8401 - loss: 17.0727 - precision: 0.8782 - recall: 0.7703 - val_accuracy: 0.8564 - val_loss: 16.4490 - val_precision: 0.8856 - val_recall: 0.8164 - learning_rate: 1.0000e-04\n","Epoch 32/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8179 - loss: 16.3615 - precision: 0.8710 - recall: 0.7717 - val_accuracy: 0.8545 - val_loss: 15.7471 - val_precision: 0.8909 - val_recall: 0.8164 - learning_rate: 1.0000e-04\n","Epoch 33/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8353 - loss: 15.6174 - precision: 0.8716 - recall: 0.7872 - val_accuracy: 0.8545 - val_loss: 15.0791 - val_precision: 0.8926 - val_recall: 0.8164 - learning_rate: 1.0000e-04\n","Epoch 34/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8325 - loss: 14.9888 - precision: 0.8958 - recall: 0.7815 - val_accuracy: 0.8564 - val_loss: 14.4373 - val_precision: 0.8926 - val_recall: 0.8164 - learning_rate: 1.0000e-04\n","Epoch 35/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8627 - loss: 14.3082 - precision: 0.8946 - recall: 0.8101 - val_accuracy: 0.8600 - val_loss: 13.8242 - val_precision: 0.8880 - val_recall: 0.8218 - learning_rate: 1.0000e-04\n","Epoch 36/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8449 - loss: 13.7590 - precision: 0.8873 - recall: 0.7980 - val_accuracy: 0.8582 - val_loss: 13.2421 - val_precision: 0.8880 - val_recall: 0.8218 - learning_rate: 1.0000e-04\n","Epoch 37/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8566 - loss: 13.0980 - precision: 0.9089 - recall: 0.8160 - val_accuracy: 0.8582 - val_loss: 12.6818 - val_precision: 0.8915 - val_recall: 0.8218 - learning_rate: 1.0000e-04\n","Epoch 38/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8343 - loss: 12.6087 - precision: 0.8795 - recall: 0.7872 - val_accuracy: 0.8600 - val_loss: 12.1486 - val_precision: 0.8933 - val_recall: 0.8218 - learning_rate: 1.0000e-04\n","Epoch 39/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8421 - loss: 12.0534 - precision: 0.8816 - recall: 0.7798 - val_accuracy: 0.8582 - val_loss: 11.6361 - val_precision: 0.8935 - val_recall: 0.8236 - learning_rate: 1.0000e-04\n","Epoch 40/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8608 - loss: 11.5286 - precision: 0.8939 - recall: 0.8281 - val_accuracy: 0.8600 - val_loss: 11.1538 - val_precision: 0.8939 - val_recall: 0.8273 - learning_rate: 1.0000e-04\n","Epoch 41/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8560 - loss: 11.0729 - precision: 0.8887 - recall: 0.8078 - val_accuracy: 0.8600 - val_loss: 10.6905 - val_precision: 0.8945 - val_recall: 0.8327 - learning_rate: 1.0000e-04\n","Epoch 42/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8609 - loss: 10.6300 - precision: 0.8940 - recall: 0.8150 - val_accuracy: 0.8600 - val_loss: 10.2476 - val_precision: 0.8949 - val_recall: 0.8364 - learning_rate: 1.0000e-04\n","Epoch 43/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8398 - loss: 10.1999 - precision: 0.8748 - recall: 0.8014 - val_accuracy: 0.8600 - val_loss: 9.8236 - val_precision: 0.8930 - val_recall: 0.8345 - learning_rate: 1.0000e-04\n","Epoch 44/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8555 - loss: 9.7739 - precision: 0.8931 - recall: 0.8154 - val_accuracy: 0.8655 - val_loss: 9.4218 - val_precision: 0.8969 - val_recall: 0.8382 - learning_rate: 1.0000e-04\n","Epoch 45/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8411 - loss: 9.3955 - precision: 0.8941 - recall: 0.7993 - val_accuracy: 0.8655 - val_loss: 9.0385 - val_precision: 0.9018 - val_recall: 0.8345 - learning_rate: 1.0000e-04\n","Epoch 46/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8295 - loss: 9.0086 - precision: 0.8842 - recall: 0.7947 - val_accuracy: 0.8655 - val_loss: 8.6704 - val_precision: 0.9018 - val_recall: 0.8345 - learning_rate: 1.0000e-04\n","Epoch 47/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8347 - loss: 8.6685 - precision: 0.8669 - recall: 0.8007 - val_accuracy: 0.8673 - val_loss: 8.3220 - val_precision: 0.9002 - val_recall: 0.8364 - learning_rate: 1.0000e-04\n","Epoch 48/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8761 - loss: 8.2672 - precision: 0.8882 - recall: 0.8260 - val_accuracy: 0.8691 - val_loss: 7.9879 - val_precision: 0.8967 - val_recall: 0.8364 - learning_rate: 1.0000e-04\n","Epoch 49/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8391 - loss: 7.9640 - precision: 0.8796 - recall: 0.8109 - val_accuracy: 0.8673 - val_loss: 7.6679 - val_precision: 0.8919 - val_recall: 0.8400 - learning_rate: 1.0000e-04\n","Epoch 50/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8586 - loss: 7.6160 - precision: 0.8949 - recall: 0.8180 - val_accuracy: 0.8636 - val_loss: 7.3593 - val_precision: 0.8934 - val_recall: 0.8382 - learning_rate: 1.0000e-04\n","Epoch 51/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8614 - loss: 7.3048 - precision: 0.9008 - recall: 0.8353 - val_accuracy: 0.8691 - val_loss: 7.0626 - val_precision: 0.9006 - val_recall: 0.8400 - learning_rate: 1.0000e-04\n","Epoch 52/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8352 - loss: 7.0471 - precision: 0.8814 - recall: 0.8091 - val_accuracy: 0.8709 - val_loss: 6.7793 - val_precision: 0.9023 - val_recall: 0.8400 - learning_rate: 1.0000e-04\n","Epoch 53/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8647 - loss: 6.7427 - precision: 0.9058 - recall: 0.8384 - val_accuracy: 0.8727 - val_loss: 6.5117 - val_precision: 0.9077 - val_recall: 0.8400 - learning_rate: 1.0000e-04\n","Epoch 54/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8606 - loss: 6.4981 - precision: 0.8936 - recall: 0.8156 - val_accuracy: 0.8709 - val_loss: 6.2592 - val_precision: 0.9043 - val_recall: 0.8418 - learning_rate: 1.0000e-04\n","Epoch 55/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8766 - loss: 6.2092 - precision: 0.9056 - recall: 0.8407 - val_accuracy: 0.8745 - val_loss: 6.0170 - val_precision: 0.8990 - val_recall: 0.8418 - learning_rate: 1.0000e-04\n","Epoch 56/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8397 - loss: 6.0162 - precision: 0.8702 - recall: 0.8105 - val_accuracy: 0.8764 - val_loss: 5.7855 - val_precision: 0.9014 - val_recall: 0.8473 - learning_rate: 1.0000e-04\n","Epoch 57/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8791 - loss: 5.7464 - precision: 0.9052 - recall: 0.8499 - val_accuracy: 0.8800 - val_loss: 5.5637 - val_precision: 0.8998 - val_recall: 0.8491 - learning_rate: 1.0000e-04\n","Epoch 58/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8723 - loss: 5.5586 - precision: 0.9034 - recall: 0.8288 - val_accuracy: 0.8745 - val_loss: 5.3503 - val_precision: 0.9050 - val_recall: 0.8491 - learning_rate: 1.0000e-04\n","Epoch 59/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8480 - loss: 5.3765 - precision: 0.8694 - recall: 0.8080 - val_accuracy: 0.8818 - val_loss: 5.1520 - val_precision: 0.9086 - val_recall: 0.8491 - learning_rate: 1.0000e-04\n","Epoch 60/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8681 - loss: 5.1489 - precision: 0.8940 - recall: 0.8463 - val_accuracy: 0.8836 - val_loss: 4.9572 - val_precision: 0.9089 - val_recall: 0.8527 - learning_rate: 1.0000e-04\n","Epoch 61/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8657 - loss: 4.9549 - precision: 0.9067 - recall: 0.8253 - val_accuracy: 0.8745 - val_loss: 4.7733 - val_precision: 0.9075 - val_recall: 0.8564 - learning_rate: 1.0000e-04\n","Epoch 62/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8763 - loss: 4.7709 - precision: 0.8987 - recall: 0.8480 - val_accuracy: 0.8745 - val_loss: 4.5952 - val_precision: 0.9072 - val_recall: 0.8527 - learning_rate: 1.0000e-04\n","Epoch 63/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8507 - loss: 4.6361 - precision: 0.8933 - recall: 0.8208 - val_accuracy: 0.8782 - val_loss: 4.4258 - val_precision: 0.9054 - val_recall: 0.8527 - learning_rate: 1.0000e-04\n","Epoch 64/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8605 - loss: 4.4340 - precision: 0.8721 - recall: 0.8226 - val_accuracy: 0.8782 - val_loss: 4.2662 - val_precision: 0.9004 - val_recall: 0.8545 - learning_rate: 1.0000e-04\n","Epoch 65/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8639 - loss: 4.2590 - precision: 0.8982 - recall: 0.8389 - val_accuracy: 0.8800 - val_loss: 4.1102 - val_precision: 0.9040 - val_recall: 0.8564 - learning_rate: 1.0000e-04\n","Epoch 66/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8564 - loss: 4.1275 - precision: 0.8955 - recall: 0.8338 - val_accuracy: 0.8764 - val_loss: 3.9661 - val_precision: 0.9027 - val_recall: 0.8600 - learning_rate: 1.0000e-04\n","Epoch 67/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8496 - loss: 3.9904 - precision: 0.8838 - recall: 0.8327 - val_accuracy: 0.8836 - val_loss: 3.8232 - val_precision: 0.9114 - val_recall: 0.8600 - learning_rate: 1.0000e-04\n","Epoch 68/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8720 - loss: 3.8370 - precision: 0.8933 - recall: 0.8511 - val_accuracy: 0.8891 - val_loss: 3.6864 - val_precision: 0.9149 - val_recall: 0.8600 - learning_rate: 1.0000e-04\n","Epoch 69/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8595 - loss: 3.7254 - precision: 0.8861 - recall: 0.8165 - val_accuracy: 0.8836 - val_loss: 3.5557 - val_precision: 0.9065 - val_recall: 0.8636 - learning_rate: 1.0000e-04\n","Epoch 70/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8878 - loss: 3.5321 - precision: 0.9140 - recall: 0.8687 - val_accuracy: 0.8836 - val_loss: 3.4355 - val_precision: 0.8962 - val_recall: 0.8636 - learning_rate: 1.0000e-04\n","Epoch 71/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9060 - loss: 3.3852 - precision: 0.9275 - recall: 0.8797 - val_accuracy: 0.8836 - val_loss: 3.3137 - val_precision: 0.9101 - val_recall: 0.8655 - learning_rate: 1.0000e-04\n","Epoch 72/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8322 - loss: 3.4508 - precision: 0.8623 - recall: 0.8103 - val_accuracy: 0.8891 - val_loss: 3.1990 - val_precision: 0.9138 - val_recall: 0.8673 - learning_rate: 1.0000e-04\n","Epoch 73/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8562 - loss: 3.2547 - precision: 0.8756 - recall: 0.8211 - val_accuracy: 0.8891 - val_loss: 3.0929 - val_precision: 0.9100 - val_recall: 0.8636 - learning_rate: 1.0000e-04\n","Epoch 74/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8549 - loss: 3.1288 - precision: 0.8728 - recall: 0.8311 - val_accuracy: 0.8873 - val_loss: 2.9848 - val_precision: 0.9087 - val_recall: 0.8691 - learning_rate: 1.0000e-04\n","Epoch 75/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8593 - loss: 3.0306 - precision: 0.8777 - recall: 0.8349 - val_accuracy: 0.8873 - val_loss: 2.8886 - val_precision: 0.9140 - val_recall: 0.8691 - learning_rate: 1.0000e-04\n","Epoch 76/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8379 - loss: 3.0048 - precision: 0.8606 - recall: 0.7969 - val_accuracy: 0.8891 - val_loss: 2.7960 - val_precision: 0.9087 - val_recall: 0.8691 - learning_rate: 1.0000e-04\n","Epoch 77/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8711 - loss: 2.8355 - precision: 0.8997 - recall: 0.8499 - val_accuracy: 0.8855 - val_loss: 2.7066 - val_precision: 0.9101 - val_recall: 0.8655 - learning_rate: 1.0000e-04\n","Epoch 78/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8799 - loss: 2.6972 - precision: 0.9021 - recall: 0.8682 - val_accuracy: 0.8891 - val_loss: 2.6161 - val_precision: 0.9212 - val_recall: 0.8709 - learning_rate: 1.0000e-04\n","Epoch 79/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8916 - loss: 2.6227 - precision: 0.9146 - recall: 0.8696 - val_accuracy: 0.8909 - val_loss: 2.5368 - val_precision: 0.9159 - val_recall: 0.8709 - learning_rate: 1.0000e-04\n","Epoch 80/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8989 - loss: 2.5074 - precision: 0.9205 - recall: 0.8790 - val_accuracy: 0.8945 - val_loss: 2.4566 - val_precision: 0.9110 - val_recall: 0.8745 - learning_rate: 1.0000e-04\n","Epoch 81/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8736 - loss: 2.4854 - precision: 0.9093 - recall: 0.8505 - val_accuracy: 0.8982 - val_loss: 2.3804 - val_precision: 0.9229 - val_recall: 0.8709 - learning_rate: 1.0000e-04\n","Epoch 82/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8685 - loss: 2.4234 - precision: 0.8848 - recall: 0.8387 - val_accuracy: 0.9073 - val_loss: 2.3069 - val_precision: 0.9249 - val_recall: 0.8727 - learning_rate: 1.0000e-04\n","Epoch 83/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8560 - loss: 2.3682 - precision: 0.8954 - recall: 0.8282 - val_accuracy: 0.8909 - val_loss: 2.2420 - val_precision: 0.9160 - val_recall: 0.8727 - learning_rate: 1.0000e-04\n","Epoch 84/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8813 - loss: 2.2557 - precision: 0.8954 - recall: 0.8706 - val_accuracy: 0.8982 - val_loss: 2.1717 - val_precision: 0.9250 - val_recall: 0.8745 - learning_rate: 1.0000e-04\n","Epoch 85/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8860 - loss: 2.2081 - precision: 0.9006 - recall: 0.8654 - val_accuracy: 0.9018 - val_loss: 2.1105 - val_precision: 0.9216 - val_recall: 0.8764 - learning_rate: 1.0000e-04\n","Epoch 86/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8771 - loss: 2.1608 - precision: 0.9021 - recall: 0.8350 - val_accuracy: 0.8909 - val_loss: 2.0545 - val_precision: 0.9146 - val_recall: 0.8764 - learning_rate: 1.0000e-04\n","Epoch 87/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8754 - loss: 2.1021 - precision: 0.8976 - recall: 0.8378 - val_accuracy: 0.8945 - val_loss: 1.9923 - val_precision: 0.9205 - val_recall: 0.8836 - learning_rate: 1.0000e-04\n","Epoch 88/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8696 - loss: 2.0573 - precision: 0.8918 - recall: 0.8488 - val_accuracy: 0.9036 - val_loss: 1.9370 - val_precision: 0.9244 - val_recall: 0.8891 - learning_rate: 1.0000e-04\n","Epoch 89/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8908 - loss: 1.9876 - precision: 0.9020 - recall: 0.8575 - val_accuracy: 0.9018 - val_loss: 1.8859 - val_precision: 0.9229 - val_recall: 0.8927 - learning_rate: 1.0000e-04\n","Epoch 90/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8759 - loss: 1.9329 - precision: 0.9004 - recall: 0.8432 - val_accuracy: 0.9055 - val_loss: 1.8364 - val_precision: 0.9242 - val_recall: 0.8873 - learning_rate: 1.0000e-04\n","Epoch 91/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8873 - loss: 1.8553 - precision: 0.9074 - recall: 0.8601 - val_accuracy: 0.9018 - val_loss: 1.7880 - val_precision: 0.9192 - val_recall: 0.8891 - learning_rate: 1.0000e-04\n","Epoch 92/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8604 - loss: 1.8475 - precision: 0.8995 - recall: 0.8458 - val_accuracy: 0.9109 - val_loss: 1.7327 - val_precision: 0.9297 - val_recall: 0.8891 - learning_rate: 1.0000e-04\n","Epoch 93/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8707 - loss: 1.8038 - precision: 0.8919 - recall: 0.8594 - val_accuracy: 0.9073 - val_loss: 1.6848 - val_precision: 0.9303 - val_recall: 0.8982 - learning_rate: 1.0000e-04\n","Epoch 94/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9006 - loss: 1.7370 - precision: 0.9207 - recall: 0.8545 - val_accuracy: 0.9036 - val_loss: 1.6474 - val_precision: 0.9247 - val_recall: 0.8927 - learning_rate: 1.0000e-04\n","Epoch 95/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8574 - loss: 1.7424 - precision: 0.8757 - recall: 0.8281 - val_accuracy: 0.9273 - val_loss: 1.5987 - val_precision: 0.9423 - val_recall: 0.8909 - learning_rate: 1.0000e-04\n","Epoch 96/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8777 - loss: 1.6609 - precision: 0.8964 - recall: 0.8621 - val_accuracy: 0.9145 - val_loss: 1.5569 - val_precision: 0.9390 - val_recall: 0.8964 - learning_rate: 1.0000e-04\n","Epoch 97/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8780 - loss: 1.6349 - precision: 0.9037 - recall: 0.8493 - val_accuracy: 0.9145 - val_loss: 1.5204 - val_precision: 0.9298 - val_recall: 0.8909 - learning_rate: 1.0000e-04\n","Epoch 98/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8608 - loss: 1.6423 - precision: 0.8874 - recall: 0.8421 - val_accuracy: 0.9164 - val_loss: 1.4783 - val_precision: 0.9407 - val_recall: 0.8945 - learning_rate: 1.0000e-04\n","Epoch 99/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8961 - loss: 1.5681 - precision: 0.9085 - recall: 0.8718 - val_accuracy: 0.9182 - val_loss: 1.4451 - val_precision: 0.9362 - val_recall: 0.9073 - learning_rate: 1.0000e-04\n","Epoch 100/100\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9382 - loss: 1.4337 - precision: 0.9491 - recall: 0.9209 - val_accuracy: 0.9200 - val_loss: 1.4011 - val_precision: 0.9449 - val_recall: 0.9036 - learning_rate: 1.0000e-04\n","\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n","K·∫øt qu·∫£ cu·ªëi c√πng tr√™n t·∫≠p valid:\n","  Quadratic Weighted Kappa (QWK): 0.9349\n","  Weighted F1 Score: 0.9161\n","  Weighted Recall: 0.9200\n","  Weighted Precision: 0.9241\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n final_ t·∫°i: /content/drive/MyDrive/working/confusion_matrix_final_episode_21.png\n","Ma tr·∫≠n nh·∫ßm l·∫´n cu·ªëi c√πng:\n","[[271   0   0   0   0]\n"," [  0  43  11   0   2]\n"," [  0   2 146   0   2]\n"," [  0   1  11  15   2]\n"," [  0   2  10   1  31]]\n","ƒê√£ l∆∞u tr·ªçng s·ªë meta-model t·∫°i: /content/drive/MyDrive/working/model.weights.h5\n","ƒê√£ l∆∞u c·∫•u h√¨nh meta-model t·∫°i: /content/drive/MyDrive/working/config.json\n","ƒê√£ l∆∞u si√™u d·ªØ li·ªáu meta-model t·∫°i: /content/drive/MyDrive/working/metadata.json\n","ƒê√£ l∆∞u c√°c ch·ªâ s·ªë ƒë√°nh gi√° t·∫°i: /content/drive/MyDrive/working/final_metrics.json\n","ƒê√£ l∆∞u bi·ªÉu ƒë·ªì l·ªãch s·ª≠ hu·∫•n luy·ªán t·∫°i: /content/drive/MyDrive/working/training_history.png\n","ƒê√°nh gi√° tr√™n t·∫≠p test...\n","\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","K·∫øt qu·∫£ tr√™n t·∫≠p test:\n","  Quadratic Weighted Kappa (QWK): 0.9067\n","  Weighted F1 Score: 0.8392\n","  Weighted Precision: 0.8464\n","  Weighted Recall: 0.8472\n","  Accuracy: 0.8472\n","ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n t·∫≠p test t·∫°i: /content/drive/MyDrive/working/confusion_matrix_test.png\n","Ma tr·∫≠n nh·∫ßm l·∫´n t·∫≠p test:\n","[[356   2   3   0   0]\n"," [  3  37  32   0   2]\n"," [  0  10 176   6   8]\n"," [  0   0  21  12   6]\n"," [  0   2  16   1  40]]\n","ƒê√£ l∆∞u c√°c ch·ªâ s·ªë ƒë√°nh gi√° t·∫≠p test t·∫°i: /content/drive/MyDrive/working/test_metrics.json\n","Ho√†n th√†nh quy tr√¨nh hu·∫•n luy·ªán v√† ƒë√°nh gi√°!\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","import pandas as pd\n","import cv2\n","import albumentations as A\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.applications import EfficientNetB1, Xception, InceptionV3, ResNet50, DenseNet121\n","from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n","from tensorflow.keras.applications.xception import preprocess_input as xception_preprocess\n","from tensorflow.keras.applications.inception_v3 import preprocess_input as inceptionv3_preprocess\n","from tensorflow.keras.applications.resnet50 import preprocess_input as resnet50_preprocess\n","from tensorflow.keras.applications.densenet import preprocess_input as densenet121_preprocess\n","from sklearn.metrics import cohen_kappa_score, f1_score, recall_score, confusion_matrix, precision_score, accuracy_score\n","from sklearn.utils import shuffle\n","from tensorflow.keras.layers import Dense, Input, BatchNormalization, Layer, Dropout\n","from sklearn.decomposition import PCA\n","from sklearn.isotonic import IsotonicRegression\n","import logging\n","from tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau\n","import json\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import gc\n","from google.colab import drive\n","import time\n","import subprocess\n","\n","# Thi·∫øt l·∫≠p tƒÉng tr∆∞·ªüng b·ªô nh·ªõ GPU\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","        print(\"ƒê√£ k√≠ch ho·∫°t tƒÉng tr∆∞·ªüng b·ªô nh·ªõ GPU\")\n","    except RuntimeError as e:\n","        print(f\"L·ªói khi thi·∫øt l·∫≠p tƒÉng tr∆∞·ªüng b·ªô nh·ªõ GPU: {e}\")\n","\n","# Thi·∫øt l·∫≠p th∆∞ m·ª•c l∆∞u tr·ªØ\n","feature_save_dir = \"/content/drive/MyDrive/working\"\n","log_dir = os.path.join(feature_save_dir, \"logs\")\n","os.makedirs(log_dir, exist_ok=True)\n","os.makedirs(feature_save_dir, exist_ok=True)\n","tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n","\n","SIZE = 244\n","NUM_CLASSES = 5\n","TEMP_AUGMENT_DIR = \"/tmp/temp_augmented_images\"\n","os.makedirs(TEMP_AUGMENT_DIR, exist_ok=True)\n","\n","\n","\n","# Ki·ªÉm tra v√† mount Google Drive\n","if not os.path.ismount('/content/drive'):\n","    drive.mount('/content/drive')\n","else:\n","    print(\"Google Drive ƒë√£ ƒë∆∞·ª£c mount.\")\n","\n","# Thi·∫øt l·∫≠p th∆∞ m·ª•c l∆∞u tr·ªØ tr√™n Google Drive\n","feature_save_dir = \"/content/drive/MyDrive/working\"\n","log_dir = os.path.join(feature_save_dir, \"logs\")\n","os.makedirs(log_dir, exist_ok=True)\n","os.makedirs(feature_save_dir, exist_ok=True)\n","tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n","\n","# ƒê·ªçc d·ªØ li·ªáu\n","drive_folder = \"/content/drive/MyDrive/kaggle_data/aptos2019\"\n","processed_folder = \"/content/processed_train_images\"\n","df_train = pd.read_csv(os.path.join(drive_folder, \"train.csv\"))\n","processed_ids = [f.replace('.png', '') for f in os.listdir(processed_folder) if f.endswith('.png')]\n","df_train_processed = df_train[df_train['id_code'].isin(processed_ids)].copy()\n","\n","\n","\n","# Chia d·ªØ li·ªáu\n","x = df_train_processed['id_code']  # id_code c√≥ ƒëu√¥i .png\n","y = df_train_processed['diagnosis']\n","if len(x) < 2:\n","    logging.error(\"S·ªë m·∫´u qu√° √≠t ƒë·ªÉ chia d·ªØ li·ªáu.\")\n","    raise ValueError(\"C·∫ßn √≠t nh·∫•t 2 m·∫´u ƒë·ªÉ chia train/valid.\")\n","x, y = shuffle(x, y, random_state=42)\n","train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.15, stratify=y, random_state=42)\n","\n","logging.info(f\"Train X shape: {train_x.shape}\")\n","logging.info(f\"Valid X shape: {valid_x.shape}\")\n","\n","# T·∫£i ·∫£nh ƒë√£ x·ª≠ l√Ω\n","def load_processed_image(image_id, processed_folder, size=244):\n","    try:\n","        img_path = os.path.join(processed_folder, f\"{image_id}.png\")\n","        if not os.path.exists(img_path):\n","            logging.error(f\"·∫¢nh kh√¥ng t·ªìn t·∫°i: {img_path}\")\n","            return None\n","        img = cv2.imread(img_path)\n","        if img is None:\n","            logging.error(f\"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c ·∫£nh: {img_path}\")\n","            return None\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = cv2.resize(img, (size, size), interpolation=cv2.INTER_AREA)\n","        return img\n","    except Exception as e:\n","        logging.error(f\"L·ªói khi t·∫£i ·∫£nh {image_id}: {str(e)}\")\n","        return None\n","\n","resized_train_x = np.array([load_processed_image(id_code, processed_folder, size=SIZE) for id_code in train_x])\n","resized_valid_x = np.array([load_processed_image(id_code, processed_folder, size=SIZE) for id_code in valid_x])\n","\n","train_y_multi = tf.keras.utils.to_categorical(train_y, num_classes=NUM_CLASSES)\n","valid_y_multi = tf.keras.utils.to_categorical(valid_y, num_classes=NUM_CLASSES)\n","\n","# T·∫£i ·∫£nh ƒë√£ x·ª≠ l√Ω\n","def load_processed_image(image_id, processed_folder, size=244):\n","    try:\n","        img_path = os.path.join(processed_folder, f\"{image_id}.png\")\n","        if not os.path.exists(img_path):\n","            logging.error(f\"·∫¢nh kh√¥ng t·ªìn t·∫°i: {img_path}\")\n","            return None\n","        img = cv2.imread(img_path)\n","        if img is None:\n","            logging.error(f\"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c ·∫£nh: {img_path}\")\n","            return None\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = cv2.resize(img, (size, size), interpolation=cv2.INTER_AREA)\n","        return img\n","    except Exception as e:\n","        logging.error(f\"L·ªói khi t·∫£i ·∫£nh {image_id}: {str(e)}\")\n","        return None\n","\n","resized_train_x = np.array([load_processed_image(id_code, processed_folder, size=SIZE) for id_code in train_x])\n","resized_valid_x = np.array([load_processed_image(id_code, processed_folder, size=SIZE) for id_code in valid_x])\n","resized_test_x = np.array([load_processed_image(id_code, processed_folder, size=SIZE) for id_code in test_x])\n","\n","train_y_multi = tf.keras.utils.to_categorical(train_y, num_classes=NUM_CLASSES)\n","valid_y_multi = tf.keras.utils.to_categorical(valid_y, num_classes=NUM_CLASSES)\n","test_y_multi = tf.keras.utils.to_categorical(test_y, num_classes=NUM_CLASSES)\n","\n","# H√†m custom_random_erasing\n","def custom_random_erasing(image, scale=(0.01, 0.05), ratio=(0.5, 2.0), p=0.3, value=None):\n","    if np.random.random() > p:\n","        return image\n","    height, width, channels = image.shape\n","    area = height * width\n","    scale_factor = np.random.uniform(scale[0], scale[1])\n","    erase_area = area * scale_factor\n","    aspect_ratio = np.random.uniform(ratio[0], ratio[1])\n","    erase_height = int(np.sqrt(erase_area / aspect_ratio))\n","    erase_width = int(np.sqrt(erase_area * aspect_ratio))\n","    erase_height = min(erase_height, height)\n","    erase_width = min(erase_width, width)\n","    if erase_height < 1 or erase_width < 1:\n","        return image\n","    x = np.random.randint(0, width - erase_width + 1)\n","    y = np.random.randint(0, height - erase_height + 1)\n","    output = image.copy()\n","    if value is None:\n","        value = np.mean(image, axis=(0, 1))\n","    output[y:y+erase_height, x:x+erase_width, :] = value\n","    return output\n","\n","# H√†m balance_and_augment_data\n","def balance_and_augment_data(images, labels, target_classes=[0, 1, 2, 3, 4], samples_per_class=None):\n","    num_classes = labels.shape[1]\n","    label_indices = np.argmax(labels, axis=1)\n","    keep_indices = np.isin(label_indices, target_classes)\n","    filtered_images = images[keep_indices]\n","    filtered_labels = labels[keep_indices]\n","    filtered_label_indices = label_indices[keep_indices]\n","    class_counts = np.bincount(filtered_label_indices, minlength=num_classes)\n","    print(f\"Ph√¢n b·ªë nh√£n ban ƒë·∫ßu: {dict(zip(range(num_classes), class_counts))}\")\n","    cls_counts = [class_counts[cls] for cls in target_classes]\n","    max_count = samples_per_class or max(cls_counts)\n","    print(f\"S·ªë m·∫´u m·ª•c ti√™u m·ªói l·ªõp: {max_count}\")\n","    augmenter = A.Compose([\n","        A.HorizontalFlip(p=0.5),\n","        A.Rotate(limit=15, p=0.3),  # Reduced rotation\n","        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),  # Reduced intensity\n","        A.GaussNoise(var_limit=(10.0, 20.0), p=0.2),\n","        A.CLAHE(clip_limit=1.0, tile_grid_size=(8, 8), p=0.2),  # Reduced CLAHE\n","    ])\n","    new_images = []\n","    new_labels = []\n","    for cls in target_classes:\n","        cls_indices = np.where(filtered_label_indices == cls)[0]\n","        cls_images = filtered_images[cls_indices]\n","        cls_labels = filtered_labels[cls_indices]\n","        current_count = len(cls_indices)\n","        new_images.extend(cls_images)\n","        new_labels.extend(cls_labels)\n","        print(f\"L·ªõp {cls}: {current_count} m·∫´u ban ƒë·∫ßu\")\n","        augment_count = max_count - current_count\n","        if augment_count > 0:\n","            print(f\"TƒÉng c∆∞·ªùng {augment_count} m·∫´u cho l·ªõp {cls}\")\n","            for _ in range(augment_count):\n","                idx = np.random.choice(cls_indices)\n","                img = filtered_images[idx].astype(np.uint8)\n","                aug_img = augmenter(image=img)['image']\n","                aug_img = custom_random_erasing(\n","                    aug_img, scale=(0.01, 0.05), ratio=(0.5, 2.0), p=0.0, value=np.mean(aug_img, axis=(0, 1))  # Disabled\n","                )\n","                new_images.append(aug_img)\n","                new_labels.append(filtered_labels[idx])\n","    new_images = np.array(new_images, dtype=np.float32)\n","    new_labels = np.array(new_labels, dtype=np.float32)\n","    new_images, new_labels = shuffle(new_images, new_labels, random_state=42)\n","    final_class_counts = np.bincount(np.argmax(new_labels, axis=1), minlength=num_classes)\n","    print(f\"Ph√¢n b·ªë nh√£n sau c√¢n b·∫±ng: {dict(zip(range(num_classes), final_class_counts))}\")\n","    return new_images, new_labels\n","\n","# C√¢n b·∫±ng d·ªØ li·ªáu train\n","class_counts = np.bincount(train_y)\n","class_0_count = class_counts[0]\n","print(f\"S·ªë m·∫´u l·ªõp 0: {class_0_count}\")\n","balanced_train_x, balanced_train_y_multi = balance_and_augment_data(\n","    resized_train_x, train_y_multi, target_classes=[1, 2, 3, 4], samples_per_class=class_0_count\n",")\n","class_0_indices = np.where(np.argmax(train_y_multi, axis=1) == 0)[0]\n","class_0_images = resized_train_x[class_0_indices]\n","class_0_labels = train_y_multi[class_0_indices]\n","balanced_train_x = np.concatenate([balanced_train_x, class_0_images], axis=0)\n","balanced_train_y_multi = np.concatenate([balanced_train_y_multi, class_0_labels], axis=0)\n","balanced_train_x, balanced_train_y_multi = shuffle(balanced_train_x, balanced_train_y_multi, random_state=42)\n","final_class_counts = np.bincount(np.argmax(balanced_train_y_multi, axis=1), minlength=5)\n","print(f\"Ph√¢n b·ªë nh√£n sau khi th√™m l·ªõp 0: {dict(zip(range(5), final_class_counts))}\")\n","print(\"balanced_train_x shape:\", balanced_train_x.shape)\n","print(\"balanced_train_y_multi shape:\", balanced_train_y_multi.shape)\n","\n","# ƒê·ªãnh nghƒ©a model_configs\n","model_configs = {\n","    \"efficientnetb1\": {\n","        \"model_type\": \"efficientnetb1\",\n","        \"config_path\": \"/content/drive/MyDrive/working/EfficientNetB1_bestqwk_aptos/config.json\",\n","        \"weights_path\": \"/content/drive/MyDrive/working/EfficientNetB1_bestqwk_aptos/model.weights.h5\",\n","        \"preprocess\": efficientnet_preprocess,\n","        \"img_size\": 244,\n","        \"base_model\": EfficientNetB1\n","    },\n","    \"xception\": {\n","        \"model_type\": \"xception\",\n","        \"config_path\": \"/content/drive/MyDrive/working/Xception_bestqwk_aptos/config.json\",\n","        \"weights_path\": \"/content/drive/MyDrive/working/Xception_bestqwk_aptos/model.weights.h5\",\n","        \"preprocess\": xception_preprocess,\n","        \"img_size\": 244,\n","        \"base_model\": Xception\n","    },\n","    \"inceptionv3\": {\n","        \"model_type\": \"inceptionv3\",\n","        \"config_path\": \"/content/drive/MyDrive/working/InceptionV3_bestqwk_aptos/config.json\",\n","        \"weights_path\": \"/content/drive/MyDrive/working/InceptionV3_bestqwk_aptos/model.weights.h5\",\n","        \"preprocess\": inceptionv3_preprocess,\n","        \"img_size\": 299,\n","        \"base_model\": InceptionV3\n","    },\n","    \"resnet50\": {\n","        \"model_type\": \"resnet50\",\n","        \"config_path\": \"/content/drive/MyDrive/working/ResNet50_bestqwk_aptos/config.json\",\n","        \"weights_path\": \"/content/drive/MyDrive/working/ResNet50_bestqwk_aptos/model.weights.h5\",\n","        \"preprocess\": resnet50_preprocess,\n","        \"img_size\": 244,\n","        \"base_model\": ResNet50\n","    },\n","    \"densenet121\": {\n","        \"model_type\": \"densenet121\",\n","        \"config_path\": \"/content/drive/MyDrive/working/DenseNet121_bestqwk_aptos/config.json\",\n","        \"weights_path\": \"/content/drive/MyDrive/working/DenseNet121_bestqwk_aptos/model.weights.h5\",\n","        \"preprocess\": densenet121_preprocess,\n","        \"img_size\": 244,\n","        \"base_model\": DenseNet121\n","    }\n","}\n","\n","# My_Generator v·ªõi mixup\n","class My_Generator(tf.keras.utils.Sequence):\n","    def __init__(self, images, labels, batch_size, is_train=False, mix=True, augment=False, size1=244, size2=299, model_type=\"default\", preprocess=None):\n","        self.labels = np.array(labels, dtype=np.float32)\n","        self.batch_size = batch_size\n","        self.is_train = is_train\n","        self.is_augment = augment\n","        self.is_mix = mix\n","        self.model_type = str(model_type).lower()\n","        self.preprocess = preprocess\n","        self.temp_augment_dir = TEMP_AUGMENT_DIR\n","        os.makedirs(self.temp_augment_dir, exist_ok=True)\n","        self.target_size = (size2, size2) if 'inceptionv3' in self.model_type or 'xception' in self.model_type else (size1, size1)\n","        self.image_paths = []\n","        if isinstance(images, np.ndarray):\n","            for i, img in enumerate(images):\n","                img_path = os.path.join(self.temp_augment_dir, f\"img_{i}_{np.random.randint(1000000)}.png\")\n","                try:\n","                    if img.dtype != np.uint8:\n","                        if img.max() <= 1.0:\n","                            img = (img * 255).astype(np.uint8)\n","                        else:\n","                            img = img.astype(np.uint8)\n","                    cv2.imwrite(img_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n","                    self.image_paths.append(img_path)\n","                except Exception as e:\n","                    logging.error(f\"L·ªói khi l∆∞u ·∫£nh {img_path}: {str(e)}\")\n","                    continue\n","        else:\n","            self.image_paths = list(images)\n","        unique_paths = []\n","        unique_indices = []\n","        seen = set()\n","        for i, path in enumerate(self.image_paths):\n","            if path not in seen:\n","                seen.add(path)\n","                unique_paths.append(path)\n","                unique_indices.append(i)\n","        self.image_paths = unique_paths\n","        self.labels = self.labels[unique_indices]\n","        if len(self.image_paths) != len(self.labels):\n","            logging.error(f\"S·ªë image_paths ({len(self.image_paths)}) kh√¥ng kh·ªõp v·ªõi s·ªë labels ({len(self.labels)})\")\n","            self.labels = self.labels[:len(self.image_paths)]\n","        if not self.image_paths:\n","            raise ValueError(\"Kh√¥ng c√≥ image_paths h·ª£p l·ªá ƒë∆∞·ª£c t·∫°o.\")\n","        print(f\"Kh·ªüi t·∫°o My_Generator: {len(self.image_paths)} m·∫´u, target_size={self.target_size}, is_train={is_train}\")\n","        self.dataset = self._create_dataset()\n","\n","    def _load_image(self, img_path):\n","        img = tf.io.read_file(img_path)\n","        img = tf.image.decode_png(img, channels=3)\n","        img = tf.image.resize(img, self.target_size, method=tf.image.ResizeMethod.BILINEAR)\n","        img = tf.ensure_shape(img, [self.target_size[0], self.target_size[1], 3])\n","        img = tf.cast(img, tf.float32) / 255.0\n","        if self.preprocess is not None:\n","            img = self.preprocess(img)\n","        return img\n","\n","    def _mixup(self, images, labels):\n","        batch_size = tf.shape(images)[0]\n","        lam = tf.random.uniform([], minval=0.2, maxval=0.4, dtype=tf.float32)\n","        indices = tf.random.shuffle(tf.range(batch_size, dtype=tf.int32))\n","        mixed_images = lam * images + (1 - lam) * tf.gather(images, indices)\n","        mixed_labels = lam * labels + (1 - lam) * tf.gather(labels, indices)\n","        return mixed_images, mixed_labels\n","\n","    def _create_dataset(self):\n","        dataset = tf.data.Dataset.from_tensor_slices((self.image_paths, self.labels))\n","        dataset = dataset.map(\n","            lambda img_path, label: (self._load_image(img_path), label),\n","            num_parallel_calls=tf.data.AUTOTUNE\n","        )\n","        if self.is_train:\n","            dataset = dataset.shuffle(buffer_size=len(self.image_paths))\n","        dataset = dataset.batch(self.batch_size, drop_remainder=False)\n","        if self.is_train and self.is_mix:\n","            dataset = dataset.map(\n","                self._mixup,\n","                num_parallel_calls=tf.data.AUTOTUNE\n","            )\n","        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n","        return dataset\n","\n","    def __len__(self):\n","        return int(np.ceil(len(self.image_paths) / self.batch_size))\n","\n","    def __iter__(self):\n","        self.iterator = iter(self.dataset)\n","        return self\n","\n","    def __next__(self):\n","        return next(self.iterator)\n","\n","# Callback ƒë·ªÉ t√≠nh tr·ªçng s·ªë l·ªõp t·ª´ confusion matrix\n","class ConfusionMatrixWeightCallback(tf.keras.callbacks.Callback):\n","    def __init__(self, valid_features, valid_labels, classification_model, num_classes=5, class_counts=None):\n","        super().__init__()\n","        self.valid_features = valid_features\n","        self.valid_labels = valid_labels\n","        self.classification_model = classification_model\n","        self.num_classes = num_classes\n","        self.prev_cm = None\n","        self.class_weights = np.ones(num_classes, dtype=np.float32)\n","        self.class_counts = class_counts\n","        self.history_dir = os.path.join(feature_save_dir, \"history\")\n","        os.makedirs(self.history_dir, exist_ok=True)\n","        self.weights_history = []\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        y_pred = self.classification_model.predict(self.valid_features, verbose=0, batch_size=32)\n","        y_true = np.argmax(self.valid_labels, axis=1)\n","        y_pred_classes = np.argmax(y_pred, axis=1)\n","        cm = confusion_matrix(y_true, y_pred_classes, labels=list(range(self.num_classes)))\n","        print(f\"Epoch {epoch+1} - Ma tr·∫≠n nh·∫ßm l·∫´n:\\n{cm}\")\n","        errors = np.sum(cm * (1 - np.eye(self.num_classes)), axis=1)\n","        total_samples_per_class = np.sum(cm, axis=1)\n","        total_samples_per_class = np.where(total_samples_per_class == 0, 1, total_samples_per_class)\n","        error_rates = errors / total_samples_per_class\n","        weak_classes = []\n","        if self.class_counts is not None:\n","            min_count = np.min(self.class_counts[self.class_counts > 0])\n","            weak_classes = np.where(self.class_counts <= min_count * 1.5)[0]\n","        high_error_classes = np.where(error_rates >= np.percentile(error_rates, 75))[0]\n","        weak_classes = np.unique(np.concatenate([weak_classes, high_error_classes])).astype(int)\n","        self.class_weights = 1.0 + error_rates\n","        for cls in weak_classes:\n","            self.class_weights[cls] *= 2.0\n","        self.class_weights /= self.class_weights.max()\n","        print(f\"Epoch {epoch+1} - L·ªõp y·∫øu: {weak_classes}\")\n","        print(f\"Epoch {epoch+1} - Tr·ªçng s·ªë l·ªõp: {self.class_weights}\")\n","        self.weights_history.append({\n","            \"epoch\": epoch + 1,\n","            \"class_weights\": self.class_weights.tolist(),\n","            \"weak_classes\": weak_classes.tolist(),\n","            \"confusion_matrix\": cm.tolist()\n","        })\n","        weights_path = os.path.join(self.history_dir, f\"class_weights_epoch_{epoch+1}.json\")\n","        with open(weights_path, 'w') as f:\n","            json.dump(self.weights_history[-1], f, indent=4)\n","        print(f\"ƒê√£ l∆∞u tr·ªçng s·ªë l·ªõp t·∫°i: {weights_path}\")\n","        plt.figure(figsize=(8, 6))\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                    xticklabels=list(range(self.num_classes)),\n","                    yticklabels=list(range(self.num_classes)))\n","        plt.title(f'Ma tr·∫≠n nh·∫ßm l·∫´n - Epoch {epoch+1}')\n","        plt.xlabel('D·ª± ƒëo√°n')\n","        plt.ylabel('Th·ª±c t·∫ø')\n","        cm_path = os.path.join(feature_save_dir, f'confusion_matrix_epoch_{epoch+1}.png')\n","        plt.savefig(cm_path)\n","        plt.close()\n","        print(f\"ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n t·∫°i: {cm_path}\")\n","        self.prev_cm = cm.copy()\n","\n","    def get_class_weights(self):\n","        return self.class_weights\n","\n","# C√°c h√†m v√† l·ªõp h·ªó tr·ª£\n","def load_model_from_config(config_path, weights_path, base_model_class):\n","    try:\n","        if config_path and os.path.exists(config_path) and weights_path and os.path.exists(weights_path):\n","            with open(config_path, 'r') as f:\n","                model_config = json.load(f)\n","            model = tf.keras.models.model_from_json(json.dumps(model_config))\n","            model.load_weights(weights_path)\n","            return model\n","        raise FileNotFoundError\n","    except:\n","        return base_model_class(weights='imagenet', include_top=False, pooling='avg')\n","\n","class GradientReversalLayer(Layer):\n","    def __init__(self, lambda_=1.0, **kwargs):\n","        super().__init__(**kwargs)\n","        self.lambda_ = lambda_\n","    def call(self, inputs, training=None):\n","        inputs = tf.convert_to_tensor(inputs, dtype=tf.float32)\n","        return inputs if not training else tf.math.multiply(-self.lambda_, inputs)\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\"lambda_\": self.lambda_})\n","        return config\n","\n","class MemoryAugmentedLayer(tf.keras.layers.Layer):\n","    def __init__(self, memory_size, memory_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.memory_size = memory_size\n","        self.memory_dim = memory_dim\n","    def build(self, input_shape):\n","        self.memory = self.add_weight(\n","            shape=(self.memory_size, self.memory_dim),\n","            initializer='zeros',\n","            trainable=False,\n","            dtype=tf.float32\n","        )\n","        super().build(input_shape)\n","    def call(self, inputs):\n","        batch_size = tf.shape(inputs)[0]\n","        memory_size = tf.shape(self.memory)[0]\n","        memory_sliced = tf.cond(\n","            batch_size > memory_size,\n","            lambda: tf.tile(self.memory, [(batch_size + memory_size - 1) // memory_size, 1])[:batch_size],\n","            lambda: self.memory[:batch_size]\n","        )\n","        return tf.reduce_mean(tf.stack([inputs, memory_sliced], axis=0), axis=0)\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({'memory_size': self.memory_size, 'memory_dim': self.memory_dim})\n","        return config\n","\n","class CustomGridDropout(tf.keras.layers.Layer):\n","    def __init__(self, ratio=0.3, holes_number=4, p=0.5, **kwargs):\n","        super().__init__(**kwargs)\n","        self.ratio = ratio\n","        self.holes_number = holes_number\n","        self.p = p\n","    def call(self, inputs, training=None):\n","        if not training:\n","            return inputs\n","        inputs = tf.convert_to_tensor(inputs, dtype=tf.float32)\n","        batch_size = tf.shape(inputs)[0]\n","        feature_dim = tf.shape(inputs)[1]\n","        hole_size = tf.maximum(1, tf.cast(tf.cast(feature_dim, tf.float32) * self.ratio, tf.int32))\n","        mask = tf.ones_like(inputs, dtype=tf.float32)\n","        random_probs = tf.random.uniform([self.holes_number], 0, 1)\n","        active_holes = tf.cast(random_probs < self.p, tf.int32)\n","        hole_indices = tf.range(self.holes_number)\n","        start_indices = (hole_indices * feature_dim) // self.holes_number\n","        end_indices = tf.minimum(start_indices + hole_size, feature_dim)\n","        all_indices = []\n","        for i in range(self.holes_number):\n","            should_apply = active_holes[i]\n","            indices = tf.cond(\n","                should_apply > 0,\n","                lambda: tf.stack([\n","                    tf.tile(tf.range(batch_size), [end_indices[i] - start_indices[i]]),\n","                    tf.repeat(tf.range(start_indices[i], end_indices[i]), batch_size)\n","                ], axis=1),\n","                lambda: tf.zeros([0, 2], dtype=tf.int32)\n","            )\n","            all_indices.append(indices)\n","        all_indices = tf.concat(all_indices, axis=0)\n","        updates = tf.zeros([tf.shape(all_indices)[0]], dtype=tf.float32)\n","        mask = tf.tensor_scatter_nd_update(mask, all_indices, updates)\n","        return inputs * mask\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"ratio\": self.ratio,\n","            \"holes_number\": self.holes_number,\n","            \"p\": self.p\n","        })\n","        return config\n","\n","from sklearn.preprocessing import StandardScaler\n","def normalize_features(features, target_dim):\n","    try:\n","        if len(features.shape) != 2:\n","            raise ValueError(f\"Expected 2D features, got shape {features.shape}\")\n","        scaler = StandardScaler()\n","        normalized = scaler.fit_transform(features)\n","        current_dim = normalized.shape[1]\n","        if current_dim == target_dim:\n","            return normalized\n","        elif current_dim > target_dim:\n","            return normalized[:, :target_dim]\n","        else:\n","            padding = np.zeros((normalized.shape[0], target_dim - current_dim), dtype=np.float32)\n","            return np.concatenate([normalized, padding], axis=1)\n","    except Exception as e:\n","        logging.error(f\"Error normalizing features: {str(e)}\")\n","        return np.zeros((features.shape[0], target_dim), dtype=np.float32)\n","\n","def extract_and_save_features(model_name, feature_extractor, generator, save_dir, sample_ids):\n","    expected_samples = len(generator.image_paths)\n","    features_2d = []\n","    processed_samples = []\n","    iterator = iter(generator.dataset)\n","    steps = int(np.ceil(expected_samples / generator.batch_size))\n","    for step in range(steps):\n","        try:\n","            batch_data = next(iterator, None)\n","            if batch_data is None:\n","                break\n","            batch_images, _ = batch_data\n","            if batch_images.shape[0] == 0:\n","                continue\n","            batch_features_2d = feature_extractor(batch_images, training=False)\n","            features_2d.append(batch_features_2d.numpy().astype(np.float32))\n","            processed_samples.extend(sample_ids[step * generator.batch_size: (step + 1) * generator.batch_size][:batch_images.shape[0]])\n","        except Exception as e:\n","            logging.error(f\"L·ªói t·∫°i batch {step+1}: {str(e)}\")\n","            continue\n","    features_2d = np.concatenate(features_2d, axis=0) if features_2d else np.zeros((expected_samples, 512), dtype=np.float32)\n","    if features_2d.shape[0] != expected_samples:\n","        features_2d = features_2d[:expected_samples] if features_2d.shape[0] > expected_samples else \\\n","                      np.pad(features_2d, ((0, expected_samples - features_2d.shape[0]), (0, 0)), mode='edge')\n","    os.makedirs(save_dir, exist_ok=True)\n","    features_2d_path = os.path.join(save_dir, f\"{model_name}_features_2d.npy\")\n","    try:\n","        np.save(features_2d_path, features_2d)\n","        for _ in range(3):\n","            subprocess.run([\"sync\"])\n","            time.sleep(1)\n","            if os.path.exists(features_2d_path):\n","                print(f\"ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D t·∫°i: {features_2d_path}, shape={features_2d.shape}\")\n","                break\n","        else:\n","            logging.error(f\"Kh√¥ng th·ªÉ l∆∞u t·ªáp 2D t·∫°i: {features_2d_path} sau nhi·ªÅu l·∫ßn th·ª≠\")\n","    except Exception as e:\n","        logging.error(f\"L·ªói khi l∆∞u ƒë·∫∑c tr∆∞ng 2D t·∫°i {features_2d_path}: {str(e)}\")\n","        raise\n","    metadata = {\n","        \"model_name\": model_name,\n","        \"features_2d_path\": features_2d_path,\n","        \"sample_ids\": processed_samples,\n","        \"timestamp\": pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n","    }\n","    metadata_path = os.path.join(save_dir, f\"{model_name}_features_metadata.json\")\n","    try:\n","        with open(metadata_path, 'w') as f:\n","            json.dump(metadata, f, indent=4)\n","        subprocess.run([\"sync\"])\n","        print(f\"ƒê√£ l∆∞u metadata t·∫°i: {metadata_path}\")\n","    except Exception as e:\n","        logging.error(f\"L·ªói khi l∆∞u metadata t·∫°i {metadata_path}: {str(e)}\")\n","    return features_2d\n","\n","def combine_and_reduce_features(features_dict, labels, sample_ids, save_dir, n_components=50, target_dim=512):\n","    num_samples = len(sample_ids)\n","    normalized_features = {}\n","    for model_name, features in features_dict.items():\n","        if features.shape[0] == num_samples:\n","            normalized_features[model_name] = normalize_features(features, target_dim)\n","        else:\n","            features_adj = features[:num_samples] if features.shape[0] > num_samples else \\\n","                           np.pad(features, ((0, num_samples - features.shape[0]), (0, 0)), mode='edge')\n","            normalized_features[model_name] = normalize_features(features_adj, target_dim)\n","    for model_name, features in normalized_features.items():\n","        features_path = os.path.join(save_dir, f\"{model_name}_normalized_features_2d.npy\")\n","        np.save(features_path, features)\n","        print(f\"ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D chu·∫©n h√≥a cho {model_name} t·∫°i: {features_path}, shape={features.shape}\")\n","    combined_features = []\n","    valid_indices = []\n","    for i in range(num_samples):\n","        sample_features = [normalized_features[model_name][i] for model_name in normalized_features\n","                          if i < len(normalized_features[model_name])]\n","        if sample_features:\n","            combined_features.append(np.concatenate(sample_features))\n","            valid_indices.append(i)\n","    combined_features = np.array(combined_features, dtype=np.float32) if combined_features else \\\n","                       np.zeros((num_samples, target_dim * len(normalized_features)), dtype=np.float32)\n","    valid_indices = np.array(valid_indices) if valid_indices else np.arange(num_samples)\n","    combined_features_path = os.path.join(save_dir, \"combined_features_2d_before_pca.npy\")\n","    np.save(combined_features_path, combined_features)\n","    print(f\"ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D k·∫øt h·ª£p (tr∆∞·ªõc PCA) t·∫°i: {combined_features_path}, shape={combined_features.shape}\")\n","    pca = None\n","    reduced_features = combined_features\n","    if n_components is not None:\n","        pca = PCA(n_components=n_components)\n","        reduced_features = pca.fit_transform(combined_features)\n","    reduced_features_path = os.path.join(save_dir, \"combined_features_2d_after_pca.npy\")\n","    np.save(reduced_features_path, reduced_features)\n","    print(f\"ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D k·∫øt h·ª£p (sau PCA) t·∫°i: {reduced_features_path}, shape={reduced_features.shape}\")\n","    metadata = {\n","        \"model_names\": list(features_dict.keys()),\n","        \"num_samples\": num_samples,\n","        \"target_dim\": target_dim,\n","        \"n_components\": n_components,\n","        \"sample_ids\": sample_ids.tolist(),\n","        \"labels\": labels.tolist(),\n","        \"features_2d_paths\": {model_name: os.path.join(save_dir, f\"{model_name}_normalized_features_2d.npy\")\n","                             for model_name in features_dict},\n","        \"combined_features_before_pca\": combined_features_path,\n","        \"combined_features_after_pca\": reduced_features_path,\n","        \"timestamp\": pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n","    }\n","    metadata_path = os.path.join(save_dir, \"combined_features_metadata.json\")\n","    with open(metadata_path, 'w') as f:\n","        json.dump(metadata, f, indent=4)\n","    print(f\"ƒê√£ l∆∞u metadata t·∫°i: {metadata_path}\")\n","    return reduced_features, pca, valid_indices, reduced_features.shape[1]\n","\n","def save_meta_learner_features(meta_feature_model, features, labels, sample_ids, save_dir):\n","    meta_features_2d = meta_feature_model.predict(features, batch_size=32, verbose=0)\n","    meta_features_2d_path = os.path.join(save_dir, \"meta_learner_features_2d.npy\")\n","    np.save(meta_features_2d_path, meta_features_2d)\n","    print(f\"ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D c·ªßa meta-learner t·∫°i: {meta_features_2d_path}, shape={meta_features_2d.shape}\")\n","    metadata = {\n","        \"model_name\": \"meta_learner\",\n","        \"features_2d_path\": meta_features_2d_path,\n","        \"sample_ids\": sample_ids.tolist(),\n","        \"labels\": labels.tolist(),\n","        \"timestamp\": pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n","    }\n","    metadata_path = os.path.join(save_dir, \"meta_learner_features_metadata.json\")\n","    with open(metadata_path, 'w') as f:\n","        json.dump(metadata, f, indent=4)\n","    print(f\"ƒê√£ l∆∞u metadata meta-learner t·∫°i: {metadata_path}\")\n","    return meta_features_2d\n","\n","def augment_single_class(features, labels, cls, num_samples_needed):\n","    cls_indices = np.where(np.argmax(labels, axis=1) == cls)[0]\n","    if len(cls_indices) == 0:\n","        return [], []\n","    augmenter = A.Compose([\n","        A.HorizontalFlip(p=0.5),\n","        A.VerticalFlip(p=0.5),\n","        A.Rotate(limit=45, p=0.7),\n","        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),\n","        A.GaussNoise(p=0.5),\n","        A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.3),\n","        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n","        A.RandomCrop(height=int(SIZE*0.9), width=int(SIZE*0.9), p=0.3),\n","        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=30, p=0.5)\n","    ])\n","    aug_features = []\n","    aug_labels = []\n","    for _ in range(num_samples_needed):\n","        idx = np.random.choice(cls_indices)\n","        img = features[idx].astype(np.uint8)\n","        aug_img = augmenter(image=img)['image']\n","        aug_img = custom_random_erasing(\n","            aug_img, scale=(0.01, 0.05), ratio=(0.5, 2.0), p=0.3, value=np.mean(aug_img, axis=(0, 1))\n","        )\n","        aug_features.append(aug_img)\n","        aug_labels.append(labels[idx])\n","    return np.array(aug_features, dtype=np.float32), np.array(aug_labels, dtype=np.float32)\n","\n","def create_episode(features, labels, n_support=10, n_query=10, hard_sample_ratio=0.3, class_3_multiplier=2):\n","    target_classes = [0, 1, 2, 3, 4]\n","    if len(labels.shape) > 1:\n","        label_indices = np.argmax(labels, axis=1)\n","    else:\n","        label_indices = labels\n","    keep_indices = np.isin(label_indices, target_classes)\n","    features = features[keep_indices]\n","    labels = labels[keep_indices]\n","    label_indices = label_indices[keep_indices]\n","    support_features, support_labels, query_features, query_labels = [], [], [], []\n","    hard_indices = []\n","    n_support_class_3 = int(n_support * class_3_multiplier)\n","    n_query_class_3 = int(n_query * class_3_multiplier)\n","    for label in target_classes:\n","        indices = np.where(label_indices == label)[0]\n","        n_support_cls = n_support_class_3 if label == 3 else n_support\n","        n_query_cls = n_query_class_3 if label == 3 else n_query\n","        min_samples_per_class = n_support_cls + n_query_cls\n","        if len(indices) == 0:\n","            logging.warning(f\"L·ªõp {label} kh√¥ng c√≥ m·∫´u. B·ªè qua.\")\n","            continue\n","        if len(indices) < min_samples_per_class:\n","            logging.info(f\"L·ªõp {label} ch·ªâ c√≥ {len(indices)} m·∫´u, c·∫ßn {min_samples_per_class}. S·ª≠ d·ª•ng oversampling.\")\n","            indices = np.random.choice(indices, size=min_samples_per_class, replace=True)\n","        support_indices = np.random.choice(indices, n_support_cls, replace=False)\n","        remaining_indices = np.setdiff1d(indices, support_indices)\n","        n_hard = int(n_query_cls * hard_sample_ratio)\n","        if len(remaining_indices) < n_hard:\n","            hard_samples = np.random.choice(remaining_indices, n_hard, replace=True)\n","        else:\n","            hard_samples = np.random.choice(remaining_indices, n_hard, replace=False)\n","        easy_samples = np.setdiff1d(remaining_indices, hard_samples)\n","        n_easy = n_query_cls - len(hard_samples)\n","        if n_easy > 0:\n","            if len(easy_samples) < n_easy:\n","                easy_samples = np.random.choice(easy_samples, n_easy, replace=True)\n","            else:\n","                easy_samples = np.random.choice(easy_samples, n_easy, replace=False)\n","            query_indices = np.concatenate([hard_samples, easy_samples])\n","        else:\n","            query_indices = hard_samples\n","        support_features.extend(features[support_indices])\n","        support_labels.extend(labels[support_indices])\n","        query_features.extend(features[query_indices])\n","        query_labels.extend(labels[query_indices])\n","        hard_indices.extend(hard_samples)\n","    support_features = np.array(support_features, dtype=np.float32)\n","    support_labels = np.array(support_labels, dtype=np.float32)\n","    query_features = np.array(query_features, dtype=np.float32)\n","    query_labels = np.array(query_labels, dtype=np.float32)\n","    hard_indices = np.array(hard_indices, dtype=np.int32)\n","    if support_features.size == 0 or query_features.size == 0:\n","        logging.warning(\"Episode r·ªóng ƒë∆∞·ª£c t·∫°o ra. Tr·∫£ v·ªÅ episode r·ªóng.\")\n","        return np.array([]), np.array([]), np.array([]), np.array([]), np.array([])\n","    support_label_counts = np.bincount(np.argmax(support_labels, axis=1), minlength=5)\n","    query_label_counts = np.bincount(np.argmax(query_labels, axis=1), minlength=5)\n","    print(f\"Episode distribution - Support: {dict(zip(range(5), support_label_counts))}\")\n","    print(f\"Episode distribution - Query: {dict(zip(range(5), query_label_counts))}\")\n","    return support_features, support_labels, query_features, query_labels, hard_indices\n","\n","class CustomIsotonicRegression:\n","    def __init__(self):\n","        self.iso_reg = IsotonicRegression()\n","        self.X_min_ = None\n","        self.X_max_ = None\n","    def fit(self, X, y):\n","        self.X_min_ = np.min(X)\n","        self.X_max_ = np.max(X)\n","        self.iso_reg.fit(X, y)\n","        return self\n","    def predict(self, X):\n","        X_clipped = np.clip(X, self.X_min_, self.X_max_)\n","        return self.iso_reg.predict(X_clipped)\n","\n","import tensorflow as tf\n","import numpy as np\n","import logging\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","import pandas as pd\n","import gc\n","from sklearn.metrics import cohen_kappa_score, f1_score, precision_score, recall_score, confusion_matrix\n","from tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout\n","from tensorflow.keras.models import Model\n","\n","def maml_fomaml_train_manual(features, labels, valid_features, valid_labels, input_dim, n_episodes=20,\n","                             n_support=10, n_query=10, inner_lr=0.0005, outer_lr=0.0005, fine_tune_lr=0.0001,\n","                             use_fomaml=True, memory_size=20, sample_ids=None):\n","    \"\"\"\n","    Hu·∫•n luy·ªán meta-learner s·ª≠ d·ª•ng MAML/FO-MAML v·ªõi log ƒë·∫ßy ƒë·ªß ch·ªâ s·ªë tr√™n support set.\n","\n","    Args:\n","        features: ƒê·∫∑c tr∆∞ng t·∫≠p train (train_combined_features).\n","        labels: Nh√£n t·∫≠p train (train_y_multi, one-hot).\n","        valid_features: ƒê·∫∑c tr∆∞ng t·∫≠p valid.\n","        valid_labels: Nh√£n t·∫≠p valid (one-hot).\n","        input_dim: K√≠ch th∆∞·ªõc ƒë·∫ßu v√†o c·ªßa m√¥ h√¨nh.\n","        n_episodes: S·ªë episode hu·∫•n luy·ªán.\n","        n_support: S·ªë m·∫´u trong support set m·ªói l·ªõp.\n","        n_query: S·ªë m·∫´u trong query set m·ªói l·ªõp.\n","        inner_lr: Learning rate cho inner loop.\n","        outer_lr: Learning rate cho outer loop.\n","        fine_tune_lr: Learning rate cho fine-tuning.\n","        use_fomaml: S·ª≠ d·ª•ng FO-MAML thay v√¨ MAML.\n","        memory_size: K√≠ch th∆∞·ªõc b·ªô nh·ªõ cho MemoryAugmentedLayer.\n","        sample_ids: ID m·∫´u cho vi·ªác l∆∞u ƒë·∫∑c tr∆∞ng meta-learner.\n","\n","    Returns:\n","        meta_model: M√¥ h√¨nh meta-learner ƒë·∫ßy ƒë·ªß.\n","        meta_classification_model: M√¥ h√¨nh ph√¢n l·ªõp.\n","        history: L·ªãch s·ª≠ hu·∫•n luy·ªán v·ªõi c√°c ch·ªâ s·ªë.\n","    \"\"\"\n","    # ƒê·ªãnh nghƒ©a th∆∞ m·ª•c l∆∞u tr·ªØ\n","    feature_save_dir = \"/content/drive/MyDrive/working\"\n","    log_dir = os.path.join(feature_save_dir, \"logs\")\n","    os.makedirs(log_dir, exist_ok=True)\n","\n","    # ƒê·ªãnh nghƒ©a l·ªõp MemoryAugmentedLayer\n","    class MemoryAugmentedLayer(tf.keras.layers.Layer):\n","        def __init__(self, memory_size, memory_dim, **kwargs):\n","            super().__init__(**kwargs)\n","            self.memory_size = memory_size\n","            self.memory_dim = memory_dim\n","        def build(self, input_shape):\n","            self.memory = self.add_weight(\n","                shape=(self.memory_size, self.memory_dim),\n","                initializer='zeros',\n","                trainable=False,\n","                dtype=tf.float32\n","            )\n","            super().build(input_shape)\n","        def call(self, inputs):\n","            batch_size = tf.shape(inputs)[0]\n","            memory_size = tf.shape(self.memory)[0]\n","            memory_sliced = tf.cond(\n","                batch_size > memory_size,\n","                lambda: tf.tile(self.memory, [(batch_size + memory_size - 1) // memory_size, 1])[:batch_size],\n","                lambda: self.memory[:batch_size]\n","            )\n","            return tf.reduce_mean(tf.stack([inputs, memory_sliced], axis=0), axis=0)\n","        def get_config(self):\n","            config = super().get_config()\n","            config.update({'memory_size': self.memory_size, 'memory_dim': self.memory_dim})\n","            return config\n","\n","    # ƒê·ªãnh nghƒ©a l·ªõp GradientReversalLayer\n","    class GradientReversalLayer(tf.keras.layers.Layer):\n","        def __init__(self, lambda_=1.0, **kwargs):\n","            super().__init__(**kwargs)\n","            self.lambda_ = lambda_\n","        def call(self, inputs, training=None):\n","            inputs = tf.convert_to_tensor(inputs, dtype=tf.float32)\n","            return inputs if not training else tf.math.multiply(-self.lambda_, inputs)\n","        def get_config(self):\n","            config = super().get_config()\n","            config.update({\"lambda_\": self.lambda_})\n","            return config\n","\n","    # H√†m t·∫°o m√¥ h√¨nh\n","    def create_model(input_dim):\n","        inputs = Input(shape=(input_dim,))\n","        x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1), dtype=tf.float32)(inputs)\n","        x = BatchNormalization(dtype=tf.float32)(x)\n","        x = Dropout(0.6)(x)\n","        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1), dtype=tf.float32)(x)\n","        x = BatchNormalization(dtype=tf.float32)(x)\n","        x = Dropout(0.6)(x)\n","        x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1), dtype=tf.float32)(x)\n","        x = BatchNormalization(dtype=tf.float32)(x)\n","        feature_output = x\n","        x = MemoryAugmentedLayer(memory_size=memory_size, memory_dim=128)(x)\n","        classification_output = Dense(5, activation='softmax', name='classification', dtype=tf.float32)(x)\n","        domain_inputs = GradientReversalLayer(lambda_=1.0)(x)\n","        domain_x = Dense(64, activation='relu', dtype=tf.float32)(domain_inputs)\n","        domain_output = Dense(2, activation='softmax', name='domain', dtype=tf.float32)(domain_x)\n","        model = Model(inputs=inputs, outputs=[classification_output, domain_output])\n","        classification_model = Model(inputs=inputs, outputs=classification_output)\n","        feature_model = Model(inputs=inputs, outputs=feature_output)\n","        memory_layer = [layer for layer in model.layers if isinstance(layer, MemoryAugmentedLayer)][0]\n","        return model, classification_model, memory_layer, feature_model\n","\n","    # H√†m t√≠nh ƒë·ªô ch√≠nh x√°c\n","    def compute_accuracy(y_true, y_pred):\n","        y_pred = tf.argmax(y_pred, axis=1, output_type=tf.int32)\n","        y_true = tf.argmax(y_true, axis=1, output_type=tf.int32)\n","        return tf.reduce_mean(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n","\n","    # H√†m t√≠nh prototype\n","    def compute_prototypes(features, labels, feature_model):\n","        features = feature_model(features, training=False)\n","        labels_arg = tf.argmax(labels, axis=1)\n","        prototypes = []\n","        for cls in range(5):\n","            cls_mask = tf.equal(labels_arg, cls)\n","            cls_features = tf.boolean_mask(features, cls_mask)\n","            prototype = tf.reduce_mean(cls_features, axis=0) if tf.shape(cls_features)[0] > 0 else \\\n","                        tf.zeros([features.shape[1]], dtype=tf.float32)\n","            prototypes.append(prototype)\n","        return tf.stack(prototypes)\n","\n","    # H√†m t√≠nh prototypical loss\n","    def prototypical_loss(query_features, query_labels, prototypes):\n","        query_labels_arg = tf.argmax(query_labels, axis=1)\n","        query_features_exp = tf.expand_dims(query_features, axis=1)\n","        prototypes_exp = tf.expand_dims(prototypes, axis=0)\n","        distances = tf.reduce_sum(tf.square(query_features_exp - prototypes_exp), axis=-1)\n","        logits = tf.cast(-distances, tf.float32)\n","        return tf.cast(tf.keras.losses.categorical_crossentropy(query_labels, logits, from_logits=True), tf.float32)\n","\n","    # H√†m d·ª± ƒëo√°n prototypical\n","    def prototypical_predict(query_features, prototypes):\n","        query_features_exp = tf.expand_dims(query_features, axis=1)\n","        prototypes_exp = tf.expand_dims(prototypes, axis=0)\n","        distances = tf.reduce_sum(tf.square(query_features_exp - prototypes_exp), axis=-1)\n","        return tf.cast(tf.nn.softmax(tf.cast(-distances, tf.float32)), tf.float32)\n","\n","    # H√†m √°p d·ª•ng temperature scaling\n","    def apply_temperature_scaling(logits, temperature=2.0):\n","        logits = tf.convert_to_tensor(logits, dtype=tf.float32)\n","        return tf.nn.softmax(logits / temperature)\n","\n","    # H√†m Laplace smoothing\n","    def laplace_smoothing(probs, epsilon=1e-5):\n","        probs = tf.convert_to_tensor(probs, dtype=tf.float32)\n","        return (probs + epsilon) / (tf.reduce_sum(probs, axis=-1, keepdims=True) + 5 * epsilon)\n","\n","    # H√†m l∆∞u confusion matrix\n","    def save_confusion_matrix(y_true, y_pred, episode, qwk, save_dir, prefix=''):\n","        cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3, 4])\n","        plt.figure(figsize=(8, 6))\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                    xticklabels=[0, 1, 2, 3, 4], yticklabels=[0, 1, 2, 3, 4])\n","        plt.title(f'Ma tr·∫≠n nh·∫ßm l·∫´n - {prefix}QWK: {qwk:.4f} t·∫°i Episode {episode+1}')\n","        plt.xlabel('D·ª± ƒëo√°n')\n","        plt.ylabel('Th·ª±c t·∫ø')\n","        cm_path = os.path.join(save_dir, f'confusion_matrix_{prefix}episode_{episode+1}.png')\n","        plt.savefig(cm_path)\n","        plt.close()\n","        print(f\"ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n {prefix} t·∫°i: {cm_path}\")\n","        return cm\n","\n","    # H√†m gi·∫£m learning rate v√† early stopping\n","    def reduce_lr_and_early_stop(episode, qwk, best_qwk, patience_lr, patience_stop, lr_patience_counter, stop_patience_counter,\n","                                 inner_lr, outer_lr, fine_tune_lr, min_lr=1e-7, reduce_factor=0.5):\n","        stop_training = False\n","        if qwk > best_qwk:\n","            lr_patience_counter = 0\n","            stop_patience_counter = 0\n","        else:\n","            lr_patience_counter += 1\n","            stop_patience_counter += 1\n","        if lr_patience_counter >= patience_lr:\n","            inner_lr = max(inner_lr * reduce_factor, min_lr)\n","            outer_lr = max(outer_lr * reduce_factor, min_lr)\n","            fine_tune_lr = max(fine_tune_lr * reduce_factor, min_lr)\n","            lr_patience_counter = 0\n","            print(f\"Episode {episode+1}: Gi·∫£m learning rate - inner_lr={inner_lr:.6f}, outer_lr={outer_lr:.6f}, fine_tune_lr={fine_tune_lr:.6f}\")\n","        if stop_patience_counter >= patience_stop:\n","            stop_training = True\n","            print(f\"Episode {episode+1}: Early stopping do QWK kh√¥ng c·∫£i thi·ªán sau {patience_stop} episode\")\n","        return inner_lr, outer_lr, fine_tune_lr, lr_patience_counter, stop_patience_counter, stop_training\n","\n","    # Kh·ªüi t·∫°o m√¥ h√¨nh v√† optimizer\n","    meta_model, meta_classification_model, memory_layer, feature_model = create_model(input_dim)\n","    meta_optimizer = tf.keras.optimizers.Adam(learning_rate=outer_lr)\n","    fine_tune_optimizer = tf.keras.optimizers.Adam(learning_rate=fine_tune_lr)\n","    loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n","    domain_loss_fn = tf.keras.losses.CategoricalCrossentropy()\n","\n","    # Kh·ªüi t·∫°o c√°c bi·∫øn theo d√µi\n","    best_qwk = -float('inf')\n","    lr_patience_counter = 0\n","    stop_patience_counter = 0\n","    patience_lr = 10\n","    patience_stop = 50\n","    min_lr = 1e-7\n","    reduce_factor = 0.5\n","    weights_filepath = os.path.join(feature_save_dir, \"meta_model_maml_fomaml_best_weights.weights.h5\")\n","    history = {\n","        'qwk': [], 'loss': [], 'support_loss': [], 'support_accuracy': [], 'query_loss': [], 'query_accuracy': [],\n","        'precision': [], 'recall': [], 'support_qwk': [], 'support_precision': [], 'support_recall': [],\n","        'support_f1': [], 'support_cm': []\n","    }\n","    class_weights = np.ones(5, dtype=np.float32)\n","    class_weights[3] = 10 / (10 * 2)\n","    source_domain_labels = tf.keras.utils.to_categorical(tf.zeros(len(features), dtype=tf.int32), num_classes=2)\n","    source_domain_labels = tf.cast(source_domain_labels, tf.float32)\n","    target_domain_labels = tf.keras.utils.to_categorical(tf.ones(len(valid_features), dtype=tf.int32), num_classes=2)\n","    target_domain_labels = tf.cast(target_domain_labels, tf.float32)\n","\n","    # L∆∞u ƒë·∫∑c tr∆∞ng meta-learner n·∫øu c√≥ sample_ids\n","    if sample_ids is not None:\n","        print(\"L∆∞u ƒë·∫∑c tr∆∞ng 2D c·ªßa meta-learner...\")\n","        meta_features_2d = feature_model.predict(valid_features, batch_size=32, verbose=0)\n","        meta_features_2d_path = os.path.join(feature_save_dir, \"meta_learner_features_2d.npy\")\n","        np.save(meta_features_2d_path, meta_features_2d)\n","        print(f\"ƒê√£ l∆∞u ƒë·∫∑c tr∆∞ng 2D c·ªßa meta-learner t·∫°i: {meta_features_2d_path}, shape={meta_features_2d.shape}\")\n","        metadata = {\n","            \"model_name\": \"meta_learner\",\n","            \"features_2d_path\": meta_features_2d_path,\n","            \"sample_ids\": sample_ids.tolist(),\n","            \"labels\": valid_labels.tolist(),\n","            \"timestamp\": pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n","        }\n","        metadata_path = os.path.join(feature_save_dir, \"meta_learner_features_metadata.json\")\n","        with open(metadata_path, 'w') as f:\n","            json.dump(metadata, f, indent=4)\n","        print(f\"ƒê√£ l∆∞u metadata meta-learner t·∫°i: {metadata_path}\")\n","\n","    # V√≤ng l·∫∑p hu·∫•n luy·ªán\n","    for episode in range(n_episodes):\n","        # T·∫°o episode\n","        support_features, support_labels, query_features, query_labels, hard_indices = create_episode(\n","            features, labels, n_support, n_query, hard_sample_ratio=0.3, class_3_multiplier=2)\n","        if support_features.size == 0 or query_features.size == 0:\n","            logging.warning(f\"Episode {episode+1}: Kh√¥ng ƒë·ªß m·∫´u\")\n","            history['qwk'].append(0.0)\n","            history['support_loss'].append(0.0)\n","            history['support_accuracy'].append(0.0)\n","            history['query_loss'].append(0.0)\n","            history['query_accuracy'].append(0.0)\n","            history['precision'].append(0.0)\n","            history['recall'].append(0.0)\n","            history['support_qwk'].append(0.0)\n","            history['support_precision'].append(0.0)\n","            history['support_recall'].append(0.0)\n","            history['support_f1'].append(0.0)\n","            history['support_cm'].append(np.zeros((5, 5)).tolist())\n","            continue\n","\n","        # Kh·ªüi t·∫°o task model\n","        task_model, task_classification_model, task_memory_layer, task_feature_model = create_model(input_dim)\n","        task_model.set_weights(meta_model.get_weights())\n","        task_optimizer = tf.keras.optimizers.Adam(learning_rate=inner_lr)\n","        class_weight_dict = {i: float(w) for i, w in enumerate(class_weights)}\n","        print(f\"Episode {episode+1} - Tr·ªçng s·ªë l·ªõp: {class_weight_dict}\")\n","        support_prototypes = compute_prototypes(support_features, support_labels, task_feature_model)\n","\n","        # Inner loop training\n","        for _ in range(10):\n","            with tf.GradientTape() as tape:\n","                class_preds, domain_preds = task_model(support_features, training=True)\n","                min_size = min(class_preds.shape[0], support_labels.shape[0])\n","                if not min_size:\n","                    break\n","                class_preds = class_preds[:min_size]\n","                support_labels_adj = support_labels[:min_size]\n","                min_size_domain = min(domain_preds.shape[0], source_domain_labels.shape[0])\n","                domain_preds = domain_preds[:min_size_domain]\n","                source_domain_labels_slice = source_domain_labels[:min_size_domain]\n","                class_loss = tf.cast(loss_fn(support_labels_adj, class_preds, sample_weight=[\n","                    class_weight_dict.get(np.argmax(label), 1.0) for label in support_labels_adj]), tf.float32)\n","                domain_loss = tf.cast(domain_loss_fn(source_domain_labels_slice, domain_preds), tf.float32)\n","                support_features_task = task_feature_model(support_features, training=False)\n","                proto_loss = prototypical_loss(support_features_task, support_labels_adj, support_prototypes)\n","                total_loss = class_loss + 0.5 * domain_loss + 0.5 * proto_loss\n","            task_grads = tape.gradient(total_loss, task_model.trainable_variables)\n","            valid_grads = [(g, v) for g, v in zip(task_grads, task_model.trainable_variables) if g is not None]\n","            task_optimizer.apply_gradients(valid_grads)\n","            task_keys = task_feature_model(support_features, training=False)\n","            if task_keys.shape[1] != 128:\n","                task_keys = Dense(128, use_bias=False, dtype=tf.float32)(task_keys)\n","            task_keys = tf.concat([task_keys, tf.zeros([memory_size - task_keys.shape[0], task_keys.shape[1]], dtype=tf.float32)], axis=0) if task_keys.shape[0] < memory_size else task_keys[:memory_size]\n","            task_memory_layer.memory.assign(task_keys)\n","            del task_grads, valid_grads\n","            gc.collect()\n","\n","        # ƒê√°nh gi√° tr√™n support set\n","        support_preds = task_model(support_features, training=False)[0]\n","        support_loss_value = float(class_loss.numpy())\n","        support_accuracy = float(compute_accuracy(support_labels_adj, support_preds).numpy())\n","        support_preds_classes = np.argmax(support_preds.numpy(), axis=1)\n","        support_true_classes = np.argmax(support_labels_adj, axis=1)\n","        support_qwk = cohen_kappa_score(support_true_classes, support_preds_classes,\n","                                        labels=[0, 1, 2, 3, 4], weights='quadratic')\n","        support_precision = precision_score(support_true_classes, support_preds_classes,\n","                                           average='weighted', zero_division=0)\n","        support_recall = recall_score(support_true_classes, support_preds_classes,\n","                                     average='weighted', zero_division=0)\n","        support_f1 = f1_score(support_true_classes, support_preds_classes,\n","                             average='weighted', zero_division=0)\n","        support_cm = confusion_matrix(support_true_classes, support_preds_classes,\n","                                     labels=[0, 1, 2, 3, 4])\n","        history['support_qwk'].append(float(support_qwk))\n","        history['support_precision'].append(float(support_precision))\n","        history['support_recall'].append(float(support_recall))\n","        history['support_f1'].append(float(support_f1))\n","        history['support_cm'].append(support_cm.tolist())\n","        support_cm = save_confusion_matrix(support_true_classes, support_preds_classes, episode,\n","                                          support_qwk, feature_save_dir, prefix='support_')\n","\n","        # ƒê√°nh gi√° tr√™n query set\n","        with tf.GradientTape() as outer_tape:\n","            query_preds, domain_preds = task_model(query_features, training=True)\n","            min_size = min(query_preds.shape[0], query_labels.shape[0])\n","            if not min_size:\n","                logging.warning(f\"Episode {episode+1}: Kh√¥ng c√≥ d·ªØ li·ªáu query h·ª£p l·ªá\")\n","                history['qwk'].append(0.0)\n","                history['support_loss'].append(support_loss_value)\n","                history['support_accuracy'].append(support_accuracy)\n","                history['query_loss'].append(0.0)\n","                history['query_accuracy'].append(0.0)\n","                history['precision'].append(0.0)\n","                history['recall'].append(0.0)\n","                continue\n","            query_preds = query_preds[:min_size]\n","            query_labels_adj = query_labels[:min_size]\n","            min_size_domain = min(domain_preds.shape[0], source_domain_labels.shape[0])\n","            domain_preds = domain_preds[:min_size_domain]\n","            source_domain_labels_slice = source_domain_labels[:min_size_domain]\n","            query_loss = tf.cast(loss_fn(query_labels_adj, query_preds, sample_weight=[\n","                class_weight_dict.get(np.argmax(label), 1.0) for label in query_labels_adj]), tf.float32)\n","            domain_loss = tf.cast(domain_loss_fn(source_domain_labels_slice, domain_preds), tf.float32)\n","            query_features_task = task_feature_model(query_features, training=False)\n","            proto_loss = prototypical_loss(query_features_task, query_labels_adj, support_prototypes)\n","            total_query_loss = query_loss + 0.5 * domain_loss + 0.5 * proto_loss\n","            query_accuracy = float(compute_accuracy(query_labels_adj, query_preds).numpy())\n","            query_loss_value = float(query_loss.numpy())\n","\n","        # C·∫≠p nh·∫≠t meta-model\n","        meta_grads = outer_tape.gradient(total_query_loss, task_model.trainable_variables)\n","        valid_grads = [(g, v) for g, v in zip(meta_grads, meta_model.trainable_variables) if g is not None]\n","        meta_optimizer.apply_gradients(valid_grads)\n","        memory_keys = feature_model(support_features, training=False)\n","        if memory_keys.shape[1] != 128:\n","            memory_keys = Dense(128, use_bias=False, dtype=tf.float32)(memory_keys)\n","        memory_keys = tf.concat([memory_keys, tf.zeros([memory_size - memory_keys.shape[0], memory_keys.shape[1]], dtype=tf.float32)], axis=0) if memory_keys.shape[0] < memory_size else memory_keys[:memory_size]\n","        memory_layer.memory.assign(memory_keys)\n","        del meta_grads, valid_grads\n","        gc.collect()\n","\n","        # Fine-tuning tr√™n query set\n","        for _ in range(5):\n","            with tf.GradientTape() as fine_tune_tape:\n","                fine_tune_preds = meta_classification_model(query_features, training=True)\n","                min_size = min(fine_tune_preds.shape[0], query_labels.shape[0])\n","                if not min_size:\n","                    continue\n","                fine_tune_preds = fine_tune_preds[:min_size]\n","                query_labels_adj = query_labels[:min_size]\n","                fine_tune_loss = tf.cast(loss_fn(query_labels_adj, fine_tune_preds, sample_weight=[\n","                    class_weight_dict.get(np.argmax(label), 1.0) for label in query_labels_adj]), tf.float32)\n","            fine_tune_grads = fine_tune_tape.gradient(fine_tune_loss, meta_classification_model.trainable_variables)\n","            valid_grads = [(g, v) for g, v in zip(fine_tune_grads, meta_classification_model.trainable_variables) if g is not None]\n","            fine_tune_optimizer.apply_gradients(valid_grads)\n","            del fine_tune_grads, valid_grads\n","            gc.collect()\n","\n","        # ƒê√°nh gi√° tr√™n valid set\n","        valid_preds_maml = tf.cast(meta_classification_model(valid_features, training=False), tf.float32)\n","        min_size = min(valid_preds_maml.shape[0], valid_labels.shape[0])\n","        if not min_size:\n","            logging.warning(f\"Episode {episode+1}: Kh√¥ng c√≥ d·ªØ li·ªáu valid h·ª£p l·ªá\")\n","            history['qwk'].append(0.0)\n","            history['support_loss'].append(support_loss_value)\n","            history['support_accuracy'].append(support_accuracy)\n","            history['query_loss'].append(query_loss_value)\n","            history['query_accuracy'].append(query_accuracy)\n","            history['precision'].append(0.0)\n","            history['recall'].append(0.0)\n","            continue\n","        valid_preds_maml = valid_preds_maml[:min_size]\n","        valid_labels_adj = valid_labels[:min_size]\n","        valid_features_task = task_feature_model(valid_features, training=False)\n","        valid_prototypes = compute_prototypes(features, labels, task_feature_model)\n","        valid_preds_proto = tf.cast(prototypical_predict(valid_features_task, valid_prototypes), tf.float32)\n","        valid_preds_ensemble = (0.5 * valid_preds_maml + 0.5 * valid_preds_proto)\n","        valid_preds_scaled = tf.cast(apply_temperature_scaling(valid_preds_ensemble, temperature=2.0), tf.float32)\n","        valid_preds_scaled = laplace_smoothing(valid_preds_scaled, epsilon=1e-5)\n","        valid_preds_classes = np.argmax(valid_preds_scaled.numpy(), axis=1)\n","        qwk = cohen_kappa_score(np.argmax(valid_labels_adj, axis=1), valid_preds_classes,\n","                                labels=[0, 1, 2, 3, 4], weights='quadratic')\n","        precision = precision_score(np.argmax(valid_labels_adj, axis=1), valid_preds_classes,\n","                                   average='weighted', zero_division=0)\n","        recall = recall_score(np.argmax(valid_labels_adj, axis=1), valid_preds_classes,\n","                             average='weighted', zero_division=0)\n","        history['qwk'].append(float(qwk))\n","        history['precision'].append(float(precision))\n","        history['recall'].append(float(recall))\n","        history['support_loss'].append(float(support_loss_value))\n","        history['support_accuracy'].append(float(support_accuracy))\n","        history['query_loss'].append(float(query_loss_value))\n","        history['query_accuracy'].append(float(query_accuracy))\n","\n","        # Gi·∫£m learning rate v√† early stopping\n","        inner_lr, outer_lr, fine_tune_lr, lr_patience_counter, stop_patience_counter, stop_training = \\\n","            reduce_lr_and_early_stop(\n","                episode, qwk, best_qwk, patience_lr, patience_stop, lr_patience_counter, stop_patience_counter,\n","                inner_lr, outer_lr, fine_tune_lr, min_lr, reduce_factor\n","            )\n","        meta_optimizer.learning_rate.assign(outer_lr)\n","        fine_tune_optimizer.learning_rate.assign(fine_tune_lr)\n","\n","        # Log v√†o TensorBoard\n","        with tf.summary.create_file_writer(log_dir).as_default():\n","            tf.summary.scalar('support_loss', support_loss_value, step=episode)\n","            tf.summary.scalar('support_accuracy', support_accuracy, step=episode)\n","            tf.summary.scalar('support_qwk', support_qwk, step=episode)\n","            tf.summary.scalar('support_precision', support_precision, step=episode)\n","            tf.summary.scalar('support_recall', support_recall, step=episode)\n","            tf.summary.scalar('support_f1', support_f1, step=episode)\n","            tf.summary.scalar('query_loss', query_loss_value, step=episode)\n","            tf.summary.scalar('query_accuracy', query_accuracy, step=episode)\n","            tf.summary.scalar('qwk', qwk, step=episode)\n","            tf.summary.scalar('precision', precision, step=episode)\n","            tf.summary.scalar('recall', recall, step=episode)\n","            tf.summary.scalar('inner_lr', inner_lr, step=episode)\n","            tf.summary.scalar('outer_lr', outer_lr, step=episode)\n","            tf.summary.scalar('fine_tune_lr', fine_tune_lr, step=episode)\n","\n","        # In k·∫øt qu·∫£\n","        print(f\"Episode {episode+1}/{n_episodes}:\")\n","        print(f\"  Support Loss: {support_loss_value:.4f}, Accuracy: {support_accuracy:.4f}\")\n","        print(f\"  Support QWK: {support_qwk:.4f}, F1: {support_f1:.4f}, Precision: {support_precision:.4f}, Recall: {support_recall:.4f}\")\n","        print(f\"  Support Confusion Matrix:\\n{support_cm}\")\n","        print(f\"  Query Loss: {query_loss_value:.4f}, Accuracy: {query_accuracy:.4f}\")\n","        print(f\"  Valid QWK (Ensemble): {qwk:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n","        print(f\"  Inner LR: {inner_lr:.6f}, Outer LR: {outer_lr:.6f}, Fine-tune LR: {fine_tune_lr:.6f}\")\n","\n","        # L∆∞u tr·ªçng s·ªë n·∫øu QWK t·ªët h∆°n\n","        if qwk > best_qwk:\n","            best_qwk = qwk\n","            try:\n","                meta_model.save_weights(weights_filepath, overwrite=True)\n","                print(f\"ƒê√£ l∆∞u tr·ªçng s·ªë t·ªët nh·∫•t t·∫°i episode {episode+1} v·ªõi QWK: {best_qwk:.4f}\")\n","                cm = save_confusion_matrix(\n","                    np.argmax(valid_labels_adj, axis=1), valid_preds_classes, episode,\n","                    best_qwk, feature_save_dir, prefix='best_'\n","                )\n","                print(f\"Ma tr·∫≠n nh·∫ßm l·∫´n cho QWK t·ªët nh·∫•t t·∫°i Episode {episode+1}:\\n{cm}\")\n","            except Exception as e:\n","                logging.error(f\"L·ªói khi l∆∞u tr·ªçng s·ªë: {str(e)}. Th·ª≠ l∆∞u .h5\")\n","                alt_weights_filepath = os.path.join(feature_save_dir, \"meta_model_maml_fomaml_best_weights.h5\")\n","                meta_model.save_weights(alt_weights_filepath, overwrite=True)\n","                print(f\"ƒê√£ l∆∞u tr·ªçng s·ªë (d·∫°ng thay th·∫ø) t·∫°i: {alt_weights_filepath}\")\n","\n","        if stop_training:\n","            print(f\"Early stopping k√≠ch ho·∫°t t·∫°i episode {episode+1}\")\n","            break\n","        gc.collect()\n","\n","    # Fine-tuning tr√™n t·∫≠p valid\n","    print(\"Fine-tuning tr√™n t·∫≠p valid...\")\n","    meta_classification_model.compile(\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tune_lr),\n","        loss='categorical_crossentropy',\n","        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n","    )\n","    meta_classification_model.fit(\n","        valid_features, valid_labels,\n","        validation_data=(valid_features, valid_labels),\n","        epochs=100,\n","        batch_size=32,\n","        verbose=1,\n","        callbacks=[\n","            tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1),\n","            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3),\n","            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","        ]\n","    )\n","\n","    # ƒê√°nh gi√° cu·ªëi c√πng tr√™n valid set\n","    valid_preds = meta_classification_model.predict(valid_features, batch_size=32)\n","    valid_preds = apply_temperature_scaling(valid_preds, temperature=2.0)\n","    valid_preds = laplace_smoothing(valid_preds, epsilon=1e-5)\n","    valid_preds_classes = np.argmax(valid_preds, axis=1)\n","    valid_true_classes = np.argmax(valid_labels, axis=1)\n","    qwk_final = cohen_kappa_score(valid_true_classes, valid_preds_classes,\n","                                  labels=[0, 1, 2, 3, 4], weights='quadratic')\n","    f1_final = f1_score(valid_true_classes, valid_preds_classes, average='weighted')\n","    recall_final = recall_score(valid_true_classes, valid_preds_classes, average='weighted')\n","    precision_final = precision_score(valid_true_classes, valid_preds_classes, average='weighted')\n","    print(f\"K·∫øt qu·∫£ cu·ªëi c√πng tr√™n t·∫≠p valid:\")\n","    print(f\"  Quadratic Weighted Kappa (QWK): {qwk_final:.4f}\")\n","    print(f\"  Weighted F1 Score: {f1_final:.4f}\")\n","    print(f\"  Weighted Recall: {recall_final:.4f}\")\n","    print(f\"  Weighted Precision: {precision_final:.4f}\")\n","    cm_final = save_confusion_matrix(\n","        valid_true_classes, valid_preds_classes, n_episodes, qwk_final, feature_save_dir, prefix='final_'\n","    )\n","    print(f\"Ma tr·∫≠n nh·∫ßm l·∫´n cu·ªëi c√πng:\\n{cm_final}\")\n","\n","    # L∆∞u tr·ªØ s·ªë\n","    final_weights_filepath = os.path.join(feature_save_dir, \"model.weights.h5\")\n","    final_config_filepath = os.path.join(feature_save_dir, \"config.json\")\n","    final_metadata_filepath = os.path.join(feature_save_dir, \"metadata.json\")\n","    try:\n","        meta_model.save_weights(final_weights_filepath, overwrite=True)\n","        print(f\"ƒê√£ l∆∞u tr·ªçng s·ªë meta-model t·∫°i: {final_weights_filepath}\")\n","    except Exception as e:\n","        logging.error(\"fL·ªói khi l∆∞u tr·ªçng s·ªë meta-model: {str(e)}\")\n","        alt_final_weights_filepath = os.path.join(feature_save_dir, \"f{model_weights_alt.h5}\")\n","        meta_model.save_weights(alt_final_weights_filepath, overwrite=True)\n","        print(f\"ƒê√£ l∆∞u tr·ªçng s·ªë (d·∫°ng thay th·∫ø) t·∫°i: {alt_final_weights_filepath}\")\n","\n","    try:\n","        model_config = meta_model.to_json()\n","        with open(final_config_filepath, 'w') as f:\n","            json.dump(json.loads(model_config), f, indent=2)\n","        print(f\"ƒê√£ l∆∞u c·∫•u h√¨nh meta-model t·∫°i: {final_config_filepath}\")\n","    except Exception as e:\n","        logging.error(f\"L·ªói khi l∆∞u config meta-model: {str(e)}\")\n","\n","    metadata = {\n","        \"model_type\": \"meta_model_maml_fomaml\",\n","        \"num_classes\": 5,\n","        \"input_dim\": input_dim,\n","        \"training_episodes\": n_episodes,\n","        \"timestamp\": pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n","    }\n","    try:\n","        with open(final_metadata_filepath, 'w') as f:\n","            json.dump(metadata, f, indent=4)\n","        print(f\"ƒê√£ l∆∞u si√™u d·ªØ li·ªáu meta-model t·∫°i: {final_metadata_filepath}\")\n","    except Exception as e:\n","        logging.error(f\"L·ªói khi l∆∞u metadata: {str(e)}\")\n","\n","    metrics_history = {\n","        'qwk': float(qwk_final),\n","        'f1_score': float(f1_final),\n","        'recall': float(recall_final),\n","        'precision': float(precision_final),\n","        'training_history': {\n","            'qwk': [float(q) for q in history['qwk']],\n","            'support_loss': [float(l) for l in history['support_loss']],\n","            'support_accuracy': [float(a) for a in history['support_accuracy']],\n","            'query_loss': [float(l) for l in history['query_loss']],\n","            'query_accuracy': [float(a) for a in history['query_accuracy']],\n","            'precision': [float(p) for p in history['precision']],\n","            'recall': [float(r) for r in history['recall']],\n","            'support_qwk': [float(q) for q in history['support_qwk']],\n","            'support_precision': [float(p) for p in history['support_precision']],\n","            'support_recall': [float(r) for r in history['support_recall']],\n","            'support_f1': [float(f) for f in history['support_f1']],\n","            'support_cm': history['support_cm']\n","        }\n","    }\n","\n","    metrics_filepath = os.path.join(feature_save_dir, 'final_metrics.json')\n","    try:\n","        with open(metrics_filepath, 'w') as f:\n","            json.dump(metrics_history, f, indent=4)\n","        print(f\"ƒê√£ l∆∞u c√°c ch·ªâ s·ªë ƒë√°nh gi√° t·∫°i: {metrics_filepath}\")\n","    except Exception as e:\n","        logging.error(f\"L·ªói khi l∆∞u metrics: {str(e)}\")\n","\n","    # V·∫Ω bi·ªÉu ƒë·ªì l·ªãch s·ª≠ hu·∫•n luy·ªán\n","    plt.figure(figsize=(15, 10))\n","    plt.subplot(2, 3, 1)\n","    plt.plot(history['qwk'], label='Valid QWK')\n","    plt.title('QWK theo Episode')\n","    plt.xlabel('Episode')\n","    plt.ylabel('QWK')\n","    plt.legend()\n","    plt.subplot(2, 3, 2)\n","    plt.plot(history['support_loss'], label='Support Loss')\n","    plt.plot(history['query_loss'], label='Query Loss')\n","    plt.title('Loss theo Episode')\n","    plt.xlabel('Episode')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.subplot(2, 3, 3)\n","    plt.plot(history['support_accuracy'], label='Support Accuracy')\n","    plt.plot(history['query_accuracy'], label='Query Accuracy')\n","    plt.title('Accuracy theo Episode')\n","    plt.xlabel('Episode')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.subplot(2, 3, 4)\n","    plt.plot(history['support_qwk'], label='Support QWK')\n","    plt.title('Support QWK theo Episode')\n","    plt.xlabel('Episode')\n","    plt.ylabel('QWK')\n","    plt.legend()\n","    plt.subplot(2, 3, 5)\n","    plt.plot(history['support_precision'], label='Support Precision')\n","    plt.plot(history['support_recall'], label='Support Recall')\n","    plt.plot(history['support_f1'], label='Support F1')\n","    plt.title('Support Metrics theo Episode')\n","    plt.xlabel('Episode')\n","    plt.ylabel('Score')\n","    plt.legend()\n","    plt.tight_layout()\n","    history_path = os.path.join(feature_save_dir, 'training_history.png')\n","    plt.savefig(history_path)\n","    plt.close()\n","    print(f\"ƒê√£ l∆∞u bi·ªÉu ƒë·ªì l·ªãch s·ª≠ hu·∫•n luy·ªán t·∫°i: {history_path}\")\n","\n","    return meta_model, meta_classification_model, history\n","\n","def evaluate_test_set(meta_classification_model, test_features, test_labels, save_dir):\n","    test_preds = meta_classification_model.predict(test_features, batch_size=32)\n","    test_preds = apply_temperature_scaling(test_preds, temperature=2.0)\n","    test_preds = laplace_smoothing(test_preds, epsilon=1e-5)\n","    test_probs = np.max(test_preds.numpy(), axis=1)\n","    test_probs = np.clip(test_probs, 0.0, 1.0)\n","    test_preds_classes = np.argmax(test_preds, axis=1)\n","    test_true_classes = np.argmax(test_labels, axis=1)\n","    qwk_test = cohen_kappa_score(test_true_classes, test_preds_classes, labels=[0, 1, 2, 3, 4], weights='quadratic')\n","    f1_test = f1_score(test_true_classes, test_preds_classes, average='weighted')\n","    precision_test = precision_score(test_true_classes, test_preds_classes, average='weighted')\n","    recall_test = recall_score(test_true_classes, test_preds_classes, average='weighted')\n","    accuracy_test = accuracy_score(test_true_classes, test_preds_classes)\n","    print(f\"K·∫øt qu·∫£ tr√™n t·∫≠p test:\")\n","    print(f\"  Quadratic Weighted Kappa (QWK): {qwk_test:.4f}\")\n","    print(f\"  Weighted F1 Score: {f1_test:.4f}\")\n","    print(f\"  Weighted Precision: {precision_test:.4f}\")\n","    print(f\"  Weighted Recall: {recall_test:.4f}\")\n","    print(f\"  Accuracy: {accuracy_test:.4f}\")\n","    cm_test = confusion_matrix(test_true_classes, test_preds_classes, labels=[0, 1, 2, 3, 4])\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=[0, 1, 2, 3, 4], yticklabels=[0, 1, 2, 3, 4])\n","    plt.title(f'Ma tr·∫≠n nh·∫ßm l·∫´n - Test Set, QWK: {qwk_test:.4f}')\n","    plt.xlabel('D·ª± ƒëo√°n')\n","    plt.ylabel('Th·ª±c t·∫ø')\n","    cm_path = os.path.join(save_dir, 'confusion_matrix_test.png')\n","    plt.savefig(cm_path)\n","    plt.close()\n","    print(f\"ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n t·∫≠p test t·∫°i: {cm_path}\")\n","    print(f\"Ma tr·∫≠n nh·∫ßm l·∫´n t·∫≠p test:\\n{cm_test}\")\n","    test_metrics = {\n","        'qwk': float(qwk_test),\n","        'f1_score': float(f1_test),\n","        'precision': float(precision_test),\n","        'recall': float(recall_test),\n","        'accuracy': float(accuracy_test),\n","        'confusion_matrix': cm_test.tolist()\n","    }\n","    metrics_filepath = os.path.join(save_dir, 'test_metrics.json')\n","    try:\n","        with open(metrics_filepath, 'w') as f:\n","            json.dump(test_metrics, f, indent=4)\n","        print(f\"ƒê√£ l∆∞u c√°c ch·ªâ s·ªë ƒë√°nh gi√° t·∫≠p test t·∫°i: {metrics_filepath}\")\n","    except Exception as e:\n","        logging.error(f\"L·ªói khi l∆∞u test metrics: {str(e)}\")\n","    return test_metrics\n","\n","def apply_temperature_scaling(logits, temperature=2.0):\n","    logits = tf.convert_to_tensor(logits, dtype=tf.float32)\n","    return tf.nn.softmax(logits / temperature)\n","\n","def laplace_smoothing(probs, epsilon=1e-5):\n","    probs = tf.convert_to_tensor(probs, dtype=tf.float32)\n","    return (probs + epsilon) / (tf.reduce_sum(probs, axis=-1, keepdims=True) + NUM_CLASSES * epsilon)\n","\n","def save_confusion_matrix(y_true, y_pred, episode, qwk, save_dir, prefix=''):\n","    cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3, 4])\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=[0, 1, 2, 3, 4], yticklabels=[0, 1, 2, 3, 4])\n","    plt.title(f'Ma tr·∫≠n nh·∫ßm l·∫´n - {prefix}QWK: {qwk:.4f} t·∫°i Episode {episode+1}')\n","    plt.xlabel('D·ª± ƒëo√°n')\n","    plt.ylabel('Th·ª±c t·∫ø')\n","    cm_path = os.path.join(save_dir, f'confusion_matrix_{prefix}qwk_episode_{episode+1}.png')\n","    plt.savefig(cm_path)\n","    plt.close()\n","    print(f\"ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n {prefix}QWK t·∫°i: {cm_path}\")\n","    return cm\n","\n","def compute_prototypes(features, labels, feature_model):\n","    features = feature_model(features, training=False)\n","    labels_arg = tf.argmax(labels, axis=1)\n","    prototypes = []\n","    for cls in range(NUM_CLASSES):\n","        cls_mask = tf.equal(labels_arg, cls)\n","        cls_features = tf.boolean_mask(features, cls_mask)\n","        prototype = tf.reduce_mean(cls_features, axis=0) if tf.shape(cls_features)[0] > 0 else \\\n","                    tf.zeros([features.shape[1]], dtype=tf.float32)\n","        prototypes.append(prototype)\n","    return tf.stack(prototypes)\n","\n","def prototypical_loss(query_features, query_labels, prototypes):\n","    query_labels_arg = tf.argmax(query_labels, axis=1)\n","    query_features_exp = tf.expand_dims(query_features, axis=1)\n","    prototypes_exp = tf.expand_dims(prototypes, axis=0)\n","    distances = tf.reduce_sum(tf.square(query_features_exp - prototypes_exp), axis=-1)\n","    logits = tf.cast(-distances, tf.float32)\n","    return tf.cast(tf.keras.losses.categorical_crossentropy(query_labels, logits, from_logits=True), tf.float32)\n","\n","def prototypical_predict(query_features, prototypes):\n","    query_features_exp = tf.expand_dims(query_features, axis=1)\n","    prototypes_exp = tf.expand_dims(prototypes, axis=0)\n","    distances = tf.reduce_sum(tf.square(query_features_exp - prototypes_exp), axis=-1)\n","    return tf.cast(tf.nn.softmax(tf.cast(-distances, tf.float32)), tf.float32)\n","\n","def reduce_lr_and_early_stop(episode, qwk, best_qwk, patience_lr, patience_stop, lr_patience_counter, stop_patience_counter,\n","                             inner_lr, outer_lr, fine_tune_lr, min_lr=1e-7, reduce_factor=0.5):\n","    stop_training = False\n","    if qwk > best_qwk:\n","        lr_patience_counter = 0\n","        stop_patience_counter = 0\n","    else:\n","        lr_patience_counter += 1\n","        stop_patience_counter += 1\n","    if lr_patience_counter >= patience_lr:\n","        inner_lr = max(inner_lr * reduce_factor, min_lr)\n","        outer_lr = max(outer_lr * reduce_factor, min_lr)\n","        fine_tune_lr = max(fine_tune_lr * reduce_factor, min_lr)\n","        lr_patience_counter = 0\n","        print(f\"Episode {episode+1}: Gi·∫£m learning rate - inner_lr={inner_lr:.6f}, outer_lr={outer_lr:.6f}, fine_tune_lr={fine_tune_lr:.6f}\")\n","    if stop_patience_counter >= patience_stop:\n","        stop_training = True\n","        print(f\"Episode {episode+1}: Early stopping do QWK kh√¥ng c·∫£i thi·ªán sau {patience_stop} episode\")\n","    return inner_lr, outer_lr, fine_tune_lr, lr_patience_counter, stop_patience_counter, stop_training\n","\n","# Tr√≠ch xu·∫•t v√† l∆∞u ƒë·∫∑c tr∆∞ng\n","train_features_dict = {}\n","valid_features_dict = {}\n","test_features_dict = {}\n","for model_name, config in model_configs.items():\n","    print(f\"X·ª≠ l√Ω m√¥ h√¨nh: {model_name}\")\n","    base_model = load_model_from_config(\n","        config['config_path'], config['weights_path'], config['base_model']\n","    )\n","    feature_layer = base_model.layers[-2].output if len(base_model.layers) > 1 else base_model.output\n","    feature_extractor = Model(inputs=base_model.input, outputs=feature_layer)\n","    feature_extractor.trainable = False\n","    train_generator = My_Generator(\n","        balanced_train_x, balanced_train_y_multi, batch_size=32, is_train=True,\n","        mix=True, augment=True, size1=SIZE, size2=config['img_size'],\n","        model_type=config['model_type'], preprocess=config['preprocess']\n","    )\n","    valid_generator = My_Generator(\n","        resized_valid_x, valid_y_multi, batch_size=32, is_train=False,\n","        mix=False, augment=False, size1=SIZE, size2=config['img_size'],\n","        model_type=config['model_type'], preprocess=config['preprocess']\n","    )\n","    test_generator = My_Generator(\n","        resized_test_x, test_y_multi, batch_size=32, is_train=False,\n","        mix=False, augment=False, size1=SIZE, size2=config['img_size'],\n","        model_type=config['model_type'], preprocess=config['preprocess']\n","    )\n","    train_features_2d = extract_and_save_features(\n","        model_name, feature_extractor, train_generator, feature_save_dir, sample_ids=train_x.values\n","    )\n","    valid_features_2d = extract_and_save_features(\n","        model_name, feature_extractor, valid_generator, feature_save_dir, sample_ids=valid_x.values\n","    )\n","    test_features_2d = extract_and_save_features(\n","        model_name, feature_extractor, test_generator, feature_save_dir, sample_ids=test_x.values\n","    )\n","    train_features_dict[model_name] = train_features_2d\n","    valid_features_dict[model_name] = valid_features_2d\n","    test_features_dict[model_name] = test_features_2d\n","    del base_model, feature_extractor\n","    tf.keras.backend.clear_session()\n","    gc.collect()\n","\n","# K·∫øt h·ª£p v√† gi·∫£m chi·ªÅu ƒë·∫∑c tr∆∞ng\n","train_combined_features, train_pca, train_valid_indices, input_dim = combine_and_reduce_features(\n","    train_features_dict, balanced_train_y_multi, train_x.values, feature_save_dir, n_components=50\n",")\n","valid_combined_features, _, valid_valid_indices, _ = combine_and_reduce_features(\n","    valid_features_dict, valid_y_multi, valid_x.values, feature_save_dir, n_components=50\n",")\n","test_combined_features, _, test_valid_indices, _ = combine_and_reduce_features(\n","    test_features_dict, test_y_multi, test_x.values, feature_save_dir, n_components=50\n",")\n","train_y_multi = balanced_train_y_multi[train_valid_indices]\n","valid_y_multi = valid_y_multi[valid_valid_indices]\n","test_y_multi = test_y_multi[test_valid_indices]\n","\n","# Hu·∫•n luy·ªán meta-model\n","print(\"Hu·∫•n luy·ªán meta-model v·ªõi MAML/FO-MAML...\")\n","meta_model, meta_classification_model, history = maml_fomaml_train_manual(\n","    train_combined_features, train_y_multi, valid_combined_features, valid_y_multi,\n","    input_dim=input_dim, n_episodes=20, n_support=10, n_query=10, inner_lr=0.001,\n","    outer_lr=0.001, fine_tune_lr=0.0001, use_fomaml=True, memory_size=20,\n","    sample_ids=valid_x.values[valid_valid_indices]\n",")\n","\n","# ƒê√°nh gi√° tr√™n t·∫≠p test\n","print(\"ƒê√°nh gi√° tr√™n t·∫≠p test...\")\n","test_metrics = evaluate_test_set(\n","    meta_classification_model, test_combined_features, test_y_multi, feature_save_dir\n",")\n","\n","print(\"Ho√†n th√†nh quy tr√¨nh hu·∫•n luy·ªán v√† ƒë√°nh gi√°!\")"]},{"cell_type":"code","source":["print(processed_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhgrsaOPtjR5","executionInfo":{"status":"ok","timestamp":1749053514219,"user_tz":-420,"elapsed":224,"user":{"displayName":"Resconnect","userId":"00882996269091038835"}},"outputId":"504cd840-06b8-4623-fc84-0ccdd1ff5696"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['cb2f3c5d71a7.png', '5c6194562ed2.png', 'e893e86dde94.png', '58059e73d2d4.png', '8ee50c26fc13.png', '6cee2e148520.png', 'd035c2bd9104.png', '22098b1fe461.png', '0a1076183736.png', 'b576c5269ad1.png', 'f4d3777f2710.png', 'd881c04f01fe.png', '14c3b41d289c.png', 'cb68fce07789.png', 'fcc55ae641ae.png', '64c6c6ee0d98.png', '4cae247d9909.png', '6b7cf869622a.png', 'e7291472109b.png', '90bde2ff8953.png', 'c0a117de7d0a.png', '7c2e852171c0.png', '42af7282349b.png', '674057ab250c.png', '8688f3d0fcaf.png', 'd6f6bdfd8011.png', '7adfb8fc0621.png', 'cc3d2e961768.png', '8958a4d17b7e.png', '26999ebc21de.png', '07a1c7073982.png', 'a8dea22ef903.png', '41345cec5957.png', '35beb47fe159.png', '79ade634c633.png', '7e4019ac7f5a.png', '79d44db3da2d.png', 'a4d41c495666.png', '415d5c5e785f.png', '6c6505a0c637.png', '0fb1053285cf.png', '838b3e4d0bb4.png', '13d411c85ffd.png', '8e76054f0831.png', '32d7d360d891.png', '6f460f9968c7.png', 'd801c0a66738.png', '38e0e28d35d3.png', '51269b77d312.png', '2927665214e1.png', 'd1b279cc02ae.png', '9b7b6e4db1d5.png', '7a46cfa69bae.png', 'f7defe70afc3.png', 'd911dd40c63b.png', '8bdb891661a8.png', 'dc3c0d8ee20b.png', '54f57cf26126.png', 'f3b27ac2d371.png', 'e540d2e35d15.png', 'fa3e544a7401.png', '62318d514160.png', '5a091e8cd95c.png', '855f0a5442b6.png', 'da6389d129aa.png', 'f7edc074f06b.png', 'fca931da5c5e.png', '12ef75375322.png', '8b26d3cd61e8.png', '6180920bc224.png', '61bbe8db6f3a.png', '1ab8d3431ffc.png', 'd29096bd94aa.png', '89b044cbaf85.png', 'f092febbf5c0.png', '0a38b552372d.png', '501c319f7a9f.png', '9faad91b6578.png', 'd0ffa0425ef1.png', 'c85b79d70079.png', 'b3f31c371e59.png', '67c03349bb31.png', '1b329a127307.png', '1a03a7970337.png', '527bea76116c.png', '5b804948e35f.png', 'ea4dcb055139.png', 'b92eacd1392a.png', '306c841af3fc.png', 'f7e9fa75c7c1.png', '1d0b93317aa8.png', 'f233638e0e90.png', 'ed246ae1ed08.png', '4b5ffea77373.png', '054b1b305160.png', '3b018e8b7303.png', '3ee4841936ef.png', '436e7a7af761.png', '4d7d6928534a.png', 'e3a7671f787b.png', '757572337fd0.png', '13ab8db8c700.png', '8d8aca52c07b.png', '878e356c8fc9.png', '17188c13e635.png', 'ad20080452de.png', 'ea15a290eb96.png', '9ac2e3e9fca5.png', '7d3835e4e63a.png', 'f85fd4fac887.png', '8e67f2d7e0ee.png', 'a12ca80bb8c7.png', 'fe2df69676cf.png', '4b422b48d0d4.png', '6ccfdb031184.png', '9859e2a6cc24.png', '6cbc3dad809c.png', '8241e43408a8.png', '9e7a63b2fc6a.png', '788ddb0b70b7.png', '6b91e99c9408.png', '0a74c92e287c.png', '9519a590985d.png', '39aa3cd93c50.png', 'd0926ed2c8e5.png', '44976c3b11a6.png', '1a19f2ef4472.png', 'cf603a9ef2d5.png', 'cd563556cb57.png', '5c8482926a08.png', '09237bf783a4.png', '18b7e34eab8f.png', 'c1437a7a52c9.png', 'f0546a45ef10.png', 'c4a8f2fcf6e8.png', 'ba735b286d62.png', '73ef3c3dcbe4.png', '3599029efeb3.png', '63b4d030b016.png', '7cc4b7aabe04.png', 'f55e1d2a19e4.png', '2821998fc002.png', '1b398c0494d1.png', 'b963a11638f2.png', '61d9c88a3a4b.png', 'e16af45285e5.png', '9b093fe95d6b.png', '06024377d573.png', 'a0cd7bffdaa0.png', '810ed108f5b7.png', 'a8582e346df0.png', '3079490a4b9c.png', '98d41bce73a8.png', '3c9529918097.png', 'fc898dfeb24f.png', '8c0d05233238.png', '7a238a1d3cf3.png', 'fca1a8738b8a.png', '596f4fdb0004.png', '76df141d966b.png', '7efc91af4ae6.png', '1dfbede13143.png', 'c739ff9580d3.png', '891392c9683c.png', 'e5197d77ec68.png', '3218a6d8eb2c.png', '1782142e17d9.png', '0d0b8fc9ab5c.png', '06be1092a062.png', '7247a2c97f71.png', '7ccb267fd394.png', '9878db94d9f3.png', 'd6dbb0820ea5.png', '875a2fc5fe23.png', '19113e5f45ec.png', 'eae70f527755.png', '30db694bee42.png', '0fd16b64697e.png', '6a2642131e4a.png', 'eadc57064154.png', '5a0fe0ee4301.png', '9b57e43b44e7.png', '55034b1dbff2.png', '09934421c79e.png', 'b927a9238434.png', '3fe282197c1c.png', 'b762c29cf2f3.png', 'cb2201c226d6.png', '6e092b306fe1.png', 'e3e490babc0c.png', '1411c8ab7161.png', '49eb73968c44.png', '7663aba8d762.png', 'ff8a0b45c789.png', '1c578b72d7b3.png', '33ffddea8c6e.png', '25dc1b41ed9c.png', 'cf0824f53dd9.png', '276b14f72328.png', '5b2648ad455e.png', '4dd5d5ccddcf.png', 'e6f0ce5bf282.png', 'cff262ed8f4c.png', 'f549294e12e1.png', '4b237b958555.png', '17eb5d4ad740.png', '65e120143825.png', '8cb6b0efaaac.png', '5077cdb88aed.png', '3bf2deaa5ef0.png', 'eeaea2c5ff34.png', 'b019a49787c1.png', 'e62490b7d0e9.png', '2a93334f663a.png', '465c618f7b23.png', 'e30a890600e1.png', '1ae3c58759fb.png', 'ce6f33a81ad5.png', '08bef347f40d.png', '6d6fcf49e515.png', '876e1dd12d38.png', '00a8624548a9.png', '524f240e0c90.png', 'e8d1c6c07cf2.png', '2221cf5c7935.png', '13063d1bc4ea.png', '89ee1fa16f90.png', '27e2be850a99.png', 'b7278b4f2448.png', '9c088d2d1559.png', '8e63fc4ab532.png', '917f76f360b6.png', 'a9d0c900b6a9.png', '5dd2e26fc244.png', 'ee74c3b177e0.png', '1c9521878baf.png', '47536db39f00.png', '40c24aded50c.png', '194814669fee.png', '5de4615a5161.png', '98e8adcf085c.png', '687759336b0d.png', 'c2f3281cf528.png', '750e0168399d.png', '3f6bccf21ce8.png', 'e594c19e2e1d.png', '7116128c65ab.png', '6ea07d19b4ce.png', 'b6a0e348a01e.png', '6028a575dc27.png', '99132193eaa0.png', '5188a8afa879.png', '51d780864365.png', 'ad312ca98202.png', 'c06024f05a16.png', '3c53198519f7.png', 'fc8fce67fbf8.png', '6d0c0531083f.png', '35362d43e753.png', 'd516f77d4516.png', '10f36b0239fb.png', '1d3e9b939732.png', 'dd90c321d7bc.png', 'd99b0f7dd9b9.png', '6110ecb3bb1c.png', 'c38dec54a9f7.png', '6cfb7b44ef6f.png', 'c2d2b4f536da.png', 'eeb231c3ef1f.png', '94b1d8ad35ec.png', 'df8365d6ac33.png', 'f576e45d1da2.png', '76cfe8967f7d.png', 'a3706ce27869.png', 'c56293f53191.png', '57469423a012.png', '3435fd8675a2.png', 'd1afdb8cf70d.png', '1d674e2e32e0.png', '5d9c841eb245.png', 'e93394175a19.png', 'd18f6431ebce.png', 'e81f4a2fbbdc.png', '1c3a6b4449e9.png', '2a099b247b10.png', '6a244e855d0e.png', 'b2748ac28fc1.png', '5cbe88914a72.png', 'bb5083fae98f.png', 'f5650eb52640.png', '991a0b7a8c87.png', '8ac0c44bbf24.png', '6fe67482bfae.png', '8191ae701985.png', 'fcc6aa6755e6.png', '0551676cc2aa.png', '1509d097b69a.png', 'b759cbef90c5.png', '9a28d4e8aef0.png', '959bb2d01091.png', '1df1530b9b8d.png', '0da321efbce6.png', '67844c46bc61.png', '4dd9d29eae5d.png', '76095c338728.png', '9c6512166557.png', 'a49b0b4484ea.png', '613028ede6a0.png', '73e83a07a16d.png', '9a3109657ac1.png', '1c0e5dd1b14c.png', '7d11dbc1e738.png', '4fa26d065ad3.png', '2ecbc2e3f239.png', 'cc9270f06b65.png', '72d98188648f.png', '2b88cb6e31cd.png', '650104ede84c.png', 'fc603cbedb41.png', 'a6b6d27c1b32.png', '2d07162a13b1.png', '36b5b3c9fb32.png', '8b58f9a338e8.png', '4ecd1fdd1435.png', 'a2ddabee14e9.png', 'e2c39ed0c941.png', '15b21c80cc31.png', 'ccd6dcb2f568.png', '4dd14c380696.png', '9a94e0316ee3.png', 'bfd5c0e55420.png', 'db3cd58aa315.png', '2585bbc91909.png', '684dd88a0d49.png', 'cb602182cde3.png', '36041171f441.png', '789f0ec1eab8.png', 'd2ffe9287dc7.png', '6c4ec95dd8ba.png', 'b5bf7b84fc66.png', 'ca30a97e9d13.png', '91b7a4179ecf.png', '6810410187a0.png', 'd6283ded6aea.png', 'dee31065f8fe.png', '504a69096fcb.png', 'dad71ba27a9b.png', '62cc7ddb53b6.png', '8ae049175db6.png', 'fb61230b99dd.png', '8ead17dfb6a6.png', '3694e8c8e09a.png', '0ac436400db4.png', '8f318a978844.png', '35777eb7859d.png', 'be6cbf6e5b10.png', '8446826853d0.png', 'de2eb5c8aa83.png', '8a3eb86ae4bd.png', 'b8e20c076b03.png', '15cd5f52d300.png', '6cd606dc52e9.png', 'a6d45de20e4d.png', '975252e325e3.png', '234399352d36.png', 'ff344e5c9341.png', '1f63d44d9e3c.png', 'e529c5757d64.png', 'bebfbd907cac.png', '3b9c1f42c2f2.png', 'c8905b8d5cf1.png', '84b88e8d3bca.png', '356304d15a5c.png', 'ed2c52c14493.png', 'd4bc001f7224.png', 'b3819a805dca.png', 'bfb578c0e8d8.png', '69df7ade0575.png', '51af8a689511.png', '7dee6bf8b9c1.png', '09f6ab477654.png', '10f6ef37fe43.png', '1e4b3b823b95.png', 'ace287a5c991.png', 'eb6b1f1c09db.png', '07a2b8cabf6b.png', '5c85d22bd0de.png', '52886bed8a07.png', 'b2aaa81cc8f0.png', '857002ed4e49.png', '282bc792d23a.png', 'e77a93c3d9a9.png', 'b17f0b81dab3.png', 'a0a0cd8af5a6.png', 'b8e9a8f4617d.png', '012a242ac6ff.png', '7b691d9ced34.png', '1d14dd912671.png', '5ba156a35ff2.png', 'e1900014dabf.png', '4a693dd3921a.png', 'dd3176bacfe2.png', 'b640e3bdff75.png', '61e301bd3c25.png', '0304bedad8fe.png', '1632c4311fc9.png', 'f580566e27f5.png', '9d1feed37610.png', 'b87f9c59748b.png', '14e3f84445f7.png', 'e2161692a0b4.png', '9fa02dfb5553.png', 'f5a8c6426a71.png', 'db52626d450c.png', '0dc031c94225.png', '2cbfc6182ba2.png', '38f1901f214a.png', 'f1979147aad4.png', '578109578b46.png', '5b47043942f4.png', 'd9ba044671e1.png', '33105f9b3a04.png', '064af6592ba6.png', 'c9485c38fdd5.png', '2da82d14e1b7.png', '92587e494d51.png', 'aba3063c5413.png', 'c8a3eb9a5b52.png', '6298468d7d75.png', '33596a635b53.png', 'd8cdb7d7283a.png', '23175b7ef453.png', '12025b34deb8.png', '36e4b704b905.png', '328d141ed3aa.png', '57a5f1015504.png', 'ea68b58a6e8f.png', '103abbd8b63e.png', '885fa5fc5da8.png', 'f1d719c97838.png', 'e4730ddde408.png', '31452ad8808c.png', '6cb96a6fb029.png', '54334705a34d.png', '9a3c03a5ad0f.png', 'a7673ac44509.png', '4d1e7def7624.png', 'e322acd46152.png', '033f2b43de6d.png', '8c87bd748996.png', '8a7765e785fb.png', 'd7ac4a0c9760.png', '9cc6b1f9bcbd.png', 'cfd1bd0fcbb4.png', 'ab78a66dee6a.png', 'cca626a0e19a.png', 'c97472ef2c66.png', '8a759f94613a.png', '7f43becd3e83.png', 'e9f82b5bbaf4.png', 'ab32db41c409.png', '36677b70b1ef.png', '5293576816aa.png', 'e6552b7432b3.png', '35d6c4c50072.png', '1faf8664816c.png', '9a56cfb980ec.png', 'ff77e8e5b5f3.png', '91e2c2890c9f.png', '4bd941611343.png', '46acc506fa61.png', 'fba493e17448.png', 'c67117c6ab3b.png', 'bf1b7e21e774.png', '247e98aba610.png', '50d8249f7bc9.png', '5e7cc6ab4ac4.png', '5ea9e447bb62.png', '03c85870824c.png', '0a4e1a29ffff.png', '55f7f018c61c.png', '3f44d749cd0b.png', '281d7b7c7676.png', '8871e6a26596.png', '0d8f60ed9280.png', 'bb08949dd70a.png', '4ef0b485a7da.png', '67ed8cc78b97.png', 'c546670d9684.png', 'bdff5d8bddf8.png', '8693ab1fd2be.png', '57a710de68a4.png', '9e2058917304.png', '4f20f9a9a65b.png', 'fb767cea406c.png', 'fd48cf452e9d.png', '8fc09fecd22f.png', 'a19507501b40.png', '3f82631e9080.png', 'aeab0a63bcaf.png', '33d72035c27a.png', '272d9c043c81.png', 'b5e6ae31493c.png', '4c5ab774a381.png', '31cb39681f6a.png', '3b58b02c89ed.png', 'f30f203ef51e.png', '2af1bf226f51.png', 'eb32a815f78c.png', '4ce74e5eb51d.png', 'ffec9a18a3ce.png', '401fdfd0db07.png', 'bda8c973b09d.png', 'dd285d9e97fe.png', '4a3da369b227.png', '511fd66b2df8.png', '23d7ca170bdb.png', 'e26bcae6c67b.png', '4b1001050f1d.png', '01eb826f6467.png', '6253f23229b1.png', '38fe9f854046.png', '5a2c27b95c7c.png', '81d79d53ed7b.png', 'ab88081e5654.png', 'a9a28c37c8c4.png', '0eff8eacb2f7.png', '66460ecab347.png', '86b3a7929bec.png', 'b6bf847fbcb2.png', 'a1822dd8d05d.png', '274f5029189b.png', '7270367410a1.png', '0851d6a69589.png', 'd9c9b9786da3.png', 'df0886f1e76b.png', '5b76117c4bcb.png', 'f9ecf1795804.png', '0d9a9896f801.png', 'a150ff5dfe07.png', '28751f290ba3.png', '2fdfb80ea53c.png', 'ed2ef440d22c.png', '8ffa608170d3.png', '453a1e2754b2.png', 'fefded6bf135.png', '12ce6a1a1f31.png', '2c2aa057afc5.png', 'fb696a8e055a.png', '6e0f78e188ff.png', 'bf8092e4001d.png', '53704c80f0d8.png', '959dc602febc.png', '525acfea47e8.png', 'cffc50047828.png', '1177d583c807.png', '8846b09384a4.png', 'f66c4ee86629.png', '780f9daaa24b.png', '1cc58b15f466.png', 'a4ee03ecff60.png', 'a125377fb985.png', '09662e462531.png', '237c078d00fc.png', '3e1f8fecb06f.png', '36865bbc64d6.png', '613bacb35c05.png', '3d3e288d490e.png', '586f5c56081e.png', '66d2ca47aa44.png', '870f433e8f37.png', '1633f8291a80.png', '7f6690fa390a.png', '45e4b7eada54.png', 'ea9e0fb6fb0b.png', 'd0079cc188e9.png', 'b8dab47a260e.png', '901a3552fe26.png', '4d17559ac1e2.png', 'f56ff0440ed1.png', '0a9ec1e99ce4.png', 'f80118bbda18.png', 'ee1ec90b980f.png', 'e1c02f6c3362.png', '22895c89792f.png', 'b37aae3c8fe1.png', 'af7a36454670.png', '0104b032c141.png', 'b09101adb478.png', '96793edb1003.png', '54bbe3da103e.png', '96ea316ed0ab.png', '7102f29e052e.png', 'd48178e4a49b.png', 'a15470303941.png', 'a4012932e18d.png', '4661006f3ba6.png', '81bc03e2ff2b.png', '934104859e68.png', '03a7f4a5786f.png', '5e5275ddee29.png', '13c191b59ed0.png', '88e5051f65bd.png', '851e40a21f81.png', 'b9bc81fcb075.png', '3a6e9730b298.png', 'bdb98063fe84.png', 'e06cccc08c59.png', 'ffd97f8cd5aa.png', '4403538fb50f.png', 'a96ff96bbae5.png', '84c663f39632.png', '78bcdffb8785.png', '2cef97083e6f.png', '8ed586c43023.png', '962c0fc85e13.png', 'bc73ce76ec43.png', 'e32a359be36d.png', '47b756014447.png', '874f8c1929f6.png', 'a0adbe677508.png', '0b64a0a06f9a.png', '4a558a1cd243.png', 'ca63fe4f4b52.png', '2f284b6a1940.png', 'ab03d50bba2f.png', 'c0968d41eb93.png', '51131b48f9d4.png', '46923eea9a4e.png', 'd67374d3fa2a.png', '0f364b7d4384.png', '3cdda8b3df19.png', 'c80f79579fed.png', 'fa573163dd8b.png', '08ee569d4721.png', 'a3957df90a78.png', '042470a92154.png', '70f5caf5f305.png', 'ca891d37a43c.png', 'eedae6b28f96.png', '23fca0693e2a.png', 'a8aed92940fb.png', 'e4f12411fd85.png', '7a6495a39d87.png', 'bfefa7344e7d.png', 'd41b33fcb94f.png', '88e4399d207c.png', '10fca1abf338.png', '7d37a2939f12.png', '6d259b5b4c76.png', '196e6a186452.png', 'abbb8791785e.png', 'bb45257258cc.png', 'c096131ad065.png', '4a0890b08532.png', '7e6e90a93aa5.png', 'b4b04d81acbb.png', '14515b8f19b6.png', '0c7e82daf5a0.png', '82bb8a01935f.png', '96b5474ae604.png', '7743f4e04a6d.png', '5f6db235c04d.png', 'd88806d9ece9.png', '144a1a426137.png', 'd868acdccb5b.png', 'a2b995b81692.png', '032d7b0b4bf6.png', '946545473380.png', 'a5a2a7003d60.png', '26d60db3bbfd.png', 'c6229222bf22.png', '8a482c024fc2.png', '7a6e384a0846.png', 'd9311f7497cb.png', 'b549af91bd30.png', '82ac8463fadd.png', '27fca9f12b3c.png', 'e4b0df29b96f.png', '43823561c3f0.png', 'eae901557a84.png', '9568eb7e9c08.png', '0e94cd271c00.png', '222f3ee3a1e8.png', 'f4ea2a2cfbb9.png', '1bf30c84bbad.png', '0c2e2369dfff.png', '784d6d302f98.png', '8b76c3c5cb3e.png', '4134b290f5f3.png', 'a3b2e93d058b.png', '6c3745a222da.png', 'fb1b8771c70a.png', '1d46f1326394.png', '8785b71238d8.png', 'acc9f29538c4.png', '64fedbf97473.png', '69fff98cb32a.png', '7635921c5efb.png', 'eed4afc8ec83.png', '3ddb86eb530e.png', '4e8585a96739.png', '60f15dd68d30.png', '5265dc9acdf8.png', '4dc2211a1c31.png', '5b3e7197ac1c.png', '31b5d6fb0256.png', '9e5ec293267c.png', '43ddd0ab0cc4.png', '49419f8d5cb4.png', '35df2bc6ae95.png', 'daad7b617f21.png', '01c7808d901d.png', 'ce754234d760.png', '2209daf71aab.png', '25a0a1e41afd.png', 'b10fca20c885.png', '29b52f64d2db.png', 'c3acf47700ea.png', 'f8fc411092c7.png', '5cc6dea19614.png', '27bab1432f61.png', '42985aa2e32f.png', '966c07831334.png', '9b95d6203406.png', 'c5a0e84e955d.png', 'a688f20f8895.png', 'cd01672507c9.png', '39923b29988a.png', '260a455692b5.png', 'a2696f444ecb.png', 'f3a4751af42e.png', 'e12df54e0d1e.png', 'ddb222ff7c1d.png', '97fdee242fea.png', '720b5f62ce80.png', '1638404f385c.png', '7427dedafccf.png', '664b1f9a2087.png', 'ef5155990874.png', '21abd36095a1.png', 'fb88783de055.png', 'ababe19ed448.png', 'e47770a2e5d1.png', 'c3b15bf9b4bc.png', '207dd0487264.png', '7179f85bfd6f.png', '24f3e70f0419.png', 'c5ba9e455d5e.png', 'e663c6627a95.png', '42b9c1977681.png', '9a4f370d341b.png', '435414ccccf7.png', '9837048b85dc.png', '59e5212f7139.png', 'e9ab8413e771.png', '12b57dac703e.png', 'e69b48516577.png', '83b61051737f.png', '2c9dfc270f1b.png', '962cf85e4f6d.png', 'a7b0d0c51731.png', 'fbfa925506f6.png', '4145dcb25053.png', 'bab776139279.png', '8bbd7835e9aa.png', '880edb2cdb69.png', 'bed8296c8dfe.png', '7828dd083cdc.png', '518e880613de.png', '3a122851e526.png', '8564b7aa3c1a.png', '64678182d8a8.png', 'f5733f77273d.png', 'a015ce4f51ad.png', '02da652c74b8.png', 'f9e779a13204.png', '34acae864963.png', '157d17349cc6.png', '20688cb25704.png', '6630f8675a97.png', 'f4de9620e3f2.png', '4e7694eebb91.png', 'e868c3da340b.png', 'cbf0394039f8.png', 'ec4649213ccf.png', '913b1890ed1e.png', 'aae8f9f3ef8c.png', '4fef9ed8a4c5.png', 'fa0c87bd75ce.png', '6061f5b7378d.png', '6bcce181be65.png', '7ed4128b2a4e.png', '52dbec057cc8.png', '9a7bd084395e.png', '799cb4c816ae.png', 'b46b09a45f39.png', 'ae61e19fb766.png', '762d6e5d5068.png', '531b39880c32.png', '188219f2d9c6.png', '7f0ffeb0a333.png', '24de56d433cd.png', '0519b934f6b1.png', '85cbb84ac8e0.png', 'c3d12a23f451.png', 'ea05c22d92e9.png', 'b7983cb3f270.png', '61a62b1dcc36.png', '1a0dbc6c0cda.png', '248dec89b3a2.png', '5e7db41b3bee.png', '0e3572b5884a.png', '3a4cfea0a766.png', '6dc0281f11e3.png', '51d0034d177d.png', '2df07eb5779f.png', 'ad6b07d5c338.png', 'bb9a3d835a94.png', '0f6e645466a2.png', '46d3316c4857.png', 'a85cda5f725d.png', '461fa5292fda.png', '50840c36f0b4.png', '0151781fe50b.png', 'cd4e7f9fa1a9.png', 'b3d135bd3bb5.png', '5eb8fb1aad41.png', '3748349334f6.png', '4dd4a4bf2421.png', '8f10e41a2f02.png', '436e1793d240.png', 'b085caa513a8.png', 'ed6bd9293a89.png', '639915f58a2f.png', 'd25b8a8ad3c4.png', 'f42b693a9414.png', '283c3aeba594.png', '201f6e10c108.png', 'aafb0c944f14.png', 'efff2f1a35f5.png', '4a1afe4044f4.png', '191a711852bd.png', '94f9ecf4b8d2.png', '7a9f45fdf29b.png', 'f7fec8935126.png', '2994f17f58a5.png', 'a44345b27804.png', 'c7c0470bcf87.png', 'b98f77098b9d.png', '3178559fbf57.png', '958c1fa044ba.png', 'e2a47a74e6e1.png', 'ef8c39eb9157.png', 'b8f1b30877db.png', '4246ed634f25.png', '5b1c4cefeb24.png', '453d553b0a94.png', 'f025f33b2c9b.png', '7ccf9d25dc48.png', '0124dffecf29.png', '096436d68d06.png', '63c3c571b8ee.png', 'e4e343eaae2a.png', '60edda7b4871.png', '735836b1ffa6.png', '5b32ece9c627.png', 'b71428739d4e.png', '1f4fb37e0854.png', '4f0866b90c27.png', '362c4a96cebb.png', '8676427e4625.png', '04efb1a284cc.png', '384631079d1e.png', 'cfed7c1172ec.png', '894a37fc3738.png', 'f6d760566a51.png', 'e0b5a982a018.png', '54cab3596214.png', '2a7373eeb352.png', 'fa9f1bc03f21.png', '72a867980067.png', '9cedf5c7016b.png', 'c1e6fa1ad314.png', '4da2961e62fe.png', '201f882365d3.png', '80e6e425f966.png', '4c17e85686f0.png', 'd7bc62d60e8c.png', '1c5ad36fb799.png', '5baed382f062.png', 'dce73d90c00c.png', '79be2ff796bf.png', 'a28bfb772f50.png', '19e350c7c83c.png', '519c6e8f78dc.png', 'e2ec22b3d07e.png', 'a77dbec966d4.png', '07e827469099.png', '92b0d27fc0ec.png', 'bb733062f494.png', '4c570172778b.png', '0cb14014117d.png', 'c8fc0df22999.png', 'b468ebf5cb11.png', 'e019b3e0f33d.png', '4ccee4db09b6.png', 'dd02d60bef14.png', 'fdd18ccbbdc5.png', '3ac3fbfca7d4.png', 'd18e5b68f6d2.png', 'b86fb2d5be1a.png', 'e966850247f4.png', 'fc4c2d35c6f8.png', 'ac17cc18a994.png', '697538183db5.png', 'd81338217fc5.png', '001639a390f0.png', '5777ef74c9ec.png', 'a07d571bf7ba.png', 'd3be5346684b.png', 'ad2f0b9d059c.png', 'c68dfa021d62.png', '9232dc06cfdc.png', 'd1a60c3b9fe5.png', '62e6f814c8f5.png', '5d5b5da5f939.png', 'b7aca95b97b9.png', '093cf723fede.png', 'f3cd489acbee.png', '7d94a000c2d0.png', 'f1dc26c4bfa3.png', '96c3e3db68bc.png', '22ce8ef69357.png', '4eabad7948cf.png', '0db1d8dcf219.png', '8aab201c0691.png', '976082127e2a.png', 'd2d523e9f669.png', '3132556f5352.png', '211518c46162.png', '9e3510963315.png', 'e9129ce55fd7.png', 'd85588ff2ebd.png', 'b22cc1bf0b8a.png', 'fce73678f650.png', 'e65f94ad9be3.png', 'f8f5942b690e.png', '04a6fc58dabc.png', 'fe0fc67c7980.png', '5bda2ed09e62.png', '65e530ee2e79.png', 'e1fb532f55df.png', 'f002ce614c59.png', '155e2df6bfcf.png', '499c8df39222.png', '1d37f1c8b6d8.png', 'a7b03e58a6e1.png', '308f7fce6f0d.png', '9041eb43456e.png', 'a8263d248523.png', 'ec6659926105.png', 'e06d3d4733f0.png', '367c7049929c.png', 'd8da9de62743.png', 'ca6842bfcbc9.png', '2608e1dac5b1.png', '7c90ab025331.png', 'b60dbf9f0744.png', '3044022c6969.png', '86d58f850a0c.png', '5327f88a1919.png', 'f999c6921e6d.png', 'c8d2d32f7f29.png', '18d8fdb140b7.png', 'e229aca862c7.png', '274f4de2a59d.png', 'f4df3d86688d.png', '01b3aed3ed4c.png', 'c027e5482e8c.png', 'ee2c2a5f7d0e.png', 'dbd062558b81.png', '1e7ccd4a1c87.png', '6d7d26025122.png', '5e18af29d812.png', '76cb010f7aa0.png', 'cd29c88c9e36.png', 'a47878630dc2.png', 'ef81cd8854cb.png', '13d014ccd136.png', 'abf09c44d5f4.png', '187f6ccda87a.png', '11b5c77fbf79.png', '3e3a3955b9c5.png', 'c9ea9d5eab65.png', '48fda42bd5d4.png', 'a9c7b83caf81.png', 'cf0575534cec.png', 'c1ebe785503a.png', '2b4c7b5f1f1e.png', 'b187b3c93afb.png', '612f2df37a1d.png', 'f36cb007a1ef.png', '1124ffcd76c2.png', '75a7bc945b7d.png', 'a963ac561580.png', '242fc19be06f.png', '4528fbbd43a3.png', '1f9ccda4ddf2.png', '5cf9127f251a.png', '222d0ac042b4.png', '97a5ad7548b7.png', 'c6d4e4a3bd4c.png', 'e23add229074.png', 'f4c7ae514c54.png', '7455e2b5fc57.png', 'd38cf0f4a9af.png', '873fe0404d6e.png', '847b04287c9c.png', 'e2856afe62c5.png', '3fd7df6099e3.png', '0a85a1e8f9e9.png', '8344c783da65.png', '026dcd9af143.png', 'd3dfd0a2dee6.png', '6e018411ba4a.png', '0953c0ac1735.png', 'aa6242f9e08c.png', '731b19a460ad.png', '8acdf12f412a.png', '5d6239c0fd39.png', '43fb6eda9b97.png', '2a08ed6bbcbc.png', '4f7755e74a9e.png', '1ca62b3e4fd3.png', '5671eb95512b.png', '0dbaa09a458c.png', '510aa0a898fa.png', 'f18abfa690ab.png', '1891698febce.png', '4a213b405ee4.png', '6e44f6d04fc9.png', 'b200c23b299b.png', 'ecad6845f630.png', '2d3f4094c08a.png', 'c6a145742708.png', 'f7116e7b2f4e.png', 'e50b0174690d.png', 'af831c158744.png', 'ae2c3f6312ef.png', 'b1b3e7d0a5f3.png', '16060f05d047.png', '9f5a8665cf2e.png', '4704dbb59536.png', '27e4c800a449.png', '3ac92ac3d65a.png', 'cd01f4f83336.png', 'd567a1a22d33.png', '895fe2bfc5b6.png', 'dd3dad6ca78f.png', 'e756495c11cb.png', '96a9706b8534.png', 'b7e0f95353f2.png', '0ceb222f6629.png', '15e24b73d4a7.png', '2f81ee5f2926.png', '38487e1a5b1f.png', '9e99ae6ee7af.png', '3b0190bbe615.png', '709784f7fcc2.png', '6f4e0538d1e4.png', 'c6a8f8f998a2.png', '0daddc45d832.png', '8e20b8fac7c3.png', '4cddfc22b0ad.png', 'bd34a0639575.png', 'ffcf7b45f213.png', '3732de8b416f.png', '34723fae6475.png', 'c24bcf7a1bc4.png', '587146a55885.png', '6f4719c6bb4b.png', 'f71333204618.png', 'd83c3efade75.png', 'da8900ac7f29.png', 'e387311a840e.png', '9b0eb9f41da4.png', 'f64b6e85f1c9.png', '1541226c5d72.png', 'b07bc463b718.png', '79059d0592c4.png', '8b079e79035f.png', '77f69c7ff324.png', '8906c9ed54a2.png', '5347b4c8e9b3.png', '3232b34cbe99.png', '870fbe6eaa68.png', 'a06a63d866b2.png', '33b91def2035.png', '2735be026d44.png', 'de50dfa745f8.png', '31616ff6b53b.png', '571bbdbf585e.png', '78a577c3e0bf.png', '30cab14951ac.png', '1d55e689cf84.png', '467b7d9d811c.png', '5e7630f8438e.png', 'bca2bdc15fc5.png', 'b69c224edd6e.png', '1002f3fe38f0.png', '269b44e628eb.png', '902dc5a91a3f.png', 'add1d681d712.png', 'f0800723bc63.png', 'ed3ce1674761.png', '2b10f138e67d.png', '2408799a09b2.png', '4f5dd7660b17.png', '3ca12e02dd4e.png', '8e3b79e1f1f7.png', '94ef1d14597f.png', 'd2cd47ed2c1d.png', 'b77b88926843.png', 'c1896142a20a.png', '72606afaf3da.png', '6fbaaf8eb67a.png', '00b74780d31d.png', 'd7bc00091cfc.png', 'cc1eebed9276.png', '6e1db8711879.png', 'bfa30ebf63a8.png', 'b67ae80f7eba.png', 'ed3a0fc5b546.png', '310c27067ac0.png', '68332fdcaa70.png', 'b06dabab4f09.png', 'ac720570dd0f.png', 'ff4cd992667b.png', '6987804eb464.png', 'c31651ea04c6.png', '6e092caa065f.png', '7fdb177b8f7d.png', '62b826899151.png', '9ce46d400cd6.png', '61c667663f2f.png', 'b2b7ccd34cbd.png', 'd436c06f0490.png', '50f5201fd18a.png', '2bb063318cf1.png', 'd952dbfb0fe4.png', 'c64c0966b4cf.png', '197af0de76e2.png', 'abdb365cacbc.png', '6a905a7202d2.png', '6bf2a81a5d91.png', 'f47a2a4a0411.png', '0c43c79e8cfb.png', '8f1e7433a95d.png', 'cd45bfa07d41.png', 'b598bc9753c2.png', '0097f532ac9f.png', '18621b9ca978.png', 'e6a58edc5b42.png', '28f98cfe3858.png', '5b068765e846.png', '32ed318235b8.png', 'a4d4b69f7404.png', 'af3b0115aad1.png', 'cf6551521a35.png', 'f03d3c4ce7fb.png', 'a64273801bde.png', '359bab5d784b.png', '1e8c31e29dd3.png', 'f58d37d48e42.png', 'f3a268d2726d.png', '8dba09a4e5ed.png', '810d3779abd9.png', '9eaac43744f5.png', 'd9ad2a0ec026.png', 'bebb3f167654.png', 'd8404680bba6.png', '8ff2733f6aef.png', '4e82c3c8d31f.png', '0f495d87656a.png', 'e135d7ba9a0e.png', '025a169a0bb0.png', '6cb98da77e3e.png', 'b553e7909535.png', '2cacdb0dffae.png', '50d8a8fb7737.png', '08a3875063c3.png', '0a61bddab956.png', 'e4c799738a19.png', 'a419fcb2dfb5.png', '9f935fb38440.png', 'dfea19863428.png', 'a1eb88562239.png', 'a3bd2e034614.png', '47d1603a555b.png', '03fd50da928d.png', '5e97cb2b0888.png', 'f1a761c68559.png', '52ae917fcea4.png', '3c78bfca247b.png', '930fee99213a.png', 'f0a2dc580009.png', 'a21b37719f9b.png', '921433215353.png', '1002b5151b8e.png', 'b4e15102cd7a.png', '4ee1ad981a6d.png', 'c0e509786f7f.png', '8dafa62f9322.png', '6d292ca4c9ad.png', '66bae1ba227f.png', 'cd48cfab4e44.png', '24b943fe725e.png', '4e54ccfd49b2.png', '408ea9d5e082.png', '65cf00be6fb4.png', 'e38f3a65b02b.png', '0c917c372572.png', '7a06ea127e02.png', '4c60b10a3a6a.png', 'cd3fd04d72f5.png', 'e933923aab15.png', 'b49b2fac2514.png', '3ccf96c1dd6d.png', '98277aeb96a7.png', '63e041a757eb.png', 'd28bd830c171.png', '61f403fdb434.png', 'a56230242a95.png', '4a44cc840ebe.png', 'ae58ccb5905e.png', 'a5bb85afc6e8.png', 'bff51afc76d4.png', 'bb783d8e496f.png', '7a42443ed106.png', 'fb6b8200b7f8.png', '5b72ff04333d.png', 'f762c272c522.png', '4c52922f3bfd.png', 'c21eb81de9fc.png', '6fe4751a3b42.png', 'e65a2ff90494.png', '5173d54fc214.png', '44a4d04162cc.png', '3e61703b5ab2.png', 'e3ec668f6fad.png', 'a8e08e7fe016.png', '3af9aaa880e9.png', 'fe674c2f73f5.png', 'b086c7cd3868.png', '437900a99871.png', 'c62585bd68fb.png', '041f09eec1e8.png', '4beeca5cc859.png', '7fc3a8bb40de.png', '3a643599f852.png', '5486da4273d7.png', 'c406325360b1.png', '922586d86cd8.png', '525d0dd8dc45.png', '144b01e7b993.png', '9a159d4674cd.png', '5299a532f0e0.png', '882a71de424e.png', 'ce887b196c23.png', 'e47452069ea1.png', '4ab8c0cece7f.png', 'ae49cc60f251.png', '84a72e15b23c.png', '07419eddd6be.png', '7e9081e95bf6.png', 'e4ae1ee6aada.png', '3710ff45299c.png', 'cf1b9d26d38d.png', 'e39b627cf648.png', '28f73575e1f2.png', 'db690e2d02f8.png', '33778d136069.png', 'b82d5f1f1145.png', '58a9e0d7f7af.png', '8273fdb4405e.png', '17d997fe1090.png', '3325b1fe55d2.png', '1ffaa51a6245.png', '02685f13cefd.png', 'f2ee81781411.png', 'bb752b179751.png', 'd3d578fe433f.png', '87774aafe068.png', '974c7d7b9c64.png', 'ace2281f00c4.png', '0e43c8298fc0.png', 'b9b6ee2b9453.png', '58f07741ee3b.png', '6e73acb2cf60.png', '83d81ba5959c.png', '65e51e18242b.png', '599498e9e4bc.png', 'f0c13be90519.png', '1f3f32efaf20.png', 'd160ebef4117.png', 'c87493ed320c.png', '6c2555a9cae4.png', 'd4583e9525dc.png', '91f3c4c1e72b.png', '76f3473df8a6.png', 'bc23f74e14dd.png', 'fdc685055659.png', 'ed648b9bcd95.png', 'd51b3fe0fa1b.png', 'b0eeae01b8ab.png', 'df9cb3729eb1.png', 'e59c5f345bb0.png', 'ad1f7445b1a8.png', '291e2ff3d834.png', '5d024177e214.png', '7e5a76c4e103.png', '4bcee3cbe232.png', '7347bd23ba80.png', '99240ee00485.png', '9e2a8135f471.png', '4409965eb2a4.png', '54bdcdecd8f3.png', '4c129470cec4.png', '2d04cead4d3a.png', 'e12d41e7b221.png', '87a9f4d20f07.png', '07929d32b5b3.png', '2967e578939f.png', 'bda91b76095b.png', '51e656e5a541.png', '76cab26493f1.png', 'ea4ce9516144.png', '3286073a976e.png', '57f5ad4b5b29.png', 'b835b6e31a59.png', '87d46b1cc4e9.png', 'a664d2055886.png', 'e17507a4a1f5.png', 'a08a0133754a.png', '90960ddf4d14.png', '8d4d14a4ab07.png', '07a3be30563b.png', '1f5496352859.png', '0c38940e1f80.png', '29bc0e721cfe.png', '582f739b8f62.png', '55fd453001cc.png', '6b07971c3bf6.png', 'd844a7252f4e.png', '6df8b7b6e837.png', '4e1e252317b5.png', '174db0854291.png', 'c5a9ebef1517.png', 'beb2ad14fd2d.png', '996f57c86ba5.png', '30263a7d5609.png', '22449af52060.png', '4158c340fa49.png', '760b6f4c6d82.png', 'b94c58d063bf.png', '8be6629a6039.png', '565f3404f9b2.png', 'e3b47ed5b511.png', 'ed6704e3b72e.png', '1594ca6c30d3.png', 'e7a372a1c3a4.png', '19545647508e.png', 'e9678824215d.png', 'd30d079e6f9a.png', '545df1bbcd61.png', 'b21abe5d9722.png', 'c561bcd519e9.png', '92889b863ae6.png', '48c49f662f7d.png', 'f5e6226bd2e0.png', 'eabf421f94d0.png', '55092c0071eb.png', 'a9bc2f892cb3.png', '756b0d6488bb.png', '7214fc7cbe03.png', '12e6e66c80a7.png', '3e6bfc4d5c65.png', '5814cbd2e9bf.png', '4abca30b676b.png', 'f7735b6d47f7.png', '2bb3c492d6d3.png', 'f72ef9ceeaa8.png', '19722bff5a09.png', '44e951e45dca.png', 'f58cdfa968be.png', '05cd0178ccfe.png', 'f9156aeffc5e.png', '8bf05909e1e1.png', 'e4d3d437b0a8.png', 'bdc6c60e9133.png', 'f2c0b41acd05.png', '3c42512c81e0.png', '3cd9713c0ecb.png', '9f436886e056.png', '59fee5bc3479.png', 'ec363f48867b.png', '2ef955d6d9ff.png', 'da1fb35f5df9.png', '35aa7f5c2ec0.png', '239f2c348ea4.png', 'da949aa67a4f.png', '71a39c660432.png', '00f6c1be5a33.png', '2d7666b8884f.png', '62b4be2799ca.png', 'df4aec4a0eaf.png', 'd93b61dc8f64.png', '98f48850ebce.png', '2e79041ef722.png', 'd4be0403e6ab.png', '7525ebb3434d.png', '3fd45879afe6.png', 'eb1ad14dd281.png', 'd91635f380b4.png', 'ec0c9f817b03.png', '7569ac24762e.png', '0a902c80d5da.png', 'e66ad813a508.png', 'e087bd4b88f2.png', 'f633c474e8b8.png', 'ccea49708830.png', '7f84284598f5.png', 'c7c3d363bc86.png', '76bc31e0d3be.png', '9b32e8ef0ca0.png', '4478b870e549.png', '513b0a4651fa.png', 'c90c6b94cf40.png', '2fb3a8606a77.png', 'a73c3d516c59.png', '0318598cfd16.png', 'c252da9b41d8.png', 'd51c2153d151.png', '7b29e3783919.png', '0125fbd2e791.png', '9985375d709f.png', '165cd2070ebd.png', 'b7f0bc7d399e.png', '00cc2b75cddd.png', 'c0e15e8e2b46.png', '1d74c4713e21.png', 'f9d8ff3e6592.png', 'acf976efd7ce.png', '29f44aea93a4.png', '2cf18033da31.png', '365f8c01d994.png', 'a04fb36db784.png', 'e2265c383348.png', '254052cf3e48.png', '3dfc50108072.png', '26d9576e8043.png', 'a06e5ac695ce.png', '0cd31078cd08.png', '25d069089c5e.png', '96c699221180.png', '1438288bb2e1.png', '2dd28ac497d2.png', '26fc2358a38d.png', '08f8838d69bb.png', '85f99e7e4052.png', 'c9e697117f3f.png', '3908b3cfd620.png', '606daaf0bfc7.png', '87b671c6d4c5.png', '5056fa7d505f.png', '77a1f1398fdb.png', '60eeae3ba23d.png', '898f0bc8acfa.png', '07751b94a88a.png', '086d41d17da8.png', '77e7c7a160c8.png', '4927945ecfed.png', 'e7578d8dba72.png', 'c334f8688b77.png', '217dad18a5ed.png', 'f6f7dba7104d.png', 'ea1d045f9fea.png', 'd5c63a8d9e94.png', '4c53cc97ea13.png', 'a34fc5376669.png', 'd1f7ea924a01.png', 'c3c8fdda50c0.png', 'd9a475dfe59a.png', '6dc07f968794.png', '188a9323be03.png', '3c326543fff6.png', 'aa60813e1a8d.png', 'bc8c6a778cde.png', 'e6a6acf7fca1.png', 'ad93d88c87ea.png', 'e1ab92228e60.png', '523d0c2cb4d6.png', 'ac5b5dddf91b.png', 'abe940882578.png', 'b13d72ceea26.png', 'bc34f52c37c7.png', 'a45d77edf8d9.png', '5b5b80a3edee.png', 'f02057c41256.png', 'cfb17a7cc8d4.png', '4c635a01593d.png', 'a26f50218b84.png', 'b9519abce0c1.png', 'b532dedd928c.png', 'bfaa0080ab61.png', '52ddde91a349.png', 'c94f37085d0f.png', '69591ebb198d.png', '6d5a8362dd1e.png', 'de6210f88536.png', '454a944eb557.png', 'd144144a2f3f.png', '4fd5ec0dca09.png', 'bcc762618e7d.png', '1f07dae3cadb.png', '7e77b61e1639.png', '90a786abe58e.png', 'aa6673241154.png', 'c12e9ca420a5.png', '2a2a6435f7f3.png', '9d74428188bb.png', 'a8a6588c8eb7.png', '374535e0adb8.png', 'dd19428c3d29.png', 'bec0acd539b2.png', '45369821d6a3.png', '39fd8ef3a45c.png', '03b373718013.png', 'be68322c7223.png', 'd2c5fb82fe5f.png', 'f5e9a307288c.png', '9ed6c2b25767.png', '2cdcc910778d.png', '462937ece243.png', '65dda202653d.png', '8bed09514c3b.png', 'aaaadb174012.png', 'dc0eea0b68a7.png', 'd94e10f42861.png', 'c18a006f7f1d.png', '342edf9b889d.png', 'f58f0b2fd718.png', '357f02a779d7.png', 'f4e68b61f480.png', 'f0e1201b5c1f.png', 'b191ba0a2b12.png', '24b87f744598.png', 'cd9e2190c73f.png', 'fda39982a810.png', '1c9c583c10bf.png', '1f15ca672675.png', '2665f72e2dd3.png', '8114d6a160df.png', '4dd71fc7f22b.png', '521b5377a727.png', 'd18b1d8ac4de.png', 'f57cf3b6f48e.png', 'f952ad2e4356.png', '83df53d58f28.png', '6666c4f18396.png', '495255c7492f.png', '3b73a3a4a734.png', '82e5bc01f8a4.png', '767d777ee889.png', '493d99f030e2.png', 'ec57cc20d776.png', 'b0f8613305a3.png', 'c05b7b4c22fe.png', 'e10190a9d52f.png', '8421107255ae.png', 'a86128b601a7.png', '9095d43fb132.png', '40527a5e95dd.png', '289a47dcbb82.png', '180afe1d5ef7.png', '96d48b073f18.png', 'daff5427c9b2.png', '18b99159a14f.png', 'a47432cd41e7.png', 'd66b6f333dc7.png', '13073f075a56.png', '59928f999ae7.png', '6531070bf03c.png', 'aed4e743c230.png', '7f1f3269f546.png', '1b32e1d775ea.png', 'a9e3d186cd1b.png', '878a3a097436.png', 'f298b7d05958.png', 'fdbc252813b1.png', 'ae1344610ebe.png', '378963f9df22.png', '7ce671f952be.png', 'a70d0f12a641.png', 'ac1667fac512.png', 'ef247f28004f.png', '75ed83dbccce.png', '52230bbef30e.png', 'c1819db0aece.png', 'd3de0d313d61.png', '05b1bb2bdb81.png', 'd9d2631f043c.png', 'f72adcac5638.png', 'c52bb7343387.png', '0b00f8a77510.png', '314862758acf.png', 'f85c78201a50.png', '1269ab57c2e6.png', '4e6071b73120.png', 'd271d3a2b552.png', '4982378d72f9.png', '80ca40196225.png', 'bd06028eb7dd.png', '1844a039b4ea.png', 'd51e5d7484ea.png', '069f43616fab.png', '3ca8be3b40d6.png', '0ae2dd2e09ea.png', '10f10fd30718.png', 'da3a2275c850.png', 'f451eee2b66b.png', '8d7bb0649a02.png', '3461dc601cc2.png', 'ae8472f8d310.png', 'ba4d2c4b3039.png', 'badb5ff8d3c7.png', '03747397839f.png', '5ad3dabeb2cd.png', '0f96c358a250.png', 'c5e238aa18be.png', 'ba2624883599.png', '0a09aa7356c0.png', '95a4cc805c7b.png', '62ecdc90dd42.png', 'b4f41b5bf0ef.png', 'edceb0657d77.png', '76fe19ff64fb.png', 'a3475dc3ac80.png', 'f4874247ede6.png', '4689b739d240.png', 'ceb32a193eff.png', '115e42dd6a81.png', '2b5bb6d33959.png', '90b8bf342032.png', '0243404e8a00.png', 'f0c0f7b5e820.png', 'a0e635689259.png', 'afc744fad65e.png', '07596907347b.png', '97c6cb55866d.png', '7ea756985353.png', 'df84e7113003.png', '15c24478ac72.png', '184a185e7447.png', '8aa3c4681542.png', '290ecdba359f.png', '7bda86d95c5b.png', '1df3e03a8f5f.png', 'dec5595e6154.png', 'de38adaae009.png', '78937523f7a8.png', 'ce8d2efd9d4f.png', '8e6df9eedcd8.png', '0da09e3ce8f1.png', '7ee6de71c140.png', 'af133a85ea0c.png', '58ccba7eec9c.png', 'f02babb3a023.png', '1409ab48175a.png', 'a15590a7d774.png', '080ee76c958c.png', 'd803598dabda.png', '08037e4490e5.png', '0d0a21fd354f.png', '0b3efe669365.png', '830e297965a1.png', 'b8ebedd382de.png', 'e1b8acb1cea1.png', 'e76a9cbb2a8c.png', 'e32dc722eca5.png', '711d1480d2e3.png', 'a8b637abd96b.png', 'f45f1485c940.png', '3f3de2a6b0f5.png', '8ead8f37423c.png', 'd5a39339ff3d.png', 'e0d229db881a.png', '27b68863349f.png', '3246f07e65b4.png', '1623e8e3adc4.png', '9688c6ef5dc5.png', 'a14fcf84bfe1.png', '842d697884f6.png', 'd78b7401096f.png', 'ae8424cdb029.png', '445a8a6da55c.png', '2db0cd3e30da.png', '94111ed3d276.png', 'daeaa5d8cf70.png', '8dc22e65c06f.png', '0fb560f9adb2.png', '43f22d1be8dd.png', 'fdd534271f3d.png', '1b4625877527.png', '3580a545016d.png', '3f98be586fe3.png', 'f8cf7ed8ef00.png', '165634a6167e.png', '53ddae6a619e.png', 'ff631653374e.png', '29d059522fa1.png', '441117562359.png', 'b7ce561a7328.png', '495106ae3b68.png', 'cf8f1bc7a215.png', '496155f71d0a.png', '3f47f83217b5.png', '77543f66a84a.png', 'e150935f66a6.png', '253e96488cfb.png', '2923971566fe.png', 'b460ca9fa26f.png', 'da9574d35b82.png', '54b322c66d01.png', '44878f34e31f.png', '5b3d41626ec5.png', '7f39c36469b5.png', '1608c82a263f.png', '41ab357d103f.png', '573ea80a53be.png', '57db4781e7ec.png', '0b2ea8f268cf.png', '710b05a96e0f.png', '9858cc2ae073.png', 'd0d59ed675b5.png', 'b033ab4fb723.png', 'aad0c0ee9268.png', '2b2f5a0f880d.png', '80b5a9519aec.png', 'b99794a0beed.png', 'c40976189f22.png', 'd865997a6280.png', '052d9a3fe55a.png', '4cf4d528c08e.png', '12e3f5f2cb17.png', '3f8d5c940ba4.png', '034cb07a550f.png', 'c597ef460944.png', '1bb0ddfe753a.png', '91e8af9ceee9.png', '5e505e25cd3e.png', 'da9262d9f5d9.png', '8c2f0f04e1ed.png', 'd26bc6e1230d.png', 'ba4e62c11cc0.png', '4e231670b48c.png', 'd99dd99be001.png', '5bf3357a2823.png', 'cf8ae5501bd6.png', 'c365c598ad4e.png', '7ad0c4975890.png', '576e189d23d4.png', '6a2c3f4ef329.png', 'e5f73f2855c0.png', '15bed5adde74.png', 'c1c8550508e0.png', '7a39c91416e2.png', '389552047476.png', '4e4a6224a04e.png', '6181aa9f75f4.png', 'c6e1e9fbf39b.png', 'eaa0dfbd5024.png', '78d53c82a23e.png', 'df841a0440d8.png', '785777558f05.png', 'ab724603ee93.png', '4a589edaea60.png', 'c0a0828e01b4.png', '6b00cb764237.png', '3b10191dfd25.png', '6653ad026901.png', 'e9faf0296643.png', 'bd500a73beae.png', 'e9ff9352ccb3.png', '3cab32dd6ef9.png', 'e29e54ff921e.png', '4ed31cc07366.png', 'da9fe02dead3.png', '80ed04a84a16.png', '34a7dbd3f05c.png', 'a1faeb4d5f10.png', '5b0e53f53ef3.png', '0afbeeef0ff7.png', 'b65ff67743b2.png', 'f71aca5a7dc3.png', '8f2996b8d855.png', 'c373b73a80c8.png', 'bacfb1029f6b.png', 'c01eae4b4939.png', '0fe31196e0e8.png', '093a42649c29.png', 'ae975c43bd8b.png', '22d843b2bbd1.png', '9a496b1e20f9.png', 'ef7eb85b75fc.png', '4860f7813654.png', 'ebd96d853918.png', 'bcd503c726ba.png', '4242c0d87f57.png', '8f06ca4642bd.png', '6504b703c429.png', '4d3de40ced3a.png', '7ec1ffe8220b.png', '97a235367f9d.png', '7f6ce40f306b.png', 'e13412678eff.png', '599b89048034.png', 'bfdee9be1f1d.png', 'cd5714db652d.png', '1fb455685328.png', '705f508d1e42.png', 'b64e1eef3d63.png', '33e8e26a75d4.png', 'a80dab8eddf4.png', '7131bf4c9e6f.png', 'a3802934bad7.png', '668a319c2d23.png', 'e724866f5084.png', '657859f893d9.png', '5c7ab966a3ee.png', '7bf981d9c7fe.png', '9fab29e69a6b.png', 'b72f59b85f7c.png', 'a88f68b0b114.png', '675de69373f8.png', '80feb1f7ca5e.png', '9d98a0b585f2.png', '3c28fd846b43.png', 'ee3fe7809e6a.png', 'c3789c1dab96.png', '143db89c11c8.png', 'a32b5ce3d48a.png', '74418f620068.png', '2fefb720869a.png', 'a4b8de38eac1.png', '302bcdb635ff.png', '12a82fc7d73e.png', '1e143fa3de57.png', 'dde43aa22ae6.png', 'cd93a472e5cd.png', 'b35cad8fe2d7.png', '457c7c927e27.png', '9287e57326d0.png', '269f0792f11f.png', '0c76fd494af6.png', 'd9bbdc33db83.png', 'b99afe7137fb.png', '887c26fc0e1f.png', 'b0f0fa677d5f.png', 'c704bd669f36.png', '7c6594b50690.png', 'b1197f2cc9b3.png', '80964d8e0863.png', 'd332d7b8a26e.png', 'a30a143a53a3.png', '04ac765f91a1.png', '3486f7096276.png', '80d24897669f.png', '03676c71ed1b.png', '1601c939412f.png', 'f06e7a9df795.png', '226c6ceb9185.png', '15cc2aef772a.png', '224c14366e11.png', 'ca2b54b95ade.png', '879744b9dc65.png', 'a9e984b57556.png', 'fa6f3d8bb1d5.png', 'ff03f74667df.png', '98fbe56dcc2c.png', '9eaf735cf01f.png', '2ba0b0d9bda2.png', '598b8f5b3822.png', 'c8823cdaf7fa.png', '4a5a6efc0bef.png', '91e82fe4e434.png', '44f4ae58990e.png', '3b185ac445d0.png', '5728b8aa98ef.png', '81371b0c01ad.png', 'ff52392372d3.png', '5e52c9fe676f.png', '1a7e3356b39c.png', '5152bf091152.png', '2d558de2cabe.png', 'b73d0bcd3d33.png', '1b8ad0afe9fb.png', '4f46d7ee61ed.png', 'c70d09370109.png', '40140a925c43.png', '4c60f6fcea75.png', '71f6a6e4620a.png', '75c180e04f65.png', '1ec95179cdfe.png', '4ef16a53d899.png', 'e97ecf4355cb.png', '98e44127872f.png', 'e79e10907295.png', 'ef7a4ed8d5d1.png', 'fd62bd0db4f1.png', 'bcb0498ed2c1.png', 'bfda2fd0533a.png', '5cde55f745af.png', '8d62ba9cb22a.png', '07083738b75e.png', '2f5c9cdfb333.png', '8543a801dce0.png', 'c9c563864ab1.png', '6dcde47060f9.png', '81b0a2651c45.png', '09f1111a388a.png', '7be1b9aa78aa.png', 'c76664770c07.png', 'bda7ff3b1562.png', '85cc6d636898.png', '54038e56131d.png', '848e66b9e199.png', 'cc839823755b.png', '3b2b91590590.png', '896ad584a841.png', '3c72f580d4ba.png', '07a0be6b347f.png', 'd866c26d76f0.png', '6d10709053ae.png', '57a5e4274275.png', '70d0392397de.png', '33e7bf536fc5.png', '2c827005b8f8.png', '07f5d7baf907.png', 'b96b518596b3.png', '0182152c50de.png', '50a2aef380c8.png', '74211a2b6dcf.png', 'c755a0c4edcc.png', '6d9effbcde78.png', '929cd3867815.png', '655cafb4c932.png', 'd1fb4efb117c.png', 'a8c950a99107.png', '1c47815f4a6b.png', 'ccd34029493d.png', '6165081b9021.png', '9274e75dc4d5.png', '54dc6e8107cd.png', '4c78d9d18da9.png', '84b79243e430.png', '8d9516ea3587.png', '8650d32f4a9e.png', '0edadb2aa127.png', 'd6f36ec5564a.png', 'c5431b81cbc9.png', '9f1efb799b7b.png', 'a95858e052d6.png', '9ba469af2980.png', '28dc010a0780.png', 'aa841de1ee82.png', '7526c59c36d3.png', 'a1872f9c0cba.png', '757e39293591.png', '22325552a4e3.png', '6966abf40b8c.png', '059bc89df7f4.png', '48543037d0b3.png', '6889bc64ab09.png', '4a05f81b3aba.png', 'd994203deb64.png', 'baaca2f7e1f0.png', 'eb175669d789.png', '5ca73d28f17f.png', '876deb29f000.png', 'b55d2ddb3e75.png', '8a25a080f28f.png', '77a9538b8362.png', '6294b378d09f.png', '21037f5c7790.png', 'b70e7c26f51e.png', 'a87f53bc984a.png', '51da6aebba8f.png', '7c3747c0b2c3.png', '5bf6f2958e53.png', '1da4a17c18c9.png', '42a67337fa8e.png', '0684311afdfc.png', 'f6f3ea0d2693.png', '780be525036d.png', '763ad1236efe.png', '11d8e5eaee5b.png', '64ac539f58cb.png', 'f0267c42907c.png', '0dce95217626.png', '86e7f98f73f1.png', 'e16fc934069f.png', '5fcff7280019.png', 'd15ca3469b87.png', 'f91cfa82b9d4.png', '07a0e34c8d20.png', '084c02cf077f.png', 'a9dc80cba9a4.png', '141735b57ec0.png', 'ad029ba7fa8b.png', '42c65af5ab16.png', 'a11c62cb3f86.png', '8e2a3978c244.png', '1c5e6cdc7ee1.png', '349f3c0ac83e.png', '8db2ce991101.png', '77acc2cafee1.png', 'e25ccfe38e44.png', 'ca7f5caddf96.png', '2fe06bedb2c4.png', '0e0fc1d9810c.png', '5a93c0f783c4.png', '5889a0c75cac.png', '0cb6b898389f.png', '663a923d5398.png', 'cbc23af521f3.png', '6f0463c1ff18.png', 'bf811911acf9.png', 'bf6cbccacf39.png', '4da6e2089d57.png', '48afe8c47454.png', '90a9a41eec6d.png', '9f285b3e57ed.png', 'be7bc89f5fec.png', 'de4cdabbce6d.png', '48a45619d1a3.png', '02358b47ea89.png', '523ff163211b.png', '58af2c054ced.png', '5a179c123fd8.png', 'a00b4cb250a7.png', '594f69b503ad.png', '6f689fced922.png', 'e4210e7fe587.png', '000c1434d8d7.png', 'a6356a3c5d11.png', '873dcc0b468f.png', '44c869174e3a.png', '2a47e5b21791.png', '7ec0e61a7e29.png', 'b498b84d383f.png', '06586082a24d.png', 'b56340f472d2.png', '9f4132bd6ed6.png', '2f9b66784109.png', 'e7b5dd5bab1f.png', '92f313287a29.png', '7c52fe73e748.png', '0afdfe5f422c.png', 'd1cad012a254.png', '8a67f1efa315.png', 'a62e995f167c.png', '7e980424868e.png', 'b0d35981708b.png', '7d0a871c45db.png', '4c6c5a1bf5ab.png', '80e7cc0a0649.png', '94076a9fb9b5.png', '28824d12d31d.png', '90c982cc2d96.png', 'c9d42d7534e0.png', '80dbeb0fdc75.png', '05e9126dfa5c.png', '4d21ce39c905.png', 'f2d2a0c92034.png', '5548a7961a3e.png', 'a82a12ad3fb1.png', '0ad7f631dedb.png', '1f543a86c4d4.png', '7e9458de5707.png', '2a2274bcb00a.png', 'e83d315d8f98.png', 'bf7221a016b5.png', 'dea7538bb91a.png', '9c72ed6befa0.png', 'b1c6f0997e27.png', '022f820027b8.png', 'fa9bece586fc.png', '9782c0489eca.png', 'b402b18d99a5.png', '8a87dd2a784e.png', '0ce062f26edc.png', '4189d4e631ec.png', '4aa07d720638.png', '1ca35d483772.png', 'aa0afc41ed19.png', '555d0bef3c5b.png', 'f080a22008be.png', 'dee1031a76ae.png', '1e742358e0b9.png', '0dbe6c26cedc.png', 'a6731dd737af.png', '2628305cbb29.png', 'ad1aa75d5630.png', '2a5a8b744f08.png', '2e26762daed5.png', 'ee6e39319b39.png', 'e9ce5bf645ab.png', '04d029cfb612.png', '3810040096cb.png', 'bba38f2294a3.png', '7e70344b0c25.png', 'aabd867043cf.png', 'd264396d8d1a.png', '19244004583f.png', '6324d77cf926.png', '3a1d3ce00f0c.png', '73d40ce06a67.png', '8c7c26c52a6c.png', '4faf4063db8c.png', '58c12863f33d.png', '486e852a3b4d.png', 'c6a2975228af.png', '61c2fbd16e38.png', '1b8701231c8f.png', '6c9c902a97de.png', 'ab686895533e.png', '49a4765f8822.png', '3cd801ffdbf0.png', 'fc1b1841eadf.png', '4ad8d3ec8789.png', '91cf56d3d1af.png', '0415fc68b176.png', 'ef48780f5d5f.png', '93a1b984de84.png', 'f460608cf4cc.png', 'b5204c0decc7.png', 'f90f8931a9bc.png', 'c568e5245ea5.png', '76e6a9238570.png', 'be161517d3ac.png', '8a81f62320d6.png', 'b7bd4a6627b6.png', '9ad92f1c1542.png', 'df3adfd6ba36.png', '6363b360aefb.png', 'e96099b961b4.png', 'eba3acc42197.png', '203275daf46d.png', '44f7f3ef9d50.png', '9dab2e6ba44b.png', 'd2c2f02bb313.png', '18af532e7e1e.png', 'd6df4fe492ec.png', '3ca637fddd56.png', '4c389d033cb0.png', '15f8d769935c.png', 'e5d56f4f359b.png', 'f00ce9b9d6f4.png', '175dd560810a.png', '61da799bf0aa.png', '12058bbb8299.png', 'a07d9a5045cb.png', '9c14ce27cbfc.png', '9b4fc15df3c8.png', 'a1edf0e66592.png', '295fdc964f6e.png', '9b70f84400af.png', 'ab3c505b624f.png', '7a77c3eb468c.png', '2d9d97a6e713.png', 'bc92a61a1f9c.png', '68ddb15a74de.png', 'd16398c971e9.png', '3254e48c8aa0.png', '77ab222bf85c.png', 'df6d13d04da1.png', '20d5fdd450ae.png', '29a13e666266.png', 'fa748b57262b.png', 'ea5c42a78979.png', '0efc93ec838b.png', '97da093947e8.png', '7356dd08b0ae.png', 'd2dc86021c67.png', 'd16e39b9d6f0.png', '7aabd768abff.png', 'b351ae99413a.png', 'b0c9a492e068.png', 'aa9b8f05f4bf.png', '71e43b4f8ba6.png', '58eb3809f456.png', 'f8d62557ad0c.png', 'd1fa0f744620.png', 'd5ad3362424c.png', '240b25a7debe.png', '291f581d365e.png', 'b0acd3593310.png', '7eeb191ad06b.png', '2b074afdf626.png', 'ad570b850a4f.png', '0024cdab0c1e.png', '932181b93b2f.png', '3abac0961bfd.png', 'cd972e5639e0.png', '6d6be4cfc73f.png', 'a66c3165876f.png', '1120f6d08d95.png', 'd1a24527a15d.png', 'aa31bc6b8f4d.png', '348598d01e18.png', '46f56c38051f.png', 'ae2e888905ba.png', '4a7dc013e802.png', '64a13949e879.png', '53273d664cd8.png', '4731e708ede3.png', 'a2163f0c2af5.png', '207a580de0ea.png', '6efa36d59ada.png', '4210809074c1.png', '1caba2fb38f6.png', '5ca779ace6e7.png', '75369248dba0.png', '1116271db4ea.png', 'c947bb6cf9f6.png', 'dbee04ae6426.png', 'b90bc89ce8d8.png', '683023cda6a5.png', '50ddd7d976df.png', '1bea04b2bb2d.png', 'fa7fa797c650.png', '807135cbc438.png', '849a91e9ab28.png', '4aa3d771c5e4.png', 'f3bb996b45ce.png', '75a071608ea6.png', '799214e8b07c.png', '04aef84a2cc1.png', 'beb00fa6e7c9.png', '3ee17aa12e46.png', 'a8c9fcdbc0be.png', '7a3ea1779b13.png', 'e251bdf05b85.png', '4eaf2f81819d.png', 'a93f1ea3ff4a.png', 'a61723fc38c2.png', 'd990a3f0cbdb.png', '9a1029536d78.png', '47e51065b819.png', '66bfec8d6bcd.png', 'a2811f512c1c.png', '9a9b21215c55.png', 'c280730cc211.png', 'a07efb1ecfc0.png', '5879285f9d8d.png', 'a987aa7aac37.png', '58b866484a05.png', '101b9ebfc720.png', 'afc345cc9145.png', '6f3b62e5b7f5.png', '7b20210d9120.png', '15e96e848b46.png', '4276b82e4489.png', 'd4f32b9c07df.png', 'f850cb51fdba.png', '26463a5fb949.png', '9be71d6d7e59.png', 'a0b7ad98df57.png', '02dda30d3acf.png', 'd97911a32918.png', '5a27b9b2a9c1.png', 'be197b663520.png', 'a0fd94e2ad76.png', '4036471a1bb7.png', 'b74de20d73de.png', '12bc439d373a.png', '27933cdbe0cc.png', '3a61e690f4bb.png', '312694ea8e6a.png', '529906ff9dfa.png', '9904939ab83d.png', 'f69835dc7c50.png', '4ffa38550c95.png', 'cb75210abebe.png', 'c81c6911f5e0.png', '436fa3fd145a.png', '4554062fa836.png', '789434d095d1.png', '2a4520f1f9a3.png', 'c4e8b1ec8893.png', '6d454444f17c.png', '5bea250d8bf5.png', 'ba25f947f4ec.png', 'e4dcca36ceb4.png', '15528e740543.png', '22a6da005395.png', 'c58e5c0c5b33.png', '7e160c8b611e.png', '6c30dd481717.png', '60e269e3e188.png', '5321ab64f9ea.png', '1e1fb019710d.png', 'cd54d022e37d.png', 'fecf4c5ae84b.png', '191cf5668f33.png', '2700754f71e9.png', '977e1ca77653.png', 'ff1e940105f9.png', '1a90fad9ffa2.png', '3c311c9109b0.png', '482088e6be44.png', 'f252046c0fe6.png', '388279491b5d.png', '45693d027798.png', '99c626e58464.png', '3b4a5fcbe5e0.png', 'dccdf750c962.png', '66a0bf258013.png', '387138ddf43d.png', '2f7fbdcc9a4b.png', '05195a3db5e2.png', 'd364423ec6f9.png', '0abf0c485f66.png', 'adb56cecafaf.png', 'c446985355f1.png', '86d6808f0609.png', 'c0202976c670.png', 'a14bbd9a583e.png', '0423237770a7.png', '1db0393cdbc1.png', '55eb405ec71e.png', '38c7153457e2.png', 'f9aa35187bf3.png', 'ee059945b08a.png', '57760be09c03.png', '62ab144d5cee.png', 'e42d9a94a66d.png', '9da74370835a.png', '5b994ff78547.png', 'a528be013a04.png', 'a8e88d4891c4.png', '178412895d5e.png', '369229040a34.png', '7831ce1d895e.png', '1df0431bfa73.png', 'cb39761f0712.png', '70d657f8f503.png', '3dbfbc11e105.png', 'f366fb1cc475.png', 'e12f9f19d1be.png', '10eefba568dd.png', '7d48f8cdfb69.png', '9e2ba2b979f1.png', '7bc00e58d419.png', 'b3a994760537.png', '65a3b13ad9a0.png', 'c613db1cab27.png', '4b6cb0bcfd44.png', 'ff0740cb484a.png', '840527bc6628.png', '91b6ebaa3678.png', 'c73c5f6ef664.png', '77baa08a1345.png', '53f6c1c65c04.png', 'fac399455195.png', 'a9a3225cf4b5.png', '24f271c87e73.png', 'a75bab2463d4.png', '2bbcfdc477db.png', 'c98f623d08d1.png', 'f4d3169b468a.png', '67d8f94f04e0.png', 'fb88d23fc5fe.png', '5d4e5fd34d91.png', 'b95d4dd8e5e2.png', 'a0445785e2f7.png', 'f09cfc6a4dbd.png', '5ed6dc419e4d.png', 'c96f743915b5.png', '2c77bf969079.png', '40e9b5630438.png', '1cb6961d141c.png', '190a309f2cc5.png', '35ac70c0d08f.png', 'd91273efb92a.png', 'c7d0deb71576.png', '4350a1b2f3cb.png', 'af6a1508cd95.png', '00cb6555d108.png', '1c4f3aa4df06.png', '73ba798fee25.png', '0b8bdec9d869.png', '32a3eb37ff40.png', '66366a90d1ef.png', '9f37c98b8187.png', '9ac41b9a809e.png', '4294a14c656a.png', 'ab7991df166b.png', '69b3a00927fc.png', '3bf3085ac167.png', 'eabc7c716255.png', '780f9c237c56.png', '2d552318eb07.png', 'a06e41bd2634.png', '2c1d5be654dd.png', 'fce93caa4758.png', 'd39752cb6e57.png', 'b9127e38d9b9.png', '992b9a07b25f.png', 'dd110d2b8c21.png', '1a1b4b2450ca.png', '28f93cad89c5.png', 'e7a7187066ad.png', '677f087cd697.png', '0cae727cf119.png', '5a36cea278ae.png', 'fed5bb685832.png', '9bafbbd152d2.png', '9ab18a4a957f.png', '5f70ad48a525.png', '4e0656629d02.png', 'f3b6b7ca1eb1.png', 'dbfd238b3468.png', '2d870833c0c9.png', 'ac81fc200162.png', '665ce639a331.png', '4818672273af.png', 'e03a74e7d74f.png', '215d2b7c3fde.png', 'beeca5f14618.png', '405085b53d7b.png', '87295c5fa1cc.png', '10a5026eb8e6.png', '55968f0e63c4.png', '17eff993386f.png', '821789e9053f.png', 'a3ad6c2db6f1.png', 'a7ec056502e7.png', '1006345f70b7.png', 'd95959798b57.png', 'cc671a73e1cb.png', '686ed1dbae20.png', '82088c6734e6.png', '27f82ada84ac.png', 'e04f3c6619a3.png', '419406328dcd.png', 'e7fc93ac5b6d.png', '4c3c1ed09771.png', '338326891d84.png', 'ca7570c5925c.png', '81704925f759.png', '9ae54843c69a.png', 'be7f791a7877.png', '38e111cac46f.png', '354b8911d6ed.png', '2131aa3a1e6f.png', '1c6d119c3d70.png', 'a2dff8dbc9f8.png', '9ed666e982cd.png', '6b664ed2a938.png', '92e3d608fd3c.png', '1df0a4c23c95.png', '1c4d87baaffc.png', '89d9c071a56f.png', '166068a24416.png', 'dfc7ec7db0e0.png', '0083ee8054ee.png', '45c39ab9e797.png', '5a5d3798c357.png', 'cadde4030858.png', 'b6304c545f95.png', '3dbc90c7ee7d.png', '383e72af1955.png', '6e92b1c5ac8e.png', '7a0cff4c24b2.png', 'e82232a3c28b.png', '18ce0cdc473d.png', '25b4080f598b.png', '1dbdc32c17db.png', '4dd7b322f342.png', '93be637084a2.png', '9f8112c710be.png', '2cfe8703f265.png', 'e811f39a1243.png', 'd3e884109b45.png', '63d217b059b6.png', '5069feccd866.png', 'bde1063a5dd7.png', 'eda29a9d78f3.png', '91a88d3b0358.png', 'a476fd984005.png', 'a7c10ca6c117.png', 'a2b97d98f130.png', '942f544c4e15.png', '8dfa629ca74e.png', '8f9819752ca0.png', '4d47300e3ddb.png', '999ad827ed35.png', '540e4973829e.png', '6b30767595d8.png', '8bc6716c2238.png', '33b893e18eb3.png', '94145d1f42cf.png', '3d2ecffe0386.png', '3730c322d35b.png', '609be3ca5ddf.png', 'c6c2bad91f23.png', '5db895d3f1fc.png', 'a8854768549f.png', 'd10d315f123f.png', 'cbd0870aa933.png', '6194e0fff071.png', '8cb6b5b2f19c.png', 'd83d0695e215.png', '9be0683649ff.png', '33b978734eab.png', '547b37da9223.png', '521d3e264d71.png', '15f440753916.png', '1ca91751be4d.png', 'd1cf31577a59.png', '25002fe43f92.png', '6107a2e9f60e.png', '150f92b45349.png', '8fd7ad26e691.png', 'e2a233493b90.png', '4d1cf360b2d7.png', 'aa9cfe639ef1.png', '7c629b491d1a.png', '37c4dfe03aba.png', '415f2d2bd2a1.png', '5eb311bcb5f9.png', '8fbb2ca39911.png', '2241b7e90782.png', '43bc7c066dfb.png', 'd81b6ed83bc2.png', '8a234d68b27e.png', '0231642cf1c2.png', '59ee65760535.png', 'a804cef3e51f.png', 'c58971bcebb2.png', 'e66855a5c583.png', 'e33766353db2.png', 'aafe980edd0c.png', 'd29b37d110f3.png', '2a3a1ed1c285.png', '4958bfcc9f38.png', 'da44f80b422b.png', 'aeccef0bdc26.png', '737ef6226677.png', 'a2ad3da4c7d6.png', '628b581aa905.png', '233d948e2544.png', 'b11dcdcbc8c8.png', '31360e44ac64.png', '6b869f37cdf3.png', '56e56aa08362.png', '5090917a2676.png', '1c0cf251b426.png', '4205e9deb058.png', '1db18bdd43aa.png', 'e1dc02a3dc2a.png', '4f6abc40c72d.png', 'b0cc9f8d06e4.png', '69f43381317b.png', '8201cab8322d.png', '0924cec998fa.png', '9785805af1b8.png', '789c60cba801.png', 'd7ab5c040294.png', '2b3a4a81d748.png', '50915e2329a1.png', '3b5dffe159b6.png', 'e1418d28d668.png', '8ab3faa3701f.png', 'eadfc8809ec8.png', '94a67ec0714f.png', 'fe06dad6851c.png', '837acf120946.png', 'ea9c41e1ced0.png', '653534ded339.png', '7b8c78b41c0d.png', 'be521870a0ea.png', '6bb30ec3231a.png', 'f9d52509c571.png', 'd968a983d4d2.png', '7b49041cbf17.png', '4826d10030b3.png', '87b1938994b5.png', '803120c5d287.png', '53c874dbc594.png', 'c62cef02efa2.png', '4393c5bc576a.png', '77e15f213b04.png', 'c261b1aaa828.png', '55eac26bd383.png', '98f7136d2e7a.png', 'bb2f89488ecd.png', '5a11d21c2828.png', 'e12b67835e03.png', 'aa8a1e814811.png', '8596a24a14bd.png', '3f752fcccec0.png', '8bad12d70368.png', 'bb7e0a2544cd.png', '03e25101e8e8.png', '300305ce82d2.png', '9c52b87d01f1.png', '57f933d3d7c7.png', 'f8372e80f731.png', 'f71bea807c96.png', '78b3f819dcc5.png', 'be21d8b60e2a.png', '84b472c49cfa.png', '0c55d58bebaf.png', '51af8c112682.png', 'ad3fc5076852.png', '44e0d56e9d42.png', '66393d8c60ba.png', 'd838d5b9f571.png', '3f5b4c2948e8.png', 'a9b3177f01c0.png', 'b82dfa63a75f.png', '7d1da90d3ca9.png', 'fe3b0e50be78.png', '49c5e7f6b8d2.png', '7d261f986bef.png', '42cc993f23a9.png', 'da0a83f074f3.png', '8a01daa423f7.png', '51a1d162e223.png', 'e740af6ac6ea.png', 'bd5013540a13.png', 'ff4955e76894.png', '1dfe599d12a9.png', '875d2ffcbf47.png', 'c013e869acce.png', '262ad704319c.png', 'a4359815f152.png', '8a8a251770cd.png', 'a76b69e443ce.png', '0981195eb9fb.png', '345b1f0abbba.png', '82d364726a58.png', '65c958379680.png', 'ef8109305128.png', '910bfd38e2f5.png', '94372043d55b.png', 'd2901144070c.png', 'a53d6d2472a6.png', 'ead23cc922ed.png', '189cbbc9e5e3.png', '38b9bb961847.png', '0fcfc6301f3d.png', '1ade1e949383.png', 'ca9c912ebad7.png', 'b99c825b93c5.png', '09935d72892b.png', 'f481f76a6b75.png', 'a19ecd0a706e.png', '0cecc2864b7f.png', '0161338f53cc.png', '7435e9a3e36e.png', 'b574d229ec4c.png', '64b9206afb3f.png', '1e4650743fa2.png', '51030843fde2.png', '26cd40b57ad1.png', '83a63c4a3e4a.png', '7b87b0015282.png', 'ba0107fb1bfd.png', '726dff37edc0.png', '8ef2eb8c51c4.png', '5b301a6d1ac7.png', '393fa5a023a5.png', 'a627eb8c08c5.png', '09eeafa9656a.png', 'cac80797770f.png', '315c1a0d87fd.png', 'fbcbc81cf9be.png', '2fdffb6160a6.png', 'b2ffa3e18559.png', '6c6efb6b1358.png', '568455854a11.png', 'ca05f7e7801b.png', '2399d68d407f.png', '0ef4c61dc056.png', 'd66ccb75ada1.png', '2f7789c1e046.png', '1efa5d443707.png', 'e5de79795c1d.png', 'f5c953bee7cd.png', 'c6916bc42016.png', '65f69234c8a7.png', '63363410389a.png', 'e26d8718ca58.png', 'a188c60b93fb.png', '7335a2d43ada.png', 'd85a842d20bd.png', 'c23ff6dcf15e.png', '1b862fb6f65d.png', '382752f6694a.png', '2f143453bb71.png', '9e5737f771c3.png', '59f3f70abddd.png', '73a07e2ea23e.png', '3d663a6a50a3.png', '014508ccb9cb.png', 'eb1d37b71fd1.png', '6c250a30593b.png', '8714d17bb6da.png', 'fcc32dffd24d.png', '352e4a939242.png', '936299166bea.png', '8000a6b97a84.png', '9d62478042b6.png', '0790515cf5af.png', 'e3cd96cb094c.png', 'ffa47f6a7bf4.png', '89fc080f7e83.png', '1a369baf9ee6.png', '10de500cf0c5.png', '692e946b1f85.png', 'da0a1043abf7.png', '49386d603494.png', 'aebe87a423c8.png', '913490237ad4.png', 'd7e5fe5245e0.png', '5cab3ef4b31c.png', '9b418ce42c13.png', 'ac2c814949f9.png', '86fbac86ed3e.png', 'a86c6283fd78.png', 'ae20112e7a1e.png', 'a94da3d3b5c0.png', 'af828dab3ffc.png', '191348830ddf.png', '6bf26b777e3a.png', '7eee3d1f1268.png', '691eeb59b4cb.png', '232549883508.png', '42b93b574f23.png', '916ec976ff30.png', '460893cd86e3.png', '8f14bca04b47.png', '721214151233.png', '3b232b394e4f.png', 'b5b913358b32.png', '246e4506824a.png', '97f290d31813.png', '69c4cbb630de.png', '0d744aed4d64.png', 'eda1d75cbcf0.png', '423abbaa5fad.png', '2a8a9e957a6c.png', 'ff59d44a70a7.png', '8e7981855125.png', 'de18071c36e6.png', '9122b31414d3.png', 'e96bd80a8a53.png', '39134907127a.png', '0eced86c9db8.png', '5288f7441f64.png', 'b402daa0864c.png', '37c523296d42.png', '812d5adafaf2.png', '261c6bd63bff.png', 'e8e44b3160e3.png', '3fa4f4d77177.png', 'bfe467b7e997.png', '08c17a2d95b7.png', '48c72dec46e5.png', '05113073b268.png', '0709652336e2.png', '5a444c32cd9a.png', '172df1330a60.png', '5257cb536da2.png', '51aa3361294c.png', 'bc34ed91c9bc.png', 'e34fa07bd64d.png', '2a3378bcfbcc.png', '0f882877bf13.png', '4f60129e9a5b.png', 'a45c30da0c72.png', '58529a8638d0.png', '6089fa333013.png', '7ba6b23c4b46.png', '7fe7309d0b4f.png', '1db6bb46c102.png', 'a15652b22ab8.png', '85fce24084da.png', '682312e82ee3.png', 'f531232ecb55.png', 'cab3dfa7962d.png', '8bf2d925dc0c.png', '98104c8c67eb.png', '6d4f6c9a8406.png', '4360a112db10.png', 'e246cd89e1cc.png', '1dd9adcbfff4.png', '83038ca49b6d.png', '4d167ca69ea8.png', '29580bed2f7d.png', 'c48ae5da188e.png', '4462fba1d2a1.png', 'c5a6f432a1ec.png', '71e4130bf5c8.png', 'd3e56584a481.png', 'c56e65f74187.png', '3b9817a39adf.png', 'ed2c06fcc573.png', 'a73d012c4c38.png', '996f9bba4ef0.png', '9bd008aab548.png', '8af50c9d0a86.png', 'e037643244b7.png', '98441214557f.png', 'ca1036496659.png', 'e2c3b037413b.png', '6e68e742f5bc.png', 'b70cb31b9abb.png', 'b50b30aa6e6c.png', '0bf37ca3156a.png', '5712e2aa73a2.png', 'c1799a6f5c65.png', '8660e1864665.png', '549381330191.png', '1ae8c165fd53.png', '76516f828d88.png', 'd27ac9e54901.png', 'ebf4b22240f4.png', '2f8d14a7d390.png', '384db24ebbd7.png', '8c29a76fa08c.png', '1eee55494271.png', 'c5ad60521f8c.png', '5723d0ec895e.png', 'f68690db78d3.png', '906d02fb822d.png', '7c2f820a6425.png', '6733544ae7a6.png', '76c0c7e1b6cb.png', '263d8851e33b.png', '2682e6da9050.png', 'ee36ca728641.png', '1f31701dd61b.png', 'a1b28bcbce00.png', '5633ced07d8e.png', '9c893e16c055.png', '2bd4d4fbed5c.png', 'db6207e62c7b.png', '83517eaeccb9.png', '3e86335bc2fd.png', '4ccfa0b4e96c.png', '4029d70e9d8a.png', '14ee87d6cc42.png', '89b725411cee.png', '7f2cce721e19.png', 'a06b353e7bed.png', 'd1f1ea894da1.png', '65a7fe9482fe.png', 'be8697eb2078.png', '6f923b60934b.png', '6cdd0f985270.png', 'd667af5742f6.png', '7d1b40fdbd86.png', '6f0e5848d9ce.png', 'e0313be77035.png', '38373431d996.png', 'd807c53c1399.png', '4bd5d0b30198.png', '135575dc57c9.png', '3ffa14d60b24.png', 'f61bf44c677c.png', '7526cf435753.png', '8329e80c10ac.png', '1d11794057ff.png', '61ac9b0dc6b9.png', '7bc4dd99eee5.png', '3128eb593012.png', 'ef26625121b3.png', '93802d1e3c41.png', 'd871895742b1.png', '060e00d1e2ab.png', 'a879c3569552.png', '331121c65e88.png', '0d310aba6373.png', '39f8935185e6.png', '5905a9b06a73.png', '905cc86bf100.png', '92fcf50b3562.png', '904b03ad5594.png', 'b310bd564329.png', 'b1f4122fd36a.png', 'ea6a53e54d0f.png', '111898ab463d.png', '19ef4d292196.png', '542964865b1e.png', '916915f01e17.png', 'da2bdf4236ac.png', '0e82bcacc475.png', 'ba08cee68c71.png', '0ca0aee4d57e.png', 'b6fd109b1bc9.png', 'ac0a48ccbf70.png', '44271f3cb18f.png', 'c4aef0d88d1b.png', 'eebd1e195952.png', 'f994a3b07935.png', 'ca360bec5851.png', '4cde86044ad1.png', 'aef9016557ca.png', '5f13e8a07344.png', 'bd9904495ccd.png', '43e9c66eb0f3.png', '77b7b71ebcc3.png', '4b618537d52f.png', 'ab50123abadb.png', '4e43d05cc2ef.png', '1d2472849dce.png', 'fc4d69128e7c.png', '2463bb04ebc3.png', 'ba2ea9182090.png', 'e9286ddf6ffe.png', '8af6a4e5396f.png', 'af87d48ffe01.png', '08b6e3240858.png', '7005be54cab1.png', '53327edb9e4d.png', 'cd66754e1b3b.png', '9c514d2d5b3f.png', '1fd5d860d4d7.png', 'de16416220de.png', 'e6a5e4718873.png', 'ef99c499d665.png', '25e9fd872182.png', 'a11bf2edd470.png', '28503940d10b.png', '370f575adb23.png', '44855f666225.png', '186c1835eec5.png', '51d574513bcb.png', '7b211d8bd249.png', 'f066db7a2efe.png', 'd2afca74cbc3.png', '57ce57a8cfb0.png', '252305189b3a.png', 'd6228d951958.png', 'bf7b4eae7ad0.png', '2b48daf24be0.png', 'fe0e2dee1834.png', 'e68bdd36e589.png', '286e9981dd9b.png', '478fc46eaa49.png', 'a56729de89e9.png', '248139c423c4.png', '7a12f49e29df.png', '5f4a8c074bd5.png', 'a763661f98a5.png', '58184d6fd087.png', '1ee355480567.png', '1d29cb2f4296.png', 'ab1c20a94f3f.png', '0cbcc7b23613.png', 'fa59221cf464.png', '11242a67122d.png', '668e853258cd.png', '52edbe29d655.png', '9910c827e2fe.png', '523b3f0fc646.png', 'd18aea8238a0.png', '6a57a3db3eff.png', 'ef4121e9bb67.png', 'd2c6b99ef62c.png', '094858f005ab.png', 'a8c54e2a4b79.png', 'bdf47b9f10c4.png', 'a654b25124c3.png', '70ed3ec68b94.png', '6cffc6c6851a.png', '17e6116b89b3.png', '4a96c28f3f07.png', '7d8f67cadc29.png', '82f2784ead76.png', '91cbe1c775ef.png', 'a646c084928c.png', 'af345c68e836.png', '5c817060c0ed.png', '23148a40ecb0.png', '6c315ad3d07f.png', '37de05ef12a5.png', '7f60f2a083d3.png', '66cd9c28e636.png', '1f4bf8e28b41.png', '9de9421f17e3.png', '8d4ff745a409.png', '00e4ddff966a.png', '8b568d47a1fd.png', 'f62b8a076833.png', '79cbae28d8b2.png', '4cfd22ae43d4.png', '971bb98ab935.png', '83e529e95b0e.png', 'f35d80bb1a22.png', '331b87d1b9d1.png', 'b665041e1633.png', 'b376def52ccc.png', 'ceaa5803d780.png', '17f6c7072f61.png', 'db49cdf1ea64.png', '1e036f2e7095.png', '318eb706a134.png', '3601dac9bed7.png', 'a2d349f567a6.png', '9c5dd3612f0c.png', '5511f114e7ee.png', '6de39b94f634.png', 'af8aa32beee4.png', '541db13517e2.png', '080f66eedfb9.png', '75a4343b12f9.png', 'd57d1be1bbd1.png', 'caec68f11c86.png', '247ac63e5510.png', 'c639d837f5e4.png', 'e580676516b0.png', '5ce5eeaf757a.png', 'c57c164bca05.png', '3ce2f8a77a32.png', '82910bba4753.png', '907aaff827e5.png', 'ec6f1797a25a.png', '9870ce41cac4.png', 'd9e58e4d8689.png', '475c7ded0f7a.png', 'bf18ff30a8f6.png', '070f67572d03.png', '8185ce1cdcef.png', '0fc6829da85b.png', 'bd375ba756b1.png', '4d9fc85a8259.png', '435d900fa7b2.png', 'c0f15fe3b4b7.png', '753b14c27c83.png', '3f49f8d100e9.png', 'e19936582c61.png', '2ef10194e80d.png', '1ab3f1c71a5f.png', 'e07045d7c5f7.png', 'e60e4edb3ca9.png', '650fbed3fdca.png', '51cfeccaf40d.png', 'fd4c946c52bf.png', '6b128e648646.png', 'ca0f1a17c8e5.png', 'cc964bf04dbc.png', '46cdc8b685bd.png', '103f97a2ab15.png', '63c0eafd6aa9.png', 'ca25745942b0.png', 'e7defafeb957.png', '36a1e3c780a0.png', '702de9dcde32.png', '883ddb650967.png', '6fb656d506b2.png', 'b3c0c3330278.png', 'addf66a50f42.png', '28a4d00927b7.png', 'c80b0f27541a.png', 'ae94ce412de9.png', 'b294927b14b0.png', 'df4913ca3712.png', 'a484bdf85b4c.png', '63a03880939c.png', '937bc1b924b1.png', '1c13a1483f4a.png', '20f86e068276.png', 'ab653b8554c0.png', '18cde9649e90.png', 'a182b5b191de.png', '500aad15b7c8.png', 'd26bb2ed6e71.png', '4926dea289f8.png', '4e85aa647534.png', 'ec01f0862669.png', '99e8bf998285.png', '3f6c627e2ff2.png', '4b6895d0cf8d.png', 'a821b6ecef33.png', 'f86d1c404acb.png', '3796af4d987a.png', '99ecdb41d5e7.png', '8eb3337a54e9.png', 'ef476be214d4.png', '9039cbfcbb2f.png', '99c6a123ed6a.png', 'a76132e79688.png', 'da6bbb76d562.png', 'd1ca85af57c9.png', 'f09fd9433dff.png', '8c4ceddeb1c6.png', 'b0619ca93a5f.png', '150fc7127582.png', 'aca88f566228.png', '050bb1eafa76.png', '44ecf3f4efa5.png', 'e5f332efcbc7.png', 'cc12453ea915.png', 'af6166d57f13.png', 'bc7bf19b84e3.png', '404ede327e98.png', '5ac7a414560e.png', '6d3d1fe6c32a.png', 'b7a1bb106051.png', 'f9e1c439d4c8.png', '6b3860e8f64f.png', 'd5b4705ac2ee.png', 'a8652b2de23f.png', 'db4ed1e07aa3.png', 'b16dd4483ca5.png', '3f73c91b7e32.png', 'cec299c2a2d5.png', '18f1f979d30d.png', '10ecc5292ab1.png', '0fbbd665431f.png', '218c822a3dd9.png', '26e231747848.png', '537e5c578f40.png', '7ef5ff774a48.png', '2fde69f20585.png', '11b220a397b8.png', 'aea59ebec445.png', '10bf25731c08.png', '82deb07a6618.png', '50916d67bb51.png', '6c3589d7ed8d.png', '8d13c46e7d75.png', 'b9fe7da14a32.png', '237aa50edc34.png', 'e821c1b6417a.png', 'a721efb1e049.png', '2f4e81787d9b.png', '1e9224ccca95.png', '71c1a3cdbe47.png', 'c5bec7f1e5f3.png', 'cac40227d3b2.png', '835b9f6e12ba.png', 'dbb2c63f6f08.png', 'b746a6681ba9.png', 'ee3f5cf52188.png', '5db2e3a4594a.png', '29192375ab1b.png', 'a88365134c3c.png', '7f2123bc89a3.png', '1ee1eb7943db.png', '42a850acd2ac.png', 'b72a86d61959.png', '51405d042000.png', '152db3de8120.png', 'c9f0dc2c8b43.png', 'b6d9974443ce.png', '72595230840c.png', '8b8fe3fc8950.png', '84b4da14bc23.png', '624fb7317106.png', 'e0863b353093.png', '08752092140d.png', 'dcc6c0ad5cad.png', 'b66f23ffa730.png', 'e03e70bc8bba.png', '7347f5133a6a.png', '6a91eb157f47.png', '944a233fbf8e.png', '0babc12807b2.png', '2776d70724d3.png', '3428230bf1bd.png', '2dc647e00ad3.png', 'd10ef306996b.png', '42b08dca9b2f.png', '883c6a184f16.png', 'a0267206d51e.png', 'b0b3b16fc305.png', '535682537302.png', 'e599151ca14b.png', '12ae44be0d38.png', '8ab8d9b3ce3f.png', '93d6d20a5ee3.png', '0773a1c326ad.png', '86722fcd802c.png', 'c5b58cc992af.png', '3323fd59782e.png', 'aec51513cf45.png', 'cae33655ca00.png', 'be3a7d9e981e.png', '107aea0d9289.png', '76e589911303.png', '5d74f98d62be.png', 'b5834ee64541.png', 'd56d32a1d62d.png', '8dfff47b06b7.png', '869bbd3170cc.png', '3c726de3ee90.png', '1414128bead0.png', 'ce207b69ff37.png', 'd85ea1220a03.png', 'e52ed5c29c5e.png', 'a790a3b36390.png', 'f7508f14dd7b.png', 'fd0a70082e7c.png', 'e31c42a8652b.png', '992599744a23.png', '2f42e20db938.png', 'd774692d9919.png', 'e4a44f9158dc.png', '45ae04cfde5d.png', '941d874c8afb.png', 'a6c9e96a10d7.png', '92d9e9f08709.png', '2376e5415458.png', 'df5ce3ea7820.png', '236f56771ec6.png', '17d7d6b092f4.png', '89d2a7403a06.png', '0180bfa26c0b.png', 'e160a3b19911.png', '6c00dd8bf708.png', 'b91ef82e723a.png', '46c1548d730e.png', 'de55ed25e0e8.png', '73881f55a3ec.png', 'f0f89314e860.png', '04579e31e4be.png', '8010c931321f.png', '437cbec4a3f8.png', 'b8ac328009e0.png', 'bb11db08584a.png', '2f2e1949ad56.png', '21d18b022429.png', '76be29bb30b2.png', 'ee02294cc3d9.png', '165c548185f8.png', 'bf87aedf2489.png', '8d3d67661620.png', '537e50fdf22e.png', '07d8db76b301.png', 'cb02bb47fdc5.png', '19b0e3c734f5.png', '2b07790a2422.png', 'a02dfd67a925.png', '983c98354e9c.png', '05339950962e.png', '86410aa13b3e.png', '405b4f78658f.png', 'c3cd0200df79.png', 'ea7e21bab610.png', '441affbe99aa.png', '65f5d2a6eb7e.png', 'ceb601fe8dba.png', '35cd9832fc0a.png', 'febfb20dc311.png', '74eee788edee.png', 'bf9cba745efc.png', '973b0facfa9b.png', 'ffc04fed30e6.png', '3cdef7c591cc.png', 'abf0f56c6f12.png', 'e4151feb8443.png', '02cd34a85b24.png', 'e907d23cce3d.png', '64bad93fde3f.png', '64eb5a79dfdd.png', '97bf61736b86.png', '80f6b30ece8c.png', 'eb050765b323.png', 'a8eb35b3bcd2.png', '1c7a013eeba7.png', '494fc9c745a3.png', '5b644a403e1f.png', '6735931000ec.png', 'e1e490773462.png', '29f9e1ac9507.png', '20404ec7b518.png', '005b95c28852.png', '01d9477b1171.png', '3dec415b188a.png', 'c25e02b39c01.png', 'aeed1f251ceb.png', '4289af3afbd2.png', '0da632ca45e0.png', '61bbc11fe503.png', 'af5a0bc4e1fa.png', 'a81b06f50612.png', '09c8323c612e.png', '7af4d8704032.png', '7ae69d22075a.png', 'a01024054596.png', '6dfd80748e72.png', 'ad944bd56bb6.png', 'b0d6417bad3e.png', '9d9bfefa809c.png', '956765d5f46d.png', '07122e268a1d.png', '6155cf375354.png', '63b71347e95d.png', '530d78467615.png', '80c67efc8101.png', 'cfdbaef73a8b.png', '1e8a1fdee5b9.png', '16ce555748d8.png', '658ad9f09f5d.png', 'c2a58b2cfd0b.png', '582115961a3d.png', '9bf060db8376.png', 'b6bfe9db60e5.png', '67f5d89da548.png', 'a5c9a8c726b2.png', '4c9f0fdaaef7.png', 'e582e56e7942.png', '857230f64a2e.png', 'ee78ce914066.png', '94b9ccc73bb9.png', '0dc8d25b3f69.png', '772af553b8b7.png', '06b71823f9cd.png', '949710bead24.png', 'a384e688e228.png', '75238d945315.png', '2cceb07ff706.png', 'fc782722a50c.png', '49e4b95ee2dc.png', '44a86263117b.png', 'ad12cde115ab.png', '4318b6adeb97.png', 'aeb6f4fd2eed.png', 'a0d04a19cf40.png', '79540be95177.png', '200d947f75db.png', '5ead17e894ae.png', '68987fb159ab.png', 'dcf109df1a2b.png', 'cd314653a4d8.png', '60aa4e649abf.png', '13d71389563f.png', 'd2fb715b0c41.png', '498f143c0374.png', '7d626a7ffe76.png', '4dbce359d0e1.png', 'b9d0b83d70c3.png', '9cf7c1349673.png', '0232dfea7547.png', 'bf7047dc683c.png', '08c60c647673.png', 'd6803e467592.png', '259d30f693b6.png', '8860c7b11530.png', 'fe37f4492920.png', '9bbb6c455913.png', 'd69698f838db.png', 'f23902998c21.png', '3fc219927a97.png', 'a32886cb31ab.png', 'b22354b5f94b.png', 'e68746d426b2.png', '4ef7144e24ff.png', '2ef4a04aed1b.png', '4df6a81b476e.png', 'e8ddfc9709ce.png', 'd473f6fafba0.png', '03ff7d159f10.png', '83fda7c0500b.png', '0369f3efe69b.png', 'bcdc8db5423b.png', '5d3c8c1f57da.png', '5445255635f0.png', 'c6f5b5b5be41.png', '2974c6ad1d58.png', '51a078d6d43a.png', 'fea14b3d44b0.png', 'f901d460517c.png', 'a7b7dc8788b9.png', '1b495ac025b7.png', 'bac1744955c2.png', '7e0598cc88a0.png', 'a386ec9aabde.png', 'd7a01fca9838.png', 'a8b3c0961d42.png', '8448af27ba07.png', '891329021e12.png', 'f69400b316a7.png', 'f0098e9d4aee.png', '3a1ecf5e2839.png', '0fffa73e2402.png', 'd74ccc796517.png', '6ba5ed791444.png', '76e5b50f95a7.png', '30941b65348b.png', 'f6f433f3306f.png', 'c3e02d4a1798.png', '4fecf87184e6.png', 'b2b79b37d314.png', '72c31aa48e2c.png', '164cd5a3a6cd.png', '070d4ce5fd90.png', '2017cd92c63d.png', 'fe3f62695b2d.png', 'd0b132d2c7ec.png', '3c1efa38d0da.png', '838c87c63422.png', 'f0860c21533b.png', 'd85d052900b4.png', '59bd19c1c5bb.png', '9a78c6a7b1c2.png', 'd06ccd0cf4b8.png', 'e632e38fd2d4.png', '7269a1d84a57.png', '299086c6d1b5.png', 'b8297a2291f5.png', 'ed88faaa325a.png', 'd6e26fe51dce.png', 'e067b06fd655.png', '18b06f56ab27.png', '18323d8f2470.png', 'cae51154e1ce.png', 'cb547e723a16.png', '840a06a9c690.png', 'b3d12069e1c5.png', 'ebe0175e530c.png', '417f408ee8e0.png', '8acffaf1f4b9.png', 'f2094a20b275.png', '6e861bc3bd7b.png', '94dcb491143f.png', '3f0d3629d69e.png', '80a02014b418.png', '360832d84ce0.png', 'f2f569a64949.png', '2b21d293fdf2.png', '698d6e422a80.png', 'a1e236fbc863.png', '1f0e223b8055.png', '0e75d51152fc.png', '912fbe06407e.png', '1943983492e5.png', '5dc23e440de3.png', '4d009cebabc9.png', '3402124408ea.png', 'f920ccd926db.png', 'b8fb9f55cd6d.png', 'e55188915f9d.png', 'e01b7bac822b.png', 'a3fcf42ff56d.png', 'c7e827fc7f41.png', '6fe67fd7f5d1.png', '5995321563b7.png', 'a1b12fdce6c3.png', '89ed6a0dd53f.png', '63c7b0265775.png', '01f7bb8be950.png', 'aa10a4b2e709.png', '7da558d92100.png', '3823acc4e464.png', '6377e23928f6.png', '9f1b14dfa14c.png', 'aa5ce75edcf5.png', 'b05922e7abd3.png', '2c8101f14723.png', '633fe9dbaf39.png', 'ca7140ecf389.png', '26453eb7e989.png', 'b83a6eca125f.png', '0a3202889f4d.png', '475300735b7f.png', '4a4cb731f91a.png', 'ea588d1e5d96.png', 'd02b79fc3200.png', 'e7d2c2c3b30f.png', 'dee687c6e88a.png', '351e842842a2.png', 'cad5b1a82e60.png', 'ee77763a6afb.png', 'd6b109c82067.png', 'ae5d31979f19.png', '93421787f520.png', '454792eb6e05.png', '8ceff4c4c860.png', '40dd4e6e4444.png', '1a1e974a7dbf.png', 'f583a722434c.png', '2eba4279e503.png', 'd659d7fd5ccf.png', '7b9d519cbd66.png', 'ecb4500285ed.png', '66b88a4bc474.png', 'bd269a1f0e4d.png', '9df31421cdd2.png', 'b5a3ca5c0a80.png', '49d69c4c6290.png', '9033f1493da1.png', '0ada12c0e78f.png', '8e4a354e3da2.png', '278aa860dffd.png', '97d02a9b94f7.png', '5f51192841f7.png', 'c3a82acb7d7a.png', 'b5c80d0ed0ff.png', '1da25637859b.png', 'd6130f2ec903.png', '3206171db5be.png', 'ff4832d55461.png', '4384fa687afa.png', 'ae57c8630249.png', 'b960142a8de7.png', 'a443c4fd489c.png', 'c7b622ec8104.png', '71c22da3d6c6.png', '5efa24b03d5e.png', '358d2224de73.png', 'b43440c6ebe4.png', 'e499434242cc.png', '441848e0f308.png', '1b3647865779.png', '74898f372d2b.png', '484dbeb9bf2a.png', '81914ceb4e74.png', 'bb85097857fa.png', 'f48241b0c995.png', '66375b3c64db.png', 'c102db7634d8.png', '8433a032b96c.png', '4a0bba3b7d83.png', '05a5183c92d0.png', '210bfe0127c6.png', '0e0003ddd8df.png', '6762b2b48ea5.png', 'fd079d2e93a2.png', 'bffca6eeb2bf.png', '1cb814ed6332.png', '3de8ad4151e1.png', 'b9b99dad668d.png', 'e3ab63dc9a60.png', 'a3d2a0c4cd17.png', 'a673193cd5a9.png', '6daef3e5ca22.png', '65e6f1bd9875.png', 'fd87b6b2e664.png', 'bacfa2b8e706.png', '5078caaf1f57.png', 'a95d9d61ddd4.png', 'e85d410d6836.png', '36ec36c301c1.png', 'b77b8a1f09f1.png', '002c21358ce6.png', '6852f4531591.png', '4ad6109706e8.png', 'dc0f6e5b489b.png', '6e3526053de0.png', '7877be80901c.png', 'fbdc796290d4.png', 'aa94cc4bfd84.png', '12ab2f6397f0.png', 'a505981d1cab.png', '79ce83c07588.png', 'aa4407aab872.png', 'cb28adab4e8a.png', 'b842b43cb7fb.png', 'dc6fa1b38b83.png', 'f02956bd7c50.png', '266fbefa58fb.png', '999115d9386b.png', '7550966ef777.png', '099021fac3c9.png', 'cd8da43e3069.png', '5a03fe3ed15c.png', 'd88c4843aec3.png', 'f3a88d3026dc.png', '6b66b0e86f7e.png', 'f64214bed40e.png', '4489d421e5aa.png', 'b9c7c5182075.png', 'de730033c683.png', '4ec7796df40e.png', '7bc2e0fa3f72.png', '51cd8d2057fa.png', '8c84e96d9b01.png', '0af296d2f04a.png', '83d6e40c869f.png', 'b89938407ee6.png', '95e732e043a1.png', '8eeac97f02f0.png', '4464bb62bf20.png', '9a326446c431.png', '969f92a390db.png', '39b5b05d6cd9.png', 'c3aa424eff9a.png', 'd7078e8b0349.png', '9d75de31f1b8.png', 'f361060eda3e.png', '0eb52045349f.png', '5264a54e1830.png', '681c3c115684.png', '2bbd1f99ecc3.png', 'd141728fa392.png', 'a01c590c444f.png', '76b950c6ed5e.png', 'f431f2e119d7.png', '384e6c915722.png', 'cb0cc98d7e35.png', '3aa2b1ce6700.png', 'de57c9e9fa93.png', '1864d3411143.png', '20c883d3bd38.png', 'f819c65b803c.png', 'cd941e5bc659.png', '2532613a584a.png', 'a3132c8828e4.png', '071435a218ec.png', 'e9f3c85a2a02.png', '881ec6186e68.png', '7350c50667c5.png', '388f12e8df0b.png', '1968183f0e61.png', '6baafa56895c.png', '92d8a7c8e718.png', '8ff863f8874f.png', 'c26f98f58350.png', '0212dd31f623.png', 'f26b02ead915.png', '8a9bef2fbd4e.png', '224bb938e2dd.png', 'a77eb914b383.png', '25b0e72705a8.png', '7ddcfcea7369.png', 'cbe633765ea7.png', '595446774178.png', '1fddd7c98fd2.png', '96ce10a1dbd7.png', '84e8c62165b5.png', '80b5697f2a5e.png', '6af071b0ac6e.png', 'd16e59a2b33a.png', '9fefe2b44795.png', 'cbc2e57447c2.png', '86baef833ae0.png', '41960d5f58c2.png', 'a247961a5cd9.png', 'cd1c98ec48b1.png', 'a62ea0043aa7.png', 'e265c870f9b3.png', 'de778495a1cd.png', '38055d8b9f08.png']\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyOVahD+GVBbNxivmLcwFb9r"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}